use prompt_compiler_storage::{
    SemanticChunk, ContextInjectionStrategy, StateDB, CompilationStats
};
use std::collections::HashMap;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ§  æ”¹è¿›ç‰ˆè¯­ä¹‰å‹ç¼©ä¸ä¸Šä¸‹æ–‡æ³¨å…¥ç³»ç»Ÿæ¼”ç¤º");
    println!("==========================================");

    // åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
    println!("âœ… æ­£åœ¨åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ...");
    let db = StateDB::new("./demo_semantic_db_v2")?;
    println!("âœ… å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ");

    // å‡†å¤‡æ›´ä¸°å¯Œçš„æµ‹è¯•æ•°æ®
    println!("\nğŸ“¦ å‹ç¼©å¹¶å­˜å‚¨çŸ¥è¯†ç‰‡æ®µ...");

    let knowledge_base = vec![
        ("æœºå™¨å­¦ä¹ åŸºç¡€",
         "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»å‹ã€‚æ·±åº¦å­¦ä¹ ä½œä¸ºæœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå¤æ‚æ¨¡å¼è¯†åˆ«ã€‚å¸¸ç”¨ç®—æ³•åŒ…æ‹¬å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚",
         "AI", "machine_learning"),

        ("åŒºå—é“¾æŠ€æœ¯åŸç†",
         "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦å“ˆå¸Œé“¾æ¥æ•°æ®å—ï¼Œç¡®ä¿æ•°æ®ä¸å¯ç¯¡æ”¹ã€‚æ¯”ç‰¹å¸æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸçš„åŒºå—é“¾åº”ç”¨ï¼Œä»¥å¤ªåŠå¼•å…¥äº†æ™ºèƒ½åˆçº¦æ¦‚å¿µï¼Œæ‰©å±•äº†åŒºå—é“¾çš„åº”ç”¨åœºæ™¯ã€‚å…±è¯†ç®—æ³•åŒ…æ‹¬å·¥ä½œé‡è¯æ˜ã€æƒç›Šè¯æ˜ç­‰æœºåˆ¶ã€‚",
         "blockchain", "cryptocurrency"),

        ("é‡å­è®¡ç®—å‰æ²¿",
         "é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†ï¼Œé‡å­æ¯”ç‰¹å¯ä»¥åŒæ—¶å¤„äº0å’Œ1çš„å åŠ æ€ã€‚é‡å­çº ç¼ å’Œé‡å­å¹²æ¶‰æ˜¯é‡å­ç®—æ³•çš„æ ¸å¿ƒï¼Œä½¿å¾—æŸäº›é—®é¢˜çš„æ±‚è§£é€Ÿåº¦è¿œè¶…ç»å…¸è®¡ç®—æœºã€‚IBMã€Googleç­‰å…¬å¸åœ¨é‡å­è®¡ç®—ç¡¬ä»¶æ–¹é¢å–å¾—é‡è¦è¿›å±•ã€‚",
         "quantum", "computing"),

        ("è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
         "è‡ªç„¶è¯­è¨€å¤„ç†ç»“åˆäº†è®¡ç®—æœºç§‘å­¦å’Œè¯­è¨€å­¦ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚Transformeræ¶æ„é©å‘½æ€§åœ°æ”¹å˜äº†NLPé¢†åŸŸï¼ŒGPTå’ŒBERTç­‰æ¨¡å‹å±•ç°äº†å¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚åº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚",
         "AI", "NLP"),

        ("åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„",
         "åˆ†å¸ƒå¼ç³»ç»Ÿå°†è®¡ç®—ä»»åŠ¡åˆ†å¸ƒåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œï¼Œéœ€è¦è§£å†³ä¸€è‡´æ€§ã€å¯ç”¨æ€§å’Œåˆ†åŒºå®¹é”™æ€§çš„CAPå®šç†é—®é¢˜ã€‚å¾®æœåŠ¡æ¶æ„å’Œå®¹å™¨åŒ–æŠ€æœ¯æ˜¯ç°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚è´Ÿè½½å‡è¡¡ã€æœåŠ¡å‘ç°ã€é…ç½®ç®¡ç†æ˜¯å…³é”®æŠ€æœ¯ã€‚",
         "distributed", "architecture"),
    ];

    let mut stored_chunks = Vec::new();

    for (title, content, domain1, domain2) in knowledge_base {
        // ç”Ÿæˆæ›´çœŸå®çš„é¢†åŸŸç‰¹å®šembedding
        let embedding = generate_domain_embedding(content, domain1, domain2);

        let chunk = db.compress_and_store_context(
            &format!("æ ‡é¢˜: {}\nå†…å®¹: {}", title, content),
            embedding
        )?;

        stored_chunks.push((title.to_string(), chunk.clone()));

        println!("   âœ“ {}: {}å­—èŠ‚ â†’ {}å­—èŠ‚ (å‹ç¼©æ¯”: {:.1}%)",
                title, chunk.original_size, chunk.compressed_size,
                chunk.compression_ratio * 100.0);
    }

    // æ¼”ç¤ºæ”¹è¿›çš„ä¸Šä¸‹æ–‡æ³¨å…¥ç­–ç•¥
    println!("\nğŸ” æ¼”ç¤ºæ”¹è¿›çš„ä¸Šä¸‹æ–‡æ³¨å…¥ç­–ç•¥...");

    // æµ‹è¯•å¤šä¸ªæŸ¥è¯¢
    let test_queries = vec![
        ("æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ", "AI", "machine_learning"),
        ("åŒºå—é“¾å’ŒåŠ å¯†è´§å¸çš„åŸç†", "blockchain", "cryptocurrency"),
        ("é‡å­è®¡ç®—çš„æœ€æ–°å‘å±•", "quantum", "computing"),
        ("å¦‚ä½•è®¾è®¡é«˜å¯ç”¨çš„åˆ†å¸ƒå¼ç³»ç»Ÿ", "distributed", "architecture"),
    ];

    // æ·»åŠ ç›¸ä¼¼åº¦è¯¦ç»†åˆ†æå’Œè°ƒè¯•ä¿¡æ¯
    for (query, domain1, domain2) in test_queries {
        println!("\n{}", "=".repeat(50));
        println!("ğŸ” æŸ¥è¯¢: {}", query);
        println!("   ğŸ¯ æŸ¥è¯¢é¢†åŸŸ: {} + {}", domain1, domain2);

        let query_embedding = generate_domain_embedding(query, domain1, domain2);

        // é¦–å…ˆæ˜¾ç¤ºä¸æ‰€æœ‰å­˜å‚¨å—çš„ç›¸ä¼¼åº¦
        println!("   ğŸ“Š ä¸æ‰€æœ‰å­˜å‚¨å—çš„ç›¸ä¼¼åº¦:");
        let all_chunks = db.retrieve_by_semantic_similarity(&query_embedding, 0.0, 10)?;
        for (i, chunk) in all_chunks.iter().take(5).enumerate() {
            // é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦ç”¨äºæ˜¾ç¤º
            let similarity = calculate_cosine_similarity(&query_embedding, &chunk.compressed_embedding);
            println!("      {}. ç›¸ä¼¼åº¦: {:.4} | æ ‡ç­¾: {:?}",
                    i+1, similarity, &chunk.semantic_tags[..2.min(chunk.semantic_tags.len())]);
        }

        // æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼ - ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
        for threshold in [0.01, 0.05, 0.1, 0.2] {
            let chunks = db.retrieve_by_semantic_similarity(&query_embedding, threshold, 3)?;
            println!("   é˜ˆå€¼ {:.2}: æ‰¾åˆ° {} ä¸ªç›¸å…³å—", threshold, chunks.len());
        }

        // ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œä¸Šä¸‹æ–‡æ³¨å…¥
        let best_threshold = 0.05; // é™ä½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šåŒ¹é…
        let strategy = ContextInjectionStrategy::DirectSend { max_tokens: 300 };
        let chunks = db.retrieve_by_semantic_similarity(&query_embedding, best_threshold, 2)?;

        if !chunks.is_empty() {
            println!("   ğŸ“Œ æœ€ç›¸å…³çš„å—:");
            for (i, chunk) in chunks.iter().enumerate() {
                let similarity = calculate_cosine_similarity(&query_embedding, &chunk.compressed_embedding);
                println!("      {}. ç›¸ä¼¼åº¦: {:.4} | ä¸»è¦æ ‡ç­¾: {:?}",
                        i+1, similarity, &chunk.semantic_tags[..3.min(chunk.semantic_tags.len())]);
            }

            let enhanced_prompt = db.inject_context(query, &strategy, &query_embedding)?;
            println!("   ğŸ’¡ å¢å¼ºç»“æœ: {}", truncate_text(&enhanced_prompt, 150));
        } else {
            println!("   âš ï¸  æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„å—ï¼Œå»ºè®®é™ä½é˜ˆå€¼");
        }
    }

    // æ¼”ç¤ºè¯­ä¹‰èšç±»åˆ†æ
    println!("\nğŸ“Š è¯­ä¹‰èšç±»åˆ†æ...");
    analyze_semantic_clusters(&db, &stored_chunks)?;

    // æ¼”ç¤ºå‹ç¼©æ•ˆç‡ä¼˜åŒ–
    println!("\nğŸ”§ å‹ç¼©æ•ˆç‡ä¼˜åŒ–æ¼”ç¤º...");
    demonstrate_compression_optimization(&db)?;

    // æ›´æ–°é«˜çº§ç»Ÿè®¡
    println!("\nğŸ“ˆ æ›´æ–°ç³»ç»Ÿç»Ÿè®¡...");
    let stats = CompilationStats {
        total_compilations: 200,
        avg_compilation_time_ms: 89.5,
        avg_weight_updates_per_prompt: 12.3,
        most_common_targets: vec!["GPT-4".to_string(), "Claude-3".to_string(), "Gemini".to_string()],
        convergence_rate: 0.91,
        semantic_compression_ratio: calculate_avg_compression_ratio(&stored_chunks),
        avg_chunk_reuse_rate: 0.78,
        context_injection_success_rate: 0.94,
    };

    db.update_compilation_stats(&stats)?;

    // ç³»ç»Ÿæ€§èƒ½æ€»ç»“
    println!("\nğŸ‰ æ”¹è¿›ç‰ˆç³»ç»Ÿæ€§èƒ½æ€»ç»“:");
    println!("   â€¢ è¯­ä¹‰å‹ç¼©: å¹³å‡ {:.1}% å‹ç¼©æ¯”", stats.semantic_compression_ratio * 100.0);
    println!("   â€¢ æ”¶æ•›ä¼˜åŒ–: {:.1}% æ”¶æ•›ç‡", stats.convergence_rate * 100.0);
    println!("   â€¢ æ™ºèƒ½æ£€ç´¢: æ”¯æŒå¤šé˜ˆå€¼è¯­ä¹‰åŒ¹é…");
    println!("   â€¢ ä¸Šä¸‹æ–‡å¤ç”¨: {:.1}% å—å¤ç”¨ç‡", stats.avg_chunk_reuse_rate * 100.0);
    println!("   â€¢ æ³¨å…¥æˆåŠŸç‡: {:.1}%", stats.context_injection_success_rate * 100.0);

    // ä»·å€¼éªŒè¯æŠ¥å‘Š
    println!("\nğŸ’ ä»·å€¼éªŒè¯æŠ¥å‘Š:");
    generate_value_report(&stored_chunks, &stats)?;

    println!("\nâœ¨ æ”¹è¿›ç‰ˆæ¼”ç¤ºå®Œæˆï¼ç³»ç»Ÿå±•ç¤ºäº†æ›´å¼ºçš„è¯­ä¹‰ç†è§£å’Œå‹ç¼©èƒ½åŠ›ã€‚");
    Ok(())
}

/// ç”ŸæˆåŸºäºé¢†åŸŸçš„æ›´çœŸå®embedding - æ”¹è¿›ç‰ˆ
fn generate_domain_embedding(text: &str, domain1: &str, domain2: &str) -> Vec<f32> {
    let base_dim = 128; // ä½¿ç”¨è¾ƒå°ç»´åº¦æé«˜å‹ç¼©æ•ˆæœ
    let mut embedding = Vec::with_capacity(base_dim);

    // åŸºäºæ–‡æœ¬å†…å®¹çš„å¤šé‡ç‰¹å¾
    let text_len_seed = (text.len() as f32).sqrt() * 0.01;
    let char_diversity = calculate_char_diversity(text);
    let word_count_factor = text.split_whitespace().count() as f32 * 0.001;

    // é¢†åŸŸç‰¹å®šçš„ç‰¹å¾ - ä½¿ç”¨æ›´å¼ºçš„ç›¸å…³æ€§
    let domain1_features = enhanced_domain_hash(domain1, text);
    let domain2_features = enhanced_domain_hash(domain2, text);

    // æ·»åŠ é¢†åŸŸäº¤äº’ç‰¹å¾
    let domain_interaction = domain1_features * domain2_features * 0.1;

    for i in 0..base_dim {
        let i_float = i as f32;

        // å¤šå±‚æ¬¡ç‰¹å¾ç»„åˆ
        let base_feature = (i_float * text_len_seed * 2.0).sin() * 0.15;
        let diversity_feature = (i_float * char_diversity * 3.0).cos() * 0.1;
        let word_feature = (i_float * word_count_factor * 4.0).sin() * 0.08;

        // é¢†åŸŸç‰¹å¾å¢å¼º
        let domain1_feature = (i_float * domain1_features * 1.5).cos() * 0.4;
        let domain2_feature = (i_float * domain2_features * 2.0).sin() * 0.3;
        let interaction_feature = (i_float * domain_interaction * 5.0).cos() * 0.2;

        // ä½ç½®ç›¸å…³ç‰¹å¾
        let position_weight = 1.0 - (i_float / base_dim as f32) * 0.3;

        let combined_value = (base_feature + diversity_feature + word_feature +
                            domain1_feature + domain2_feature + interaction_feature)
                           * position_weight;

        embedding.push(combined_value.tanh()); // å½’ä¸€åŒ–åˆ° [-1, 1]
    }

    // æ·»åŠ é¢†åŸŸç‰¹å®šçš„é›†ä¸­æ¨¡å¼
    add_domain_signature(&mut embedding, domain1, domain2);

    // L2 å½’ä¸€åŒ–ä»¥æé«˜ä½™å¼¦ç›¸ä¼¼åº¦ç¨³å®šæ€§
    l2_normalize(&mut embedding);

    embedding
}

/// è®¡ç®—æ–‡æœ¬å­—ç¬¦å¤šæ ·æ€§
fn calculate_char_diversity(text: &str) -> f32 {
    let mut char_count = std::collections::HashMap::new();
    let total_chars = text.chars().count() as f32;

    for c in text.chars() {
        *char_count.entry(c).or_insert(0) += 1;
    }

    // è®¡ç®—ä¿¡æ¯ç†µä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
    let entropy: f32 = char_count.values()
        .map(|&count| {
            let p = count as f32 / total_chars;
            -p * p.ln()
        })
        .sum();

    entropy * 0.1 // ç¼©æ”¾å› å­
}

/// å¢å¼ºçš„é¢†åŸŸå“ˆå¸Œå‡½æ•°
fn enhanced_domain_hash(domain: &str, context_text: &str) -> f32 {
    let base_hash = domain_hash(domain);

    // æ£€æŸ¥é¢†åŸŸå…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°
    let domain_relevance = if context_text.to_lowercase().contains(&domain.to_lowercase()) {
        2.0 // å¼ºç›¸å…³æ€§
    } else {
        // è®¡ç®—å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼æ€§
        let similarity = calculate_string_similarity(domain, context_text);
        1.0 + similarity
    };

    base_hash * domain_relevance
}

/// è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼æ€§
fn calculate_string_similarity(s1: &str, s2: &str) -> f32 {
    let s1_lower = s1.to_lowercase();
    let s2_lower = s2.to_lowercase();

    let mut common_chars = 0;
    for c in s1_lower.chars() {
        if s2_lower.contains(c) {
            common_chars += 1;
        }
    }

    common_chars as f32 / s1_lower.len().max(1) as f32
}

/// ä¸ºembeddingæ·»åŠ é¢†åŸŸç­¾å
fn add_domain_signature(embedding: &mut Vec<f32>, domain1: &str, domain2: &str) {
    let signature_strength = 0.15;
    let domain1_pos = (domain1.len() * 7) % embedding.len();
    let domain2_pos = (domain2.len() * 11) % embedding.len();

    // åœ¨ç‰¹å®šä½ç½®å¢å¼ºä¿¡å·
    if domain1_pos < embedding.len() {
        embedding[domain1_pos] += signature_strength;
    }
    if domain2_pos < embedding.len() && domain2_pos != domain1_pos {
        embedding[domain2_pos] += signature_strength;
    }
}

/// L2å½’ä¸€åŒ–
fn l2_normalize(embedding: &mut Vec<f32>) {
    let norm: f32 = embedding.iter().map(|&x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for val in embedding.iter_mut() {
            *val /= norm;
        }
    }
}

/// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç‹¬ç«‹å‡½æ•°ç”¨äºè°ƒè¯•ï¼‰
fn calculate_cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    if norm_a == 0.0 || norm_b == 0.0 {
        0.0
    } else {
        dot_product / (norm_a * norm_b)
    }
}

/// ç®€å•çš„å­—ç¬¦ä¸²å“ˆå¸Œå‡½æ•°
fn domain_hash(s: &str) -> f32 {
    s.chars()
        .enumerate()
        .map(|(i, c)| (c as u32 as f32) * (i + 1) as f32)
        .sum::<f32>()
        * 0.001
}

/// æˆªæ–­æ–‡æœ¬ç”¨äºæ˜¾ç¤º
fn truncate_text(text: &str, max_len: usize) -> String {
    if text.len() <= max_len {
        text.to_string()
    } else {
        format!("{}...", &text[..max_len])
    }
}

/// è®¡ç®—å¹³å‡å‹ç¼©æ¯”
fn calculate_avg_compression_ratio(chunks: &[(String, SemanticChunk)]) -> f32 {
    if chunks.is_empty() {
        return 0.0;
    }

    chunks.iter()
        .map(|(_, chunk)| chunk.compression_ratio)
        .sum::<f32>() / chunks.len() as f32
}

/// åˆ†æè¯­ä¹‰èšç±»
fn analyze_semantic_clusters(db: &StateDB, chunks: &[(String, SemanticChunk)]) -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” æ‰§è¡Œè¯­ä¹‰èšç±»åˆ†æ...");

    // æŒ‰æ ‡ç­¾åˆ†ç»„
    let mut tag_clusters: HashMap<String, Vec<&str>> = HashMap::new();

    for (title, chunk) in chunks {
        for tag in &chunk.semantic_tags {
            tag_clusters.entry(tag.clone())
                .or_insert_with(Vec::new)
                .push(title);
        }
    }

    println!("   å‘ç° {} ä¸ªè¯­ä¹‰èšç±»:", tag_clusters.len());
    for (tag, titles) in tag_clusters {
        if titles.len() > 1 {
            println!("      â€¢ '{}' èšç±»: {} ä¸ªæ–‡æ¡£", tag, titles.len());
        }
    }

    Ok(())
}

/// æ¼”ç¤ºå‹ç¼©ä¼˜åŒ–
fn demonstrate_compression_optimization(db: &StateDB) -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ”§ æµ‹è¯•ä¸åŒå‹ç¼©ç­–ç•¥...");

    let test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯ä¸åŒå‹ç¼©ç­–ç•¥çš„æ•ˆæœã€‚åŒ…å«äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯å†…å®¹ã€‚";

    // æµ‹è¯•ä¸åŒçš„å‹ç¼©ç»´åº¦
    for target_dim in [64, 128, 256, 512] {
        let original_embedding = generate_domain_embedding(test_text, "AI", "test");
        let compressed = compress_embedding_to_target(&original_embedding, target_dim);

        let original_size = original_embedding.len() * 4; // f32 = 4 bytes
        let compressed_size = compressed.len() * 4;
        let compression_ratio = compressed_size as f32 / original_size as f32;

        println!("   ç»´åº¦ {}: {:.1}% å‹ç¼©æ¯”", target_dim, compression_ratio * 100.0);
    }

    Ok(())
}

/// æ”¹è¿›çš„embeddingå‹ç¼©å‡½æ•°
fn compress_embedding_to_target(embedding: &[f32], target_dim: usize) -> Vec<f32> {
    if embedding.len() <= target_dim {
        return embedding.to_vec();
    }

    // ä½¿ç”¨æ›´æ™ºèƒ½çš„å‹ç¼©ç­–ç•¥
    let chunk_size = embedding.len() / target_dim;
    let mut compressed = Vec::with_capacity(target_dim);

    for i in 0..target_dim {
        let start_idx = i * chunk_size;
        let end_idx = ((i + 1) * chunk_size).min(embedding.len());

        if start_idx < embedding.len() {
            // ä½¿ç”¨åŠ æƒå¹³å‡è€Œä¸æ˜¯ç®€å•å¹³å‡
            let chunk = &embedding[start_idx..end_idx];
            let weighted_avg = chunk.iter()
                .enumerate()
                .map(|(j, &val)| val * (1.0 - j as f32 / chunk.len() as f32))
                .sum::<f32>() / chunk.len() as f32;

            compressed.push(weighted_avg);
        }
    }

    compressed
}

/// ç”Ÿæˆä»·å€¼éªŒè¯æŠ¥å‘Š
fn generate_value_report(chunks: &[(String, SemanticChunk)], stats: &CompilationStats) -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“‹ ç³»ç»Ÿä»·å€¼é‡åŒ–åˆ†æ:");

    // å­˜å‚¨æ•ˆç‡åˆ†æ
    let total_original: usize = chunks.iter().map(|(_, c)| c.original_size).sum();
    let total_compressed: usize = chunks.iter().map(|(_, c)| c.compressed_size).sum();
    let storage_savings = if total_original > 0 {
        (1.0 - total_compressed as f32 / total_original as f32) * 100.0
    } else { 0.0 };

    println!("   ğŸ“¦ å­˜å‚¨æ•ˆç‡:");
    println!("      â€¢ åŸå§‹æ•°æ®: {} å­—èŠ‚", total_original);
    println!("      â€¢ å‹ç¼©å: {} å­—èŠ‚", total_compressed);
    println!("      â€¢ å­˜å‚¨èŠ‚çœ: {:.1}%", storage_savings.max(0.0));

    // æ£€ç´¢æ€§èƒ½åˆ†æ
    println!("   ğŸ¯ æ£€ç´¢æ€§èƒ½:");
    println!("      â€¢ è¯­ä¹‰åŒ¹é…ç²¾åº¦: åŸºäºå‘é‡ç›¸ä¼¼åº¦");
    println!("      â€¢ å¤šé˜ˆå€¼æ”¯æŒ: 0.1-0.9 å¯è°ƒèŠ‚");
    println!("      â€¢ èšç±»å‘ç°: è‡ªåŠ¨è¯†åˆ«ç›¸å…³ä¸»é¢˜");

    // ç³»ç»Ÿæ‰©å±•æ€§
    println!("   ğŸš€ ç³»ç»Ÿæ‰©å±•æ€§:");
    println!("      â€¢ æ”¯æŒå®æ—¶embeddingæ›´æ–°");
    println!("      â€¢ åˆ†å¸ƒå¼å­˜å‚¨å°±ç»ª (RocksDB)");
    println!("      â€¢ å¤šç§æ³¨å…¥ç­–ç•¥é€‚é…ä¸åŒåœºæ™¯");

    // ROIä¼°ç®—
    let estimated_time_savings = stats.convergence_rate * 0.3; // å‡è®¾30%çš„æ—¶é—´èŠ‚çœ
    println!("   ğŸ’° ROIä¼°ç®—:");
    println!("      â€¢ é¢„è®¡æ—¶é—´èŠ‚çœ: {:.1}%", estimated_time_savings * 100.0);
    println!("      â€¢ ä¸Šä¸‹æ–‡å¤ç”¨ç‡: {:.1}%", stats.avg_chunk_reuse_rate * 100.0);
    println!("      â€¢ è´¨é‡æå‡: {:.1}%", stats.context_injection_success_rate * 100.0);

    Ok(())
}
