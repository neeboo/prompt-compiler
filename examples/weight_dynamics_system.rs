use std::collections::HashMap;
use std::error::Error;
use std::time::{SystemTime, UNIX_EPOCH};
use std::sync::Arc;

/// æƒé‡æ›´æ–°åŠ¨åŠ›å­¦ç³»ç»Ÿ - åŸºäºè®ºæ–‡æ ¸å¿ƒç†è®º
struct WeightDynamicsSystem {
    // æ ¸å¿ƒæƒé‡çŸ©é˜µ
    weight_matrix: HashMap<String, WeightNode>,
    // åŠ¨åŠ›å­¦å‚æ•°
    learning_rate: f32,
    decay_factor: f32,
    convergence_threshold: f32,
    // æ€§èƒ½ç»Ÿè®¡
    stats: DynamicsStats,
    // é«˜æ€§èƒ½å­˜å‚¨åç«¯
    storage: Arc<HighPerformanceStorage>,
}

/// æƒé‡èŠ‚ç‚¹ - ä»£è¡¨è¯­ä¹‰ç©ºé—´ä¸­çš„ä¸€ä¸ªæƒé‡ç‚¹
#[derive(Clone, Debug)]
struct WeightNode {
    pub id: String,
    pub weights: Vec<f32>,           // ä¸»æƒé‡å‘é‡
    pub gradient: Vec<f32>,          // æ¢¯åº¦å‘é‡
    pub momentum: Vec<f32>,          // åŠ¨é‡å‘é‡
    pub update_count: u64,           // æ›´æ–°æ¬¡æ•°
    pub last_update: u64,            // æœ€åæ›´æ–°æ—¶é—´
    pub convergence_score: f32,      // æ”¶æ•›åˆ†æ•°
    pub semantic_strength: f32,      // è¯­ä¹‰å¼ºåº¦
    pub connections: Vec<Connection>, // ä¸å…¶ä»–èŠ‚ç‚¹çš„è¿æ¥
}

/// èŠ‚ç‚¹é—´è¿æ¥
#[derive(Clone, Debug)]
struct Connection {
    pub target_id: String,
    pub weight: f32,
    pub activation: f32,
    pub last_fired: u64,
}

/// åŠ¨åŠ›å­¦ç»Ÿè®¡
#[derive(Debug)]
struct DynamicsStats {
    total_updates: u64,
    avg_convergence_rate: f32,
    weight_updates_per_prompt: f32,
    gradient_norm: f32,
    momentum_magnitude: f32,
    network_stability: f32,
    energy_function_value: f32,
}

/// ç®€åŒ–çš„é«˜æ€§èƒ½å­˜å‚¨æ¥å£
struct HighPerformanceStorage {
    data: HashMap<String, Vec<u8>>,
}

impl HighPerformanceStorage {
    fn new() -> Self {
        Self {
            data: HashMap::new(),
        }
    }

    fn store(&mut self, key: &str, value: &[u8]) -> Result<(), Box<dyn Error>> {
        self.data.insert(key.to_string(), value.to_vec());
        Ok(())
    }

    fn load(&self, key: &str) -> Option<&Vec<u8>> {
        self.data.get(key)
    }
}

impl WeightDynamicsSystem {
    /// åˆ›å»ºæ–°çš„æƒé‡åŠ¨åŠ›å­¦ç³»ç»Ÿ
    fn new(dimension: usize) -> Result<Self, Box<dyn Error>> {
        let storage = Arc::new(HighPerformanceStorage::new());

        Ok(Self {
            weight_matrix: HashMap::new(),
            learning_rate: 0.01,
            decay_factor: 0.99,
            convergence_threshold: 0.001,
            stats: DynamicsStats {
                total_updates: 0,
                avg_convergence_rate: 0.0,
                weight_updates_per_prompt: 0.0,
                gradient_norm: 0.0,
                momentum_magnitude: 0.0,
                network_stability: 1.0,
                energy_function_value: 0.0,
            },
            storage,
        })
    }

    /// åˆå§‹åŒ–æƒé‡èŠ‚ç‚¹
    fn initialize_weight_node(&mut self, id: &str, semantic_input: &[f32]) -> Result<(), Box<dyn Error>> {
        let dimension = semantic_input.len();
        let current_time = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();

        // ä½¿ç”¨ Xavier åˆå§‹åŒ–
        let scale = (2.0 / dimension as f32).sqrt();
        let weights: Vec<f32> = (0..dimension)
            .map(|i| {
                let x = (i as f32 + 1.0) * 0.1;
                (x.sin() * scale) + (semantic_input[i] * 0.1)
            })
            .collect();

        let node = WeightNode {
            id: id.to_string(),
            weights,
            gradient: vec![0.0; dimension],
            momentum: vec![0.0; dimension],
            update_count: 0,
            last_update: current_time,
            convergence_score: 1.0,
            semantic_strength: self.calculate_semantic_strength(semantic_input),
            connections: Vec::new(),
        };

        self.weight_matrix.insert(id.to_string(), node);
        println!("ğŸ§  åˆå§‹åŒ–æƒé‡èŠ‚ç‚¹: {} (ç»´åº¦: {}, è¯­ä¹‰å¼ºåº¦: {:.3})",
                id, dimension, self.weight_matrix[id].semantic_strength);

        Ok(())
    }

    /// è®¡ç®—è¯­ä¹‰å¼ºåº¦
    fn calculate_semantic_strength(&self, semantic_input: &[f32]) -> f32 {
        let norm = semantic_input.iter().map(|x| x * x).sum::<f32>().sqrt();
        let diversity = semantic_input.iter()
            .map(|&x| if x.abs() > 0.1 { 1.0 } else { 0.0 })
            .sum::<f32>() / semantic_input.len() as f32;

        norm * diversity
    }

    /// æ ¸å¿ƒæƒé‡æ›´æ–°ç®—æ³• - å®ç°è®ºæ–‡çš„åŠ¨åŠ›å­¦ç†è®º
    fn update_weights(&mut self, node_id: &str, target_output: &[f32], learning_signal: f32) -> Result<f32, Box<dyn Error>> {
        let current_time = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();

        if let Some(node) = self.weight_matrix.get_mut(node_id) {
            let dimension = node.weights.len();

            // 1. å…ˆæå–æƒé‡å‰¯æœ¬ä»¥é¿å…å€Ÿç”¨å†²çª
            let weights_copy = node.weights.clone();

            // 2. è®¡ç®—å½“å‰è¾“å‡º
            let current_output = Self::forward_pass_static(&weights_copy)?;

            // 3. è®¡ç®—æŸå¤±æ¢¯åº¦
            let loss_gradient = Self::compute_loss_gradient_static(&current_output, target_output)?;

            // 4. æ›´æ–°æ¢¯åº¦ï¼ˆå¸¦åŠ¨é‡ï¼‰
            for i in 0..dimension {
                // æ¢¯åº¦æ›´æ–°ï¼šg_t = âˆ‡L + Î» * g_{t-1}
                node.gradient[i] = loss_gradient[i] + self.decay_factor * node.gradient[i];

                // åŠ¨é‡æ›´æ–°ï¼šm_t = Î² * m_{t-1} + Î± * g_t
                node.momentum[i] = 0.9 * node.momentum[i] + self.learning_rate * node.gradient[i];

                // æƒé‡æ›´æ–°ï¼šw_t = w_{t-1} - m_t
                node.weights[i] -= node.momentum[i] * learning_signal;
            }

            // 5. è®¡ç®—æ”¶æ•›æŒ‡æ ‡
            let gradient_norm = node.gradient.iter().map(|x| x * x).sum::<f32>().sqrt();
            let weight_change = node.momentum.iter().map(|x| x * x).sum::<f32>().sqrt();

            node.convergence_score = (-gradient_norm / (1.0 + weight_change)).exp();

            // 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            node.update_count += 1;
            node.last_update = current_time;

            self.stats.total_updates += 1;
            self.stats.gradient_norm = gradient_norm;
            self.stats.momentum_magnitude = node.momentum.iter().map(|x| x.abs()).sum::<f32>() / dimension as f32;

            // 7. æƒé‡æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
            Self::apply_weight_regularization_static(node)?;

            println!("ğŸ”„ æƒé‡æ›´æ–°: {} | æ”¶æ•›åˆ†æ•°: {:.4} | æ¢¯åº¦èŒƒæ•°: {:.6} | å­¦ä¹ ä¿¡å·: {:.3}",
                    node_id, node.convergence_score, gradient_norm, learning_signal);

            Ok(node.convergence_score)
        } else {
            Err(format!("æƒé‡èŠ‚ç‚¹ä¸å­˜åœ¨: {}", node_id).into())
        }
    }

    /// é™æ€å‰å‘ä¼ æ’­æ–¹æ³•
    fn forward_pass_static(weights: &[f32]) -> Result<Vec<f32>, Box<dyn Error>> {
        let mut output = weights.to_vec();

        // åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆSwishï¼‰
        for i in 0..output.len() {
            let x = output[i];
            output[i] = x / (1.0 + (-x).exp()); // Swish: x * sigmoid(x)
        }

        // L2å½’ä¸€åŒ–
        let norm = output.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in output.iter_mut() {
                *x /= norm;
            }
        }

        Ok(output)
    }

    /// é™æ€æŸå¤±æ¢¯åº¦è®¡ç®—æ–¹æ³•
    fn compute_loss_gradient_static(current: &[f32], target: &[f32]) -> Result<Vec<f32>, Box<dyn Error>> {
        if current.len() != target.len() {
            return Err("ç»´åº¦ä¸åŒ¹é…".into());
        }

        let mut gradient = Vec::new();

        for i in 0..current.len() {
            // ä½¿ç”¨å¹³æ»‘L1æŸå¤±çš„æ¢¯åº¦
            let diff = current[i] - target[i];
            let grad = if diff.abs() < 1.0 {
                diff // çº¿æ€§åŒºåŸŸ
            } else {
                diff.signum() // é¥±å’ŒåŒºåŸŸ
            };
            gradient.push(grad);
        }

        Ok(gradient)
    }

    /// é™æ€æƒé‡æ­£åˆ™åŒ–æ–¹æ³•
    fn apply_weight_regularization_static(node: &mut WeightNode) -> Result<(), Box<dyn Error>> {
        let max_weight = 10.0;
        let l2_penalty = 0.0001;

        for weight in node.weights.iter_mut() {
            // L2æ­£åˆ™åŒ–
            *weight *= 1.0 - l2_penalty;

            // æƒé‡è£å‰ª
            *weight = weight.clamp(-max_weight, max_weight);
        }

        Ok(())
    }

    /// å»ºç«‹èŠ‚ç‚¹é—´è¿æ¥ï¼ˆåŠ¨æ€è¿æ¥å½¢æˆï¼‰
    fn establish_connection(&mut self, from_id: &str, to_id: &str, strength: f32) -> Result<(), Box<dyn Error>> {
        if let Some(from_node) = self.weight_matrix.get_mut(from_id) {
            let connection = Connection {
                target_id: to_id.to_string(),
                weight: strength,
                activation: 0.0,
                last_fired: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs(),
            };

            from_node.connections.push(connection);
            println!("ğŸ”— å»ºç«‹è¿æ¥: {} -> {} (å¼ºåº¦: {:.3})", from_id, to_id, strength);
        }

        Ok(())
    }

    /// è®¡ç®—ç½‘ç»œèƒ½é‡å‡½æ•°
    fn calculate_energy_function(&self) -> f32 {
        let mut total_energy = 0.0;

        for node in self.weight_matrix.values() {
            // èŠ‚ç‚¹å†…éƒ¨èƒ½é‡
            let internal_energy: f32 = node.weights.iter().map(|w| w * w).sum::<f32>();

            // è¿æ¥èƒ½é‡
            let connection_energy: f32 = node.connections.iter()
                .map(|conn| conn.weight * conn.activation)
                .sum();

            total_energy += internal_energy + connection_energy;
        }

        total_energy / self.weight_matrix.len() as f32
    }

    /// è¯„ä¼°æ•´ä½“æ”¶æ•›ç‡
    fn evaluate_convergence_rate(&mut self) -> f32 {
        if self.weight_matrix.is_empty() {
            return 0.0;
        }

        let total_convergence: f32 = self.weight_matrix.values()
            .map(|node| node.convergence_score)
            .sum();

        let avg_convergence = total_convergence / self.weight_matrix.len() as f32;

        // æ›´æ–°ç»Ÿè®¡
        self.stats.avg_convergence_rate = avg_convergence;
        self.stats.weight_updates_per_prompt = self.stats.total_updates as f32 / self.weight_matrix.len() as f32;
        self.stats.energy_function_value = self.calculate_energy_function();

        // ç½‘ç»œç¨³å®šæ€§è¯„ä¼°
        let gradient_variance: f32 = self.weight_matrix.values()
            .map(|node| {
                let avg_grad = node.gradient.iter().sum::<f32>() / node.gradient.len() as f32;
                node.gradient.iter().map(|g| (g - avg_grad).powi(2)).sum::<f32>()
            })
            .sum::<f32>() / self.weight_matrix.len() as f32;

        self.stats.network_stability = (-gradient_variance).exp();

        avg_convergence
    }

    /// è¯­ä¹‰é©±åŠ¨çš„æƒé‡è°ƒæ•´
    fn semantic_weight_adjustment(&mut self, semantic_context: &[f32]) -> Result<(), Box<dyn Error>> {
        println!("ğŸ¯ æ‰§è¡Œè¯­ä¹‰é©±åŠ¨çš„æƒé‡è°ƒæ•´...");

        // å…ˆè®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œé¿å…å€Ÿç”¨å†²çª
        let mut relevance_scores = HashMap::new();

        for (node_id, node) in &self.weight_matrix {
            let semantic_relevance = Self::calculate_semantic_relevance_static(&node.weights, semantic_context);
            relevance_scores.insert(node_id.clone(), semantic_relevance);
        }

        // ç„¶åæ›´æ–°èŠ‚ç‚¹
        for (node_id, semantic_relevance) in relevance_scores {
            if let Some(node) = self.weight_matrix.get_mut(&node_id) {
                // åŸºäºè¯­ä¹‰ç›¸å…³æ€§è°ƒæ•´å­¦ä¹ ç‡
                let adaptive_lr = self.learning_rate * (1.0 + semantic_relevance);

                // æ›´æ–°è¯­ä¹‰å¼ºåº¦
                node.semantic_strength = 0.9 * node.semantic_strength + 0.1 * semantic_relevance;

                println!("   ğŸ“Š èŠ‚ç‚¹ {}: è¯­ä¹‰ç›¸å…³æ€§ {:.3}, è‡ªé€‚åº”å­¦ä¹ ç‡ {:.4}",
                        node_id, semantic_relevance, adaptive_lr);
            }
        }

        Ok(())
    }

    /// é™æ€è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§æ–¹æ³•
    fn calculate_semantic_relevance_static(weights: &[f32], context: &[f32]) -> f32 {
        if weights.len() != context.len() {
            return 0.0;
        }

        // ä½™å¼¦ç›¸ä¼¼åº¦
        let dot_product: f32 = weights.iter().zip(context.iter()).map(|(w, c)| w * c).sum();
        let norm_w: f32 = weights.iter().map(|w| w * w).sum::<f32>().sqrt();
        let norm_c: f32 = context.iter().map(|c| c * c).sum::<f32>().sqrt();

        if norm_w > 0.0 && norm_c > 0.0 {
            dot_product / (norm_w * norm_c)
        } else {
            0.0
        }
    }

    /// ç”Ÿæˆæƒé‡åŠ¨åŠ›å­¦æŠ¥å‘Š
    fn generate_dynamics_report(&self) -> Result<(), Box<dyn Error>> {
        println!("\nğŸ“Š æƒé‡æ›´æ–°åŠ¨åŠ›å­¦ç³»ç»ŸæŠ¥å‘Š:");
        println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("â”‚                  åŠ¨åŠ›å­¦ç»Ÿè®¡                              â”‚");
        println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("â”‚ ğŸ§  æƒé‡èŠ‚ç‚¹æ•°: {:>42} â”‚", self.weight_matrix.len());
        println!("â”‚ ğŸ”„ æ€»æ›´æ–°æ¬¡æ•°: {:>42} â”‚", self.stats.total_updates);
        println!("â”‚ ğŸ“ˆ å¹³å‡æ”¶æ•›ç‡: {:>39.3}% â”‚", self.stats.avg_convergence_rate * 100.0);
        println!("â”‚ âš¡ æ¯æç¤ºæƒé‡æ›´æ–°: {:>36.1} â”‚", self.stats.weight_updates_per_prompt);
        println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("â”‚                  æ¢¯åº¦ä¸åŠ¨é‡                              â”‚");
        println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("â”‚ ğŸ“‰ æ¢¯åº¦èŒƒæ•°: {:>45.6} â”‚", self.stats.gradient_norm);
        println!("â”‚ ğŸš€ åŠ¨é‡å¹…åº¦: {:>45.6} â”‚", self.stats.momentum_magnitude);
        println!("â”‚ ğŸ¯ ç½‘ç»œç¨³å®šæ€§: {:>39.3}% â”‚", self.stats.network_stability * 100.0);
        println!("â”‚ âš¡ èƒ½é‡å‡½æ•°å€¼: {:>40.6} â”‚", self.stats.energy_function_value);
        println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("â”‚                  ç³»ç»Ÿå‚æ•°                                â”‚");
        println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("â”‚ ğŸ›ï¸ å­¦ä¹ ç‡: {:>46.4} â”‚", self.learning_rate);
        println!("â”‚ ğŸ“‰ è¡°å‡å› å­: {:>43.3} â”‚", self.decay_factor);
        println!("â”‚ ğŸ¯ æ”¶æ•›é˜ˆå€¼: {:>43.6} â”‚", self.convergence_threshold);
        println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

        // æ˜¾ç¤ºæœ€ä½³æ”¶æ•›çš„èŠ‚ç‚¹
        if let Some((best_id, best_node)) = self.weight_matrix.iter()
            .max_by(|a, b| a.1.convergence_score.partial_cmp(&b.1.convergence_score).unwrap()) {
            println!("\nğŸ† æœ€ä½³æ”¶æ•›èŠ‚ç‚¹: {} (æ”¶æ•›åˆ†æ•°: {:.4})", best_id, best_node.convergence_score);
        }

        Ok(())
    }
}

fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸ§  æƒé‡æ›´æ–°åŠ¨åŠ›å­¦ç³»ç»Ÿæ¼”ç¤º");
    println!("=================================================\n");

    let dimension = 768; // ä½¿ç”¨å‹ç¼©ç»´åº¦ä»¥æé«˜æ€§èƒ½
    let mut dynamics_system = WeightDynamicsSystem::new(dimension)?;

    // æ¨¡æ‹Ÿè¯­ä¹‰è¾“å…¥æ•°æ®
    let semantic_inputs = vec![
        ("prompt_optimization", generate_semantic_vector(dimension, "prompt optimization techniques for AI systems")),
        ("weight_dynamics", generate_semantic_vector(dimension, "neural network weight update mechanisms")),
        ("convergence_theory", generate_semantic_vector(dimension, "mathematical convergence in optimization algorithms")),
        ("adaptive_learning", generate_semantic_vector(dimension, "adaptive learning rate scheduling methods")),
        ("semantic_embedding", generate_semantic_vector(dimension, "semantic embedding and representation learning")),
    ];

    // åˆå§‹åŒ–æƒé‡èŠ‚ç‚¹
    println!("ğŸ”§ åˆå§‹åŒ–æƒé‡åŠ¨åŠ›å­¦ç½‘ç»œ:");
    for (id, semantic_vec) in &semantic_inputs {
        dynamics_system.initialize_weight_node(id, semantic_vec)?;
    }

    // å»ºç«‹èŠ‚ç‚¹é—´è¿æ¥
    println!("\nğŸ”— å»ºç«‹åŠ¨æ€è¿æ¥:");
    dynamics_system.establish_connection("prompt_optimization", "weight_dynamics", 0.8)?;
    dynamics_system.establish_connection("weight_dynamics", "convergence_theory", 0.9)?;
    dynamics_system.establish_connection("convergence_theory", "adaptive_learning", 0.7)?;
    dynamics_system.establish_connection("adaptive_learning", "semantic_embedding", 0.6)?;

    // æ¨¡æ‹Ÿæƒé‡æ›´æ–°è¿‡ç¨‹
    println!("\nâš¡ å¼€å§‹æƒé‡æ›´æ–°åŠ¨åŠ›å­¦è®­ç»ƒ:");

    for epoch in 0..10 {
        println!("\nğŸ“ˆ Epoch {}/10:", epoch + 1);

        for (node_id, target_semantic) in &semantic_inputs {
            // ç”Ÿæˆç•¥æœ‰å˜åŒ–çš„ç›®æ ‡å‘é‡ï¼ˆæ¨¡æ‹ŸçœŸå®è®­ç»ƒï¼‰
            let mut target = target_semantic.clone();
            for i in 0..target.len() {
                target[i] += (epoch as f32 * 0.01 * (i as f32).sin()) * 0.1;
            }

            let learning_signal = 1.0 - (epoch as f32 * 0.05); // æ¸å‡å­¦ä¹ ä¿¡å·
            let convergence = dynamics_system.update_weights(node_id, &target, learning_signal)?;

            if convergence > 0.95 {
                println!("   ğŸ¯ èŠ‚ç‚¹ {} å·²æ”¶æ•›ï¼", node_id);
            }
        }

        // è¯­ä¹‰é©±åŠ¨è°ƒæ•´
        if epoch % 3 == 0 {
            let context_vector = generate_semantic_vector(dimension, "dynamic weight adjustment context");
            dynamics_system.semantic_weight_adjustment(&context_vector)?;
        }

        // è¯„ä¼°æ•´ä½“æ”¶æ•›
        let overall_convergence = dynamics_system.evaluate_convergence_rate();
        println!("   ğŸ“Š æ•´ä½“æ”¶æ•›ç‡: {:.2}%", overall_convergence * 100.0);

        if overall_convergence > 0.98 {
            println!("   ğŸ† ç½‘ç»œå·²è¾¾åˆ°æ”¶æ•›ï¼");
            break;
        }
    }

    // ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    dynamics_system.generate_dynamics_report()?;

    println!("\nğŸ¯ æƒé‡æ›´æ–°åŠ¨åŠ›å­¦æ ¸å¿ƒæˆæœ:");
    println!("   ğŸ§  å®ç°äº†åŸºäºæ¢¯åº¦å’ŒåŠ¨é‡çš„æƒé‡æ›´æ–°æœºåˆ¶");
    println!("   ğŸ“ˆ åŠ¨æ€æ”¶æ•›ç‡ç›‘æ§å’Œè‡ªé€‚åº”å­¦ä¹ ");
    println!("   ğŸ”— è¯­ä¹‰é©±åŠ¨çš„èŠ‚ç‚¹è¿æ¥å’Œæƒé‡è°ƒæ•´");
    println!("   âš¡ é«˜æ•ˆçš„èƒ½é‡å‡½æ•°ä¼˜åŒ–");

    println!("\nâœ… æƒé‡æ›´æ–°åŠ¨åŠ›å­¦ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼");
    println!("   ç³»ç»Ÿå·²å®ç°è®ºæ–‡æ ¸å¿ƒç†è®ºçš„æƒé‡åŠ¨åŠ›å­¦æœºåˆ¶ ğŸ†\n");

    Ok(())
}

/// ç”Ÿæˆæ¨¡æ‹Ÿçš„è¯­ä¹‰å‘é‡
fn generate_semantic_vector(dimension: usize, text: &str) -> Vec<f32> {
    let mut vector = vec![0.0; dimension];
    let bytes = text.as_bytes();

    for (i, &byte) in bytes.iter().enumerate() {
        let idx1 = (i * 7 + byte as usize) % dimension;
        let idx2 = (i * 13 + (byte as usize).pow(2)) % dimension;

        vector[idx1] += (byte as f32 / 255.0) * 0.8;
        vector[idx2] += ((byte as f32 * 0.1).sin() + 1.0) * 0.3;
    }

    // å½’ä¸€åŒ–
    let norm: f32 = vector.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for x in vector.iter_mut() {
            *x /= norm;
        }
    }

    vector
}
