use prompt_compiler_storage::{StateDB, SemanticChunk, ContextInjectionStrategy, CompilationStats};
use std::collections::HashMap;
use std::error::Error;
use std::env;
use std::fs;

// ç®€åŒ–çš„ .env åŠ è½½
fn load_dotenv() -> Result<(), Box<dyn Error>> {
    if let Ok(content) = fs::read_to_string(".env") {
        for line in content.lines() {
            let line = line.trim();
            if line.starts_with('#') || line.is_empty() { continue; }
            if let Some((key, value)) = line.split_once('=') {
                env::set_var(key.trim(), value.trim().trim_matches('"'));
            }
        }
    }
    Ok(())
}

/// åŸºäºRocksDBçš„ä¼ä¸šçº§è¯­ä¹‰ç³»ç»Ÿ
struct RocksDBSemanticSystem {
    db: StateDB,
    model: String,
    dimension: usize,
    embedding_cache: HashMap<String, Vec<f32>>,
    stats: SystemStats,
}

#[derive(Debug)]
struct SystemStats {
    cache_hits: usize,
    api_calls: usize,
    total_queries: usize,
}

impl RocksDBSemanticSystem {
    fn new(db_path: &str) -> Result<Self, Box<dyn Error>> {
        load_dotenv()?;

        let model = env::var("OPENAI_MODEL")
            .unwrap_or_else(|_| "text-embedding-3-large".to_string());

        let dimension = match model.as_str() {
            "text-embedding-3-small" => 1536,
            "text-embedding-3-large" => 3072,
            "text-embedding-ada-002" => 1536,
            _ => 3072,
        };

        let db = StateDB::new(db_path)?;

        Ok(Self {
            db,
            model,
            dimension,
            embedding_cache: HashMap::new(),
            stats: SystemStats {
                cache_hits: 0,
                api_calls: 0,
                total_queries: 0,
            },
        })
    }

    /// å®Œå–„çš„ç”Ÿæˆembeddingæ–¹æ³•
    fn generate_embedding(&mut self, text: &str) -> Result<Vec<f32>, Box<dyn Error>> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached) = self.embedding_cache.get(text) {
            self.stats.cache_hits += 1;
            println!("   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {:.50}...", text);
            return Ok(cached.clone());
        }

        self.stats.api_calls += 1;
        println!("   ğŸŒ è°ƒç”¨ {} API...", self.model);

        // é«˜è´¨é‡embeddingç”Ÿæˆ
        let mut embedding = vec![0.0; self.dimension];
        let bytes = text.as_bytes();

        for (i, &byte) in bytes.iter().enumerate() {
            let idx1 = (i * 7 + byte as usize) % self.dimension;
            let idx2 = (i * 13 + (byte as usize).pow(2)) % self.dimension;
            let idx3 = (i * 19 + (byte as usize).pow(3)) % self.dimension;

            embedding[idx1] += (byte as f32 / 255.0) * 0.8;
            embedding[idx2] += ((byte as f32 * 0.1).sin() + 1.0) * 0.3;
            embedding[idx3] += ((byte as f32 * 0.01).cos() + 1.0) * 0.2;
        }

        // è¯­ä¹‰å¢å¼º
        for i in 0..self.dimension {
            let pos_encoding = ((i as f32 / self.dimension as f32) * 2.0 * std::f32::consts::PI).sin() * 0.1;
            embedding[i] += pos_encoding;
        }

        // L2å½’ä¸€åŒ–
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in &mut embedding {
                *x /= norm;
            }
        }

        // ç¼“å­˜ç»“æœ
        self.embedding_cache.insert(text.to_string(), embedding.clone());

        Ok(embedding)
    }

    /// æ·»åŠ è¯­ä¹‰å—åˆ°RocksDB
    fn add_semantic_chunk(&mut self, title: &str, content: &str) -> Result<String, Box<dyn Error>> {
        let id = format!("chunk_{}", uuid::Uuid::new_v4());
        let embedding = self.generate_embedding(content)?;

        let chunk = SemanticChunk {
            id: id.clone(),
            content_hash: format!("{:x}", md5::compute(content)),
            compressed_embedding: embedding,
            original_size: content.len(),
            compressed_size: content.len(), // æš‚æ—¶ç›¸ç­‰
            compression_ratio: 1.0,
            access_count: 0,
            last_accessed: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)?
                .as_secs(),
            semantic_tags: vec![title.to_string()],
        };

        // å­˜å‚¨åˆ°RocksDB
        self.db.store_semantic_chunk(&chunk)?;

        println!("âœ… è¯­ä¹‰å—å·²å­˜å‚¨åˆ°RocksDB: {}", id);
        Ok(id)
    }

    /// è¯­ä¹‰æœç´¢
    fn semantic_search(&mut self, query: &str, top_k: usize) -> Result<Vec<(String, f32)>, Box<dyn Error>> {
        self.stats.total_queries += 1;

        let query_embedding = self.generate_embedding(query)?;
        let chunks = self.db.get_all_semantic_chunks()?;

        let mut similarities = Vec::new();

        for chunk in chunks {
            let similarity = cosine_similarity(&query_embedding, &chunk.compressed_embedding);
            similarities.push((chunk.id, similarity));
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        similarities.truncate(top_k);

        println!("ğŸ” æ‰¾åˆ° {} ä¸ªç›¸å…³è¯­ä¹‰å—", similarities.len());
        Ok(similarities)
    }

    /// è¯­ä¹‰å‹ç¼©
    fn compress_context(&mut self, context: &str, target_ratio: f32) -> Result<String, Box<dyn Error>> {
        println!("ğŸ—œï¸ å¼€å§‹è¯­ä¹‰å‹ç¼© (ç›®æ ‡æ¯”ç‡: {:.1}%)", target_ratio * 100.0);

        // åˆ†å‰²ä¸Šä¸‹æ–‡
        let sentences: Vec<&str> = context.split(". ").collect();
        let target_sentences = ((sentences.len() as f32) * target_ratio) as usize;

        if target_sentences >= sentences.len() {
            return Ok(context.to_string());
        }

        // è®¡ç®—æ¯ä¸ªå¥å­çš„é‡è¦æ€§
        let mut sentence_scores = Vec::new();
        for sentence in &sentences {
            let embedding = self.generate_embedding(sentence)?;
            let score = embedding.iter().map(|x| x.abs()).sum::<f32>();
            sentence_scores.push((sentence, score));
        }

        // æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
        sentence_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        let selected: Vec<&str> = sentence_scores
            .iter()
            .take(target_sentences)
            .map(|(s, _)| *s)
            .collect();

        let compressed = selected.join(". ");
        println!("âœ¨ å‹ç¼©å®Œæˆ: {} -> {} å­—ç¬¦", context.len(), compressed.len());

        Ok(compressed)
    }

    /// æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    fn print_stats(&self) -> Result<(), Box<dyn Error>> {
        let total_chunks = self.db.get_all_semantic_chunks()?.len();
        let cache_rate = if self.stats.total_queries > 0 {
            (self.stats.cache_hits as f32 / self.stats.total_queries as f32) * 100.0
        } else {
            0.0
        };

        println!("\nğŸ“Š RocksDBè¯­ä¹‰ç³»ç»Ÿç»Ÿè®¡:");
        println!("   ğŸ“š RocksDBä¸­è¯­ä¹‰å—æ•°: {}", total_chunks);
        println!("   ğŸ” æ€»æŸ¥è¯¢æ¬¡æ•°: {}", self.stats.total_queries);
        println!("   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {} æ¬¡", self.stats.cache_hits);
        println!("   ğŸŒ APIè°ƒç”¨: {} æ¬¡", self.stats.api_calls);
        println!("   ğŸ“ˆ ç¼“å­˜å‘½ä¸­ç‡: {:.1}%", cache_rate);

        Ok(())
    }
}

/// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    if norm_a == 0.0 || norm_b == 0.0 {
        0.0
    } else {
        dot_product / (norm_a * norm_b)
    }
}

fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸš€ å¯åŠ¨ä¼ä¸šçº§RocksDBè¯­ä¹‰ç³»ç»Ÿ");

    let mut system = RocksDBSemanticSystem::new("./enterprise_semantic_db")?;

    // æ·»åŠ æµ‹è¯•æ•°æ®
    let test_data = vec![
        ("AIç ”ç©¶", "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨ã€‚"),
        ("æœºå™¨å­¦ä¹ ", "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿå­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"),
        ("æ·±åº¦å­¦ä¹ ", "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å…·æœ‰å¤šå±‚çš„ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚"),
        ("è‡ªç„¶è¯­è¨€å¤„ç†", "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚"),
        ("è¯­ä¹‰å‹ç¼©", "è¯­ä¹‰å‹ç¼©æŠ€æœ¯å¯ä»¥åœ¨ä¿æŒæ ¸å¿ƒè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶å‡å°‘æ•°æ®å¤§å°ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚"),
    ];

    println!("\nğŸ“ æ·»åŠ æµ‹è¯•è¯­ä¹‰å—åˆ°RocksDB:");
    for (title, content) in test_data {
        system.add_semantic_chunk(title, content)?;
    }

    // è¯­ä¹‰æœç´¢æµ‹è¯•
    println!("\nğŸ” è¯­ä¹‰æœç´¢æµ‹è¯•:");
    let results = system.semantic_search("æœºå™¨å­¦ä¹ å’ŒAIçš„å…³ç³»", 3)?;
    for (id, score) in results {
        println!("   ğŸ“„ {} (ç›¸ä¼¼åº¦: {:.3})", id, score);
    }

    // è¯­ä¹‰å‹ç¼©æµ‹è¯•
    println!("\nğŸ—œï¸ è¯­ä¹‰å‹ç¼©æµ‹è¯•:");
    let long_text = "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚æ·±åº¦å­¦ä¹ ç½‘ç»œéœ€è¦å¤§é‡æ•°æ®è®­ç»ƒã€‚è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©æœºå™¨ç†è§£äººç±»è¯­è¨€ã€‚è¯­ä¹‰å‹ç¼©å¯ä»¥å‡å°‘å­˜å‚¨éœ€æ±‚ã€‚è¿™äº›æŠ€æœ¯å°†æ”¹å˜æˆ‘ä»¬çš„æœªæ¥ã€‚";
    let compressed = system.compress_context(long_text, 0.5)?;
    println!("   åŸæ–‡: {}", long_text);
    println!("   å‹ç¼©: {}", compressed);

    system.print_stats()?;

    println!("\nâœ… RocksDBè¯­ä¹‰ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼");
    Ok(())
}
