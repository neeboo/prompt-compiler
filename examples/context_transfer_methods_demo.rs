// Contextä¼ é€’æ–¹å¼çš„å·¥ç¨‹å®ç°
// æ¼”ç¤ºPCå¦‚ä½•å°†å‹ç¼©Contextä¼ é€’ç»™LLMçš„ä¸åŒæ–¹æ³•

use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use tokio;

/// ğŸ¯ æ–¹å¼ä¸€ï¼šæ™®é€šAPIä¼ é€’ï¼ˆæœ€å¸¸è§ï¼‰
/// å°†å‹ç¼©Contextä½œä¸ºpromptçš„ä¸€éƒ¨åˆ†ç›´æ¥å‘é€ç»™LLM
pub struct StandardAPITransfer {
    pub llm_client: LLMClient,
}

impl StandardAPITransfer {
    /// æ ‡å‡†æ–¹å¼ï¼šå°†ContextåµŒå…¥åˆ°promptä¸­
    pub async fn send_context_via_prompt(
        &self,
        compressed_context: &CompressedContext,
        user_query: &str,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ“¤ Method 1: Standard API Transfer via Prompt");

        // æ„å»ºåŒ…å«Contextçš„å®Œæ•´prompt
        let full_prompt = self.build_contextualized_prompt(compressed_context, user_query);

        // å‘é€åˆ°LLM API
        let request = LLMRequest {
            messages: vec![
                LLMMessage {
                    role: "system".to_string(),
                    content: "You are a helpful assistant with access to user context.".to_string(),
                },
                LLMMessage {
                    role: "user".to_string(),
                    content: full_prompt,
                }
            ],
            max_tokens: Some(500),
            temperature: Some(0.7),
        };

        println!("   ğŸ“Š Prompt tokens: {} | Context tokens: {}",
                self.estimate_tokens(&full_prompt), compressed_context.compressed_tokens);

        let response = self.llm_client.chat_completion(request).await?;
        Ok(response.choices[0].message.content.clone())
    }

    /// æ„å»ºåŒ…å«Contextçš„prompt
    fn build_contextualized_prompt(&self, context: &CompressedContext, query: &str) -> String {
        format!(
            "Context Information:\n\
            - User Profile: {}\n\
            - Relevant History: {}\n\
            - Domain Knowledge: {}\n\
            - Personalization: {}\n\n\
            User Query: {}\n\n\
            Please provide a response considering the above context:",
            context.essential_user_info,
            context.relevant_history,
            context.task_knowledge,
            context.personalization_hints,
            query
        )
    }

    fn estimate_tokens(&self, text: &str) -> u32 {
        (text.len() / 4) as u32
    }
}

/// ğŸ¯ æ–¹å¼äºŒï¼šç»“æ„åŒ–APIä¼ é€’ï¼ˆOpenAI Functions/Toolsï¼‰
/// ä½¿ç”¨LLMçš„ç»“æ„åŒ–è¾“å…¥èƒ½åŠ›ï¼Œå°†Contextä½œä¸ºå·¥å…·è°ƒç”¨å‚æ•°
pub struct StructuredAPITransfer {
    pub llm_client: LLMClient,
}

impl StructuredAPITransfer {
    /// ç»“æ„åŒ–æ–¹å¼ï¼šä½¿ç”¨Function Callingä¼ é€’Context
    pub async fn send_context_via_functions(
        &self,
        compressed_context: &CompressedContext,
        user_query: &str,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ“¤ Method 2: Structured API Transfer via Functions");

        let request = LLMRequest {
            messages: vec![
                LLMMessage {
                    role: "user".to_string(),
                    content: user_query.to_string(),
                }
            ],
            functions: Some(vec![
                LLMFunction {
                    name: "respond_with_context".to_string(),
                    description: "Respond to user query using provided context".to_string(),
                    parameters: FunctionParameters {
                        context_info: compressed_context.clone(),
                        user_query: user_query.to_string(),
                    },
                }
            ]),
            function_call: Some(FunctionCall::Auto),
            max_tokens: Some(500),
            temperature: Some(0.7),
        };

        println!("   ğŸ“Š Structured context transfer | Context size: {} tokens",
                compressed_context.compressed_tokens);

        let response = self.llm_client.chat_completion(request).await?;

        // å¤„ç†function callå“åº”
        if let Some(function_call) = &response.choices[0].message.function_call {
            Ok(function_call.arguments.clone())
        } else {
            Ok(response.choices[0].message.content.clone())
        }
    }
}

/// ğŸ¯ æ–¹å¼ä¸‰ï¼šç³»ç»Ÿæ¶ˆæ¯ä¼ é€’ï¼ˆæ¨èç”¨äºæ•æ„ŸContextï¼‰
/// å°†Contextæ”¾åœ¨system messageä¸­ï¼Œä¸ç”¨æˆ·æŸ¥è¯¢åˆ†ç¦»
pub struct SystemMessageTransfer {
    pub llm_client: LLMClient,
}

impl SystemMessageTransfer {
    /// ç³»ç»Ÿæ¶ˆæ¯æ–¹å¼ï¼šContextä½œä¸ºç³»ç»Ÿçº§æŒ‡ä»¤
    pub async fn send_context_via_system_message(
        &self,
        compressed_context: &CompressedContext,
        user_query: &str,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ“¤ Method 3: System Message Transfer");

        // æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«æ‰€æœ‰Contextä¿¡æ¯
        let system_message = self.build_system_context_message(compressed_context);

        let request = LLMRequest {
            messages: vec![
                LLMMessage {
                    role: "system".to_string(),
                    content: system_message,
                },
                LLMMessage {
                    role: "user".to_string(),
                    content: user_query.to_string(),
                }
            ],
            max_tokens: Some(500),
            temperature: Some(0.7),
        };

        println!("   ğŸ“Š System context: {} tokens | User query: {} tokens",
                compressed_context.compressed_tokens,
                self.estimate_tokens(user_query));

        let response = self.llm_client.chat_completion(request).await?;
        Ok(response.choices[0].message.content.clone())
    }

    /// æ„å»ºç³»ç»ŸContextæ¶ˆæ¯
    fn build_system_context_message(&self, context: &CompressedContext) -> String {
        format!(
            "You are an AI assistant with access to the following context about the current user:\n\n\
            USER PROFILE: {}\n\
            INTERACTION HISTORY: {}\n\
            RELEVANT KNOWLEDGE: {}\n\
            COMMUNICATION PREFERENCES: {}\n\n\
            Use this context to provide personalized and relevant responses. \
            Do not explicitly mention having access to this context unless relevant to the conversation.",
            context.essential_user_info,
            context.relevant_history,
            context.task_knowledge,
            context.personalization_hints
        )
    }

    fn estimate_tokens(&self, text: &str) -> u32 {
        (text.len() / 4) as u32
    }
}

/// ğŸ¯ æ–¹å¼å››ï¼šæµå¼ä¼ é€’ï¼ˆé€‚ç”¨äºå¤§å‹Contextï¼‰
/// å°†å¤§å‹Contextåˆ†å—æµå¼ä¼ é€’ç»™LLM
pub struct StreamingTransfer {
    pub llm_client: LLMClient,
}

impl StreamingTransfer {
    /// æµå¼æ–¹å¼ï¼šåˆ†å—ä¼ é€’å¤§å‹Context
    pub async fn send_context_via_streaming(
        &self,
        compressed_context: &CompressedContext,
        user_query: &str,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ“¤ Method 4: Streaming Transfer");

        // å°†Contextåˆ†æˆå¤šä¸ªchunks
        let context_chunks = self.split_context_into_chunks(compressed_context, 200); // æ¯å—200 tokens

        let mut conversation_messages = vec![
            LLMMessage {
                role: "system".to_string(),
                content: "You will receive context information in chunks. Acknowledge each chunk and prepare to answer based on all provided context.".to_string(),
            }
        ];

        // é€æ­¥å‘é€Context chunks
        for (i, chunk) in context_chunks.iter().enumerate() {
            conversation_messages.push(LLMMessage {
                role: "user".to_string(),
                content: format!("Context chunk {}: {}", i + 1, chunk),
            });

            // æ¨¡æ‹ŸLLMç¡®è®¤æ¥æ”¶
            conversation_messages.push(LLMMessage {
                role: "assistant".to_string(),
                content: format!("Understood context chunk {}. Ready for next chunk or query.", i + 1),
            });

            println!("   ğŸ“¦ Sent chunk {}/{} ({} tokens)",
                    i + 1, context_chunks.len(), self.estimate_tokens(chunk));
        }

        // å‘é€æœ€ç»ˆæŸ¥è¯¢
        conversation_messages.push(LLMMessage {
            role: "user".to_string(),
            content: format!("Now please answer this query using all the context provided: {}", user_query),
        });

        let request = LLMRequest {
            messages: conversation_messages,
            max_tokens: Some(500),
            temperature: Some(0.7),
        };

        let response = self.llm_client.chat_completion(request).await?;
        Ok(response.choices[0].message.content.clone())
    }

    /// å°†Contextåˆ†å—
    fn split_context_into_chunks(&self, context: &CompressedContext, chunk_size: u32) -> Vec<String> {
        let full_context = format!(
            "User: {} | History: {} | Knowledge: {} | Preferences: {}",
            context.essential_user_info,
            context.relevant_history,
            context.task_knowledge,
            context.personalization_hints
        );

        // ç®€åŒ–çš„åˆ†å—é€»è¾‘
        let chunk_char_size = (chunk_size * 4) as usize; // å¤§çº¦4å­—ç¬¦=1token
        let mut chunks = Vec::new();
        let mut start = 0;

        while start < full_context.len() {
            let end = std::cmp::min(start + chunk_char_size, full_context.len());
            chunks.push(full_context[start..end].to_string());
            start = end;
        }

        chunks
    }

    fn estimate_tokens(&self, text: &str) -> u32 {
        (text.len() / 4) as u32
    }
}

/// ğŸ¯ æ–¹å¼äº”ï¼šç¼“å­˜è¾…åŠ©ä¼ é€’ï¼ˆé€‚ç”¨äºé‡å¤Contextï¼‰
/// åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡ç¼“å­˜èƒ½åŠ›ï¼Œå‡å°‘é‡å¤ä¼ è¾“
pub struct CacheAssistedTransfer {
    pub llm_client: LLMClient,
    pub context_cache: HashMap<String, String>, // context_id -> cached_reference
}

impl CacheAssistedTransfer {
    /// ç¼“å­˜è¾…åŠ©æ–¹å¼ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜
    pub async fn send_context_via_cache(
        &mut self,
        compressed_context: &CompressedContext,
        user_query: &str,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ“¤ Method 5: Cache-Assisted Transfer");

        let context_id = &compressed_context.context_id;

        // æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if let Some(cached_ref) = self.context_cache.get(context_id) {
            println!("   ğŸ’¾ Using cached context reference: {}", cached_ref);

            // ä½¿ç”¨ç¼“å­˜å¼•ç”¨
            let request = LLMRequest {
                messages: vec![
                    LLMMessage {
                        role: "system".to_string(),
                        content: format!("Use previously cached context: {}", cached_ref),
                    },
                    LLMMessage {
                        role: "user".to_string(),
                        content: user_query.to_string(),
                    }
                ],
                max_tokens: Some(500),
                temperature: Some(0.7),
            };

            let response = self.llm_client.chat_completion(request).await?;
            return Ok(response.choices[0].message.content.clone());
        }

        // é¦–æ¬¡ä¼ è¾“ï¼Œå»ºç«‹ç¼“å­˜
        println!("   ğŸ†• First time context, establishing cache...");

        let cache_establishment_request = LLMRequest {
            messages: vec![
                LLMMessage {
                    role: "system".to_string(),
                    content: format!(
                        "Please cache the following context with ID '{}' for future reference:\n\n\
                        User Profile: {}\n\
                        History: {}\n\
                        Knowledge: {}\n\
                        Preferences: {}\n\n\
                        Respond with 'Context cached successfully' when ready.",
                        context_id,
                        compressed_context.essential_user_info,
                        compressed_context.relevant_history,
                        compressed_context.task_knowledge,
                        compressed_context.personalization_hints
                    ),
                }
            ],
            max_tokens: Some(100),
            temperature: Some(0.1),
        };

        let cache_response = self.llm_client.chat_completion(cache_establishment_request).await?;

        // å­˜å‚¨ç¼“å­˜å¼•ç”¨
        self.context_cache.insert(context_id.clone(), format!("cached_context_{}", context_id));

        println!("   âœ… Context cached: {}", cache_response.choices[0].message.content);

        // ç°åœ¨ä½¿ç”¨ç¼“å­˜å¤„ç†å®é™…æŸ¥è¯¢
        self.send_context_via_cache(compressed_context, user_query).await
    }
}

/// ğŸ¯ å®Œæ•´çš„Contextä¼ é€’ç®¡ç†å™¨
/// æ ¹æ®ä¸åŒåœºæ™¯é€‰æ‹©æœ€ä¼˜çš„ä¼ é€’æ–¹å¼
pub struct ContextTransferManager {
    standard_transfer: StandardAPITransfer,
    structured_transfer: StructuredAPITransfer,
    system_transfer: SystemMessageTransfer,
    streaming_transfer: StreamingTransfer,
    cache_transfer: CacheAssistedTransfer,
}

impl ContextTransferManager {
    pub fn new(llm_client: LLMClient) -> Self {
        Self {
            standard_transfer: StandardAPITransfer { llm_client: llm_client.clone() },
            structured_transfer: StructuredAPITransfer { llm_client: llm_client.clone() },
            system_transfer: SystemMessageTransfer { llm_client: llm_client.clone() },
            streaming_transfer: StreamingTransfer { llm_client: llm_client.clone() },
            cache_transfer: CacheAssistedTransfer {
                llm_client: llm_client.clone(),
                context_cache: HashMap::new(),
            },
        }
    }

    /// æ™ºèƒ½é€‰æ‹©æœ€ä¼˜ä¼ é€’æ–¹å¼
    pub async fn send_context_intelligently(
        &mut self,
        compressed_context: &CompressedContext,
        user_query: &str,
        preferences: &TransferPreferences,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {

        println!("ğŸ¤– Intelligent Context Transfer Selection");
        println!("   ğŸ“Š Context size: {} tokens", compressed_context.compressed_tokens);
        println!("   ğŸ¯ Transfer preferences: {:?}", preferences);

        match self.select_optimal_method(compressed_context, preferences) {
            TransferMethod::Standard => {
                println!("   âœ… Selected: Standard API Transfer");
                self.standard_transfer.send_context_via_prompt(compressed_context, user_query).await
            },
            TransferMethod::Structured => {
                println!("   âœ… Selected: Structured API Transfer");
                self.structured_transfer.send_context_via_functions(compressed_context, user_query).await
            },
            TransferMethod::SystemMessage => {
                println!("   âœ… Selected: System Message Transfer");
                self.system_transfer.send_context_via_system_message(compressed_context, user_query).await
            },
            TransferMethod::Streaming => {
                println!("   âœ… Selected: Streaming Transfer");
                self.streaming_transfer.send_context_via_streaming(compressed_context, user_query).await
            },
            TransferMethod::CacheAssisted => {
                println!("   âœ… Selected: Cache-Assisted Transfer");
                self.cache_transfer.send_context_via_cache(compressed_context, user_query).await
            },
        }
    }

    /// é€‰æ‹©æœ€ä¼˜ä¼ é€’æ–¹æ³•
    fn select_optimal_method(
        &self,
        context: &CompressedContext,
        preferences: &TransferPreferences,
    ) -> TransferMethod {

        // åŸºäºContextå¤§å°å’Œåå¥½é€‰æ‹©æ–¹æ³•
        match (context.compressed_tokens, preferences.priority) {
            // å°Contextï¼Œæ ‡å‡†ä¼ é€’
            (0..=300, _) => TransferMethod::Standard,

            // ä¸­ç­‰Contextï¼Œæ ¹æ®åå¥½é€‰æ‹©
            (301..=600, TransferPriority::Privacy) => TransferMethod::SystemMessage,
            (301..=600, TransferPriority::Structure) => TransferMethod::Structured,
            (301..=600, TransferPriority::Performance) => TransferMethod::CacheAssisted,

            // å¤§Contextï¼Œæµå¼ä¼ é€’
            (601.., _) => TransferMethod::Streaming,
        }
    }
}

/// ä¼ é€’æ–¹æ³•æšä¸¾
#[derive(Debug, Clone)]
pub enum TransferMethod {
    Standard,        // æ ‡å‡†promptåµŒå…¥
    Structured,      // ç»“æ„åŒ–Functionè°ƒç”¨
    SystemMessage,   // ç³»ç»Ÿæ¶ˆæ¯ä¼ é€’
    Streaming,       // æµå¼åˆ†å—ä¼ é€’
    CacheAssisted,   // ç¼“å­˜è¾…åŠ©ä¼ é€’
}

/// ä¼ é€’åå¥½è®¾ç½®
#[derive(Debug, Clone)]
pub struct TransferPreferences {
    pub priority: TransferPriority,
    pub max_context_tokens: u32,
    pub cache_enabled: bool,
    pub privacy_level: PrivacyLevel,
}

#[derive(Debug, Clone)]
pub enum TransferPriority {
    Performance,     // ä¼˜å…ˆè€ƒè™‘æ€§èƒ½
    Privacy,         // ä¼˜å…ˆè€ƒè™‘éšç§
    Structure,       // ä¼˜å…ˆè€ƒè™‘ç»“æ„åŒ–
    CostOptimization, // ä¼˜å…ˆè€ƒè™‘æˆæœ¬
}

#[derive(Debug, Clone)]
pub enum PrivacyLevel {
    Low,    // å…è®¸åœ¨promptä¸­åŒ…å«Context
    Medium, // ä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯éš”ç¦»
    High,   // ä½¿ç”¨ç¼“å­˜å’Œå¼•ç”¨
}

// æ”¯æ’‘ç»“æ„å®šä¹‰
#[derive(Debug, Clone)]
pub struct CompressedContext {
    pub context_id: String,
    pub essential_user_info: String,
    pub relevant_history: String,
    pub task_knowledge: String,
    pub personalization_hints: String,
    pub compressed_tokens: u32,
}

#[derive(Debug, Clone)]
pub struct LLMClient;

impl LLMClient {
    pub async fn chat_completion(&self, request: LLMRequest) -> Result<LLMResponse, Box<dyn std::error::Error + Send + Sync>> {
        // æ¨¡æ‹ŸAPIè°ƒç”¨
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

        Ok(LLMResponse {
            choices: vec![
                LLMChoice {
                    message: LLMMessage {
                        role: "assistant".to_string(),
                        content: "Based on the provided context, here's my response...".to_string(),
                        function_call: None,
                    }
                }
            ]
        })
    }
}

#[derive(Debug, Clone)]
pub struct LLMRequest {
    pub messages: Vec<LLMMessage>,
    pub functions: Option<Vec<LLMFunction>>,
    pub function_call: Option<FunctionCall>,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f32>,
}

#[derive(Debug, Clone)]
pub struct LLMMessage {
    pub role: String,
    pub content: String,
    pub function_call: Option<FunctionCallResponse>,
}

#[derive(Debug, Clone)]
pub struct LLMResponse {
    pub choices: Vec<LLMChoice>,
}

#[derive(Debug, Clone)]
pub struct LLMChoice {
    pub message: LLMMessage,
}

#[derive(Debug, Clone)]
pub struct LLMFunction {
    pub name: String,
    pub description: String,
    pub parameters: FunctionParameters,
}

#[derive(Debug, Clone)]
pub struct FunctionParameters {
    pub context_info: CompressedContext,
    pub user_query: String,
}

#[derive(Debug, Clone)]
pub enum FunctionCall {
    Auto,
}

#[derive(Debug, Clone)]
pub struct FunctionCallResponse {
    pub arguments: String,
}

/// ğŸš€ Demoä¸»å‡½æ•°
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    println!("ğŸ”„ Context Transfer Methods Demonstration");
    println!("{}", "=".repeat(60));

    // åˆ›å»ºæ¨¡æ‹Ÿçš„å‹ç¼©Context
    let compressed_context = CompressedContext {
        context_id: "ctx_user_zhang_001".to_string(),
        essential_user_info: "User: å¼ å…ˆç”Ÿ (æœ¬ç§‘), technical level: high, satisfaction: 8.5".to_string(),
        relevant_history: "Recent: 1. login issue â†’ cache clear resolved; 2. password reset â†’ guided".to_string(),
        task_knowledge: "Similar cases: 3 login resolutions via cache clearing".to_string(),
        personalization_hints: "Adjust tone for concise communication".to_string(),
        compressed_tokens: 280,
    };

    let user_query = "I'm having trouble logging in again, similar to last week's issue.";
    let llm_client = LLMClient;

    // åˆ›å»ºä¼ é€’ç®¡ç†å™¨
    let mut transfer_manager = ContextTransferManager::new(llm_client);

    // è®¾ç½®ä¼ é€’åå¥½
    let preferences = TransferPreferences {
        priority: TransferPriority::Performance,
        max_context_tokens: 500,
        cache_enabled: true,
        privacy_level: PrivacyLevel::Medium,
    };

    // æ¼”ç¤ºæ™ºèƒ½ä¼ é€’
    let response = transfer_manager.send_context_intelligently(
        &compressed_context,
        user_query,
        &preferences,
    ).await?;

    println!("\nğŸ“‹ Final Response: {}", response);

    println!("\nğŸ¯ Key Insights:");
    println!("   ğŸ“¤ 5 different context transfer methods available");
    println!("   ğŸ¤– Intelligent selection based on context size and preferences");
    println!("   ğŸ’¾ Cache-assisted transfer for repeated contexts");
    println!("   ğŸ”’ Privacy-aware transfer options");
    println!("   âš¡ Performance optimization through method selection");

    println!("\nâœ¨ Context transfer methods demonstrated successfully!");
    Ok(())
}
