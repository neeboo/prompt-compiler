// Context Compression & Sharing Infrastructure Architecture
// ä½œä¸ºAgentå’ŒLLMä¹‹é—´çš„ç‹¬ç«‹åŸºç¡€è®¾æ–½

use std::collections::HashMap;
use std::error::Error;
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};

/// ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ç»„ä»¶ - Prompt Compilerä½œä¸ºLLMä»£ç†å±‚
///
/// ç³»ç»Ÿæ¶æ„ï¼šAgenté€šè¿‡PCä¸LLMäº¤äº’ï¼ŒPCè´Ÿè´£æ‰€æœ‰çš„ä¼˜åŒ–å’Œç®¡ç†
///
/// ```
/// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
/// â”‚   Agent A   â”‚â”€â”€â”€â–¶â”‚        Prompt Compiler              â”‚â”€â”€â”€â–¶â”‚   LLM API   â”‚
/// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       (æ™ºèƒ½ä»£ç†å±‚)                   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                                     â”‚          â”‚
/// â”‚   Agent B   â”‚â”€â”€â”€â–¶â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚          â”‚
/// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ Context Manager â”‚  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
/// â”‚   Agent C   â”‚â”€â”€â”€â–¶â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
/// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ Response Proxy  â”‚                â”‚
///                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
///                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
///                    â”‚  â”‚ Weight Dynamics â”‚                â”‚
///                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
///                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
///                    â”‚  â”‚ Storage Engine  â”‚                â”‚
///                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
///                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
///                    â”‚  â”‚ Semantic Cache  â”‚                â”‚
///                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
///                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
///                                â”‚
///                                â–¼
///                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
///                    â”‚    Shared Knowledge     â”‚
///                    â”‚    Base (RocksDB)       â”‚
///                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/// ```
///
/// ğŸ¯ å…³é”®ç‚¹ï¼š
/// 1. Agentä¸ç›´æ¥è®¿é—®LLMï¼Œæ‰€æœ‰è¯·æ±‚éƒ½é€šè¿‡PCä»£ç†
/// 2. PCè´Ÿè´£ä¸Šä¸‹æ–‡å‹ç¼©ã€çŸ¥è¯†å…±äº«ã€æˆæœ¬ä¼˜åŒ–
/// 3. PCç®¡ç†æ‰€æœ‰ä¸LLMçš„äº¤äº’ï¼ŒåŒ…æ‹¬ç¼“å­˜ã€é™çº§ã€é‡è¯•
/// 4. Agentåªéœ€è¦å…³æ³¨ä¸šåŠ¡é€»è¾‘ï¼Œä¸éœ€è¦è€ƒè™‘æ•ˆç‡é—®é¢˜

/// Agentè¯·æ±‚ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentRequest {
    pub agent_id: String,
    pub session_id: String,
    pub context: AgentContext,
    pub query: String,
    pub metadata: RequestMetadata,
}

/// Agentä¸Šä¸‹æ–‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentContext {
    pub agent_type: String,        // "customer_service", "technical_support", etc.
    pub user_profile: Option<UserProfile>,
    pub conversation_history: Vec<ConversationTurn>,
    pub domain_knowledge: Vec<String>,
    pub preferences: HashMap<String, String>,
}

/// ç”¨æˆ·æ¡£æ¡ˆ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UserProfile {
    pub user_id: String,
    pub demographics: HashMap<String, String>,
    pub interaction_history: Vec<String>,
    pub preferences: HashMap<String, String>,
    pub satisfaction_scores: Vec<f64>,
}

/// å¯¹è¯è½®æ¬¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConversationTurn {
    pub timestamp: u64,
    pub user_message: String,
    pub agent_response: String,
    pub quality_score: f64,
}

/// è¯·æ±‚å…ƒæ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RequestMetadata {
    pub priority: Priority,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f32>,
    pub context_budget: Option<u32>, // ä¸Šä¸‹æ–‡tokené¢„ç®—
    pub quality_threshold: Option<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Priority {
    Low,
    Normal,
    High,
    Critical,
}

/// å‹ç¼©åçš„ä¸Šä¸‹æ–‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompressedContext {
    pub context_id: String,
    pub semantic_vector: Vec<f64>,
    pub key_information: String, // å…³é”®ä¿¡æ¯æ‘˜è¦
    pub compression_ratio: f64,
    pub original_tokens: u32,
    pub compressed_tokens: u32,
    pub reuse_count: u32,
    pub last_accessed: u64,
}

/// LLMè¯·æ±‚ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizedLLMRequest {
    pub optimized_prompt: String,
    pub context_references: Vec<String>, // ä¸Šä¸‹æ–‡å¼•ç”¨ID
    pub metadata: LLMRequestMetadata,
    pub fallback_context: Option<String>, // é™çº§æ–¹æ¡ˆ
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMRequestMetadata {
    pub original_tokens: u32,
    pub optimized_tokens: u32,
    pub compression_ratio: f64,
    pub context_reuse_percentage: f64,
    pub estimated_cost: f64,
}

/// ç³»ç»Ÿå“åº”
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InfrastructureResponse {
    pub agent_id: String,
    pub session_id: String,
    pub response: String,
    pub metrics: ResponseMetrics,
    pub context_updates: Vec<ContextUpdate>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResponseMetrics {
    pub tokens_saved: u32,
    pub processing_time_ms: u64,
    pub compression_ratio: f64,
    pub cache_hit_rate: f64,
    pub quality_score: f64,
    pub cost_reduction: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContextUpdate {
    pub context_id: String,
    pub update_type: UpdateType,
    pub new_information: String,
    pub confidence: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum UpdateType {
    UserProfile,
    DomainKnowledge,
    ConversationHistory,
    Preferences,
}

/// ğŸ¯ æ ¸å¿ƒåŸºç¡€è®¾æ–½ - Context Compression Infrastructure
pub struct ContextCompressionInfrastructure {
    context_manager: Arc<RwLock<ContextManager>>,
    weight_dynamics: Arc<RwLock<WeightDynamicsEngine>>,
    storage_engine: Arc<RwLock<StorageEngine>>,
    semantic_cache: Arc<RwLock<SemanticCache>>,
    llm_interface: Arc<RwLock<LLMInterface>>,
    metrics_collector: Arc<RwLock<MetricsCollector>>,
}

impl ContextCompressionInfrastructure {
    /// ğŸš€ å®Œæ•´çš„å¤„ç†workflow
    pub async fn process_agent_request(
        &self,
        request: AgentRequest,
    ) -> Result<InfrastructureResponse, Box<dyn Error + Send + Sync>> {

        println!("ğŸ¯ Processing request from agent: {}", request.agent_id);

        // Phase 1: Context Analysis & Compression
        let compressed_context = self.analyze_and_compress_context(&request).await?;

        // Phase 2: Semantic Cache Lookup
        let cache_result = self.check_semantic_cache(&request, &compressed_context).await?;

        // Phase 3: Context Sharing & Cross-Agent Learning
        let shared_knowledge = self.retrieve_shared_knowledge(&request).await?;

        // Phase 4: Prompt Optimization
        let optimized_request = self.optimize_prompt(
            &request,
            &compressed_context,
            &shared_knowledge,
            cache_result.as_ref(),
        ).await?;

        // Phase 5: LLM Interaction (if cache miss)
        let llm_response = if cache_result.is_none() {
            Some(self.interact_with_llm(&optimized_request).await?)
        } else {
            None
        };

        // Phase 6: Response Processing & Context Update
        let final_response = self.process_response(
            &request,
            cache_result.or(llm_response),
            &compressed_context,
        ).await?;

        // Phase 7: Knowledge Base Update
        self.update_knowledge_base(&request, &final_response).await?;

        // Phase 8: Metrics Collection
        self.collect_metrics(&request, &final_response).await?;

        Ok(final_response)
    }

    /// Phase 1: ä¸Šä¸‹æ–‡åˆ†æä¸å‹ç¼©
    async fn analyze_and_compress_context(
        &self,
        request: &AgentRequest,
    ) -> Result<CompressedContext, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 1: Analyzing and compressing context...");

        let context_manager = self.context_manager.read().await;

        // 1.1 åˆ†æä¸Šä¸‹æ–‡å¤æ‚åº¦
        let complexity_score = context_manager.analyze_context_complexity(&request.context);

        // 1.2 è¯†åˆ«å¯å‹ç¼©çš„ä¿¡æ¯
        let compressible_parts = context_manager.identify_compressible_information(&request.context);

        // 1.3 åº”ç”¨è¯­ä¹‰å‹ç¼©ç®—æ³•
        let semantic_vector = context_manager.generate_semantic_vector(&request.context);

        // 1.4 ç”Ÿæˆå…³é”®ä¿¡æ¯æ‘˜è¦
        let key_information = context_manager.extract_key_information(&request.context);

        // 1.5 è®¡ç®—å‹ç¼©æ¯”ç‡
        let original_tokens = context_manager.estimate_tokens(&request.context);
        let compressed_tokens = context_manager.estimate_compressed_tokens(&key_information);
        let compression_ratio = 1.0 - (compressed_tokens as f64 / original_tokens as f64);

        println!("   âœ… Compression ratio: {:.1}% ({} â†’ {} tokens)",
                compression_ratio * 100.0, original_tokens, compressed_tokens);

        Ok(CompressedContext {
            context_id: format!("ctx_{}_{}", request.agent_id, request.session_id),
            semantic_vector,
            key_information,
            compression_ratio,
            original_tokens,
            compressed_tokens,
            reuse_count: 0,
            last_accessed: chrono::Utc::now().timestamp() as u64,
        })
    }

    /// Phase 2: è¯­ä¹‰ç¼“å­˜æ£€æŸ¥
    async fn check_semantic_cache(
        &self,
        request: &AgentRequest,
        compressed_context: &CompressedContext,
    ) -> Result<Option<String>, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 2: Checking semantic cache...");

        let cache = self.semantic_cache.read().await;

        // 2.1 è®¡ç®—æŸ¥è¯¢çš„è¯­ä¹‰å‘é‡
        let query_vector = cache.generate_query_vector(&request.query, &compressed_context.semantic_vector);

        // 2.2 åœ¨ç¼“å­˜ä¸­æœç´¢ç›¸ä¼¼çš„æŸ¥è¯¢
        let similar_entries = cache.find_similar_queries(&query_vector, 0.85); // 85%ç›¸ä¼¼åº¦é˜ˆå€¼

        if let Some(cache_hit) = similar_entries.first() {
            println!("   ğŸ¯ Cache HIT! Similarity: {:.1}%", cache_hit.similarity * 100.0);

            // 2.3 éªŒè¯ç¼“å­˜æ¡ç›®æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if cache.is_cache_valid(&cache_hit.entry, compressed_context) {
                return Ok(Some(cache_hit.response.clone()));
            }
        }

        println!("   âŒ Cache MISS");
        Ok(None)
    }

    /// Phase 3: å…±äº«çŸ¥è¯†æ£€ç´¢
    async fn retrieve_shared_knowledge(
        &self,
        request: &AgentRequest,
    ) -> Result<SharedKnowledge, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 3: Retrieving shared knowledge...");

        let storage = self.storage_engine.read().await;

        // 3.1 æ£€ç´¢ç›¸åŒç±»å‹Agentçš„ç»éªŒ
        let agent_type_knowledge = storage.get_agent_type_knowledge(&request.context.agent_type).await?;

        // 3.2 æ£€ç´¢ç”¨æˆ·ç›¸å…³çš„å†å²ç»éªŒ
        let user_knowledge = if let Some(profile) = &request.context.user_profile {
            storage.get_user_knowledge(&profile.user_id).await?
        } else {
            Vec::new()
        };

        // 3.3 æ£€ç´¢é¢†åŸŸç›¸å…³çŸ¥è¯†
        let domain_knowledge = storage.get_domain_knowledge(&request.context.domain_knowledge).await?;

        // 3.4 è·¨Agentåä½œçŸ¥è¯†
        let cross_agent_knowledge = storage.get_cross_agent_knowledge(&request.agent_id).await?;

        println!("   âœ… Retrieved {} knowledge entries",
                agent_type_knowledge.len() + user_knowledge.len() + domain_knowledge.len());

        Ok(SharedKnowledge {
            agent_type_knowledge,
            user_knowledge,
            domain_knowledge,
            cross_agent_knowledge,
        })
    }

    /// Phase 4: Promptä¼˜åŒ–
    async fn optimize_prompt(
        &self,
        request: &AgentRequest,
        compressed_context: &CompressedContext,
        shared_knowledge: &SharedKnowledge,
        cached_response: Option<&String>,
    ) -> Result<OptimizedLLMRequest, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 4: Optimizing prompt...");

        if cached_response.is_some() {
            println!("   âš¡ Using cached response, skipping LLM call");
            return Ok(OptimizedLLMRequest {
                optimized_prompt: "CACHED_RESPONSE".to_string(),
                context_references: vec![compressed_context.context_id.clone()],
                metadata: LLMRequestMetadata {
                    original_tokens: compressed_context.original_tokens,
                    optimized_tokens: 0, // ç¼“å­˜å‘½ä¸­ï¼Œæ— éœ€token
                    compression_ratio: 1.0,
                    context_reuse_percentage: 1.0,
                    estimated_cost: 0.0,
                },
                fallback_context: None,
            });
        }

        let context_manager = self.context_manager.read().await;

        // 4.1 æ„å»ºä¼˜åŒ–çš„prompt
        let base_prompt = context_manager.build_base_prompt(&request.context.agent_type);

        // 4.2 æ³¨å…¥å‹ç¼©çš„ä¸Šä¸‹æ–‡
        let context_injection = context_manager.inject_compressed_context(compressed_context);

        // 4.3 æ·»åŠ å…±äº«çŸ¥è¯†å¼•ç”¨
        let knowledge_references = context_manager.build_knowledge_references(shared_knowledge);

        // 4.4 ä¸ªæ€§åŒ–è°ƒæ•´
        let personalization = if let Some(profile) = &request.context.user_profile {
            context_manager.build_personalization(profile)
        } else {
            String::new()
        };

        // 4.5 ç»„è£…æœ€ç»ˆprompt
        let optimized_prompt = format!(
            "{}\n{}\n{}\n{}\nQuery: {}",
            base_prompt,
            context_injection,
            knowledge_references,
            personalization,
            request.query
        );

        let optimized_tokens = context_manager.estimate_prompt_tokens(&optimized_prompt);
        let compression_ratio = 1.0 - (optimized_tokens as f64 / compressed_context.original_tokens as f64);

        println!("   âœ… Prompt optimized: {} â†’ {} tokens ({:.1}% compression)",
                compressed_context.original_tokens, optimized_tokens, compression_ratio * 100.0);

        Ok(OptimizedLLMRequest {
            optimized_prompt,
            context_references: vec![compressed_context.context_id.clone()],
            metadata: LLMRequestMetadata {
                original_tokens: compressed_context.original_tokens,
                optimized_tokens,
                compression_ratio,
                context_reuse_percentage: shared_knowledge.calculate_reuse_percentage(),
                estimated_cost: (optimized_tokens as f64 / 1000.0) * 0.03, // GPT-4 pricing
            },
            fallback_context: Some(compressed_context.key_information.clone()),
        })
    }

    /// Phase 5: LLMäº¤äº’
    async fn interact_with_llm(
        &self,
        optimized_request: &OptimizedLLMRequest,
    ) -> Result<String, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 5: Interacting with LLM...");

        let llm_interface = self.llm_interface.read().await;

        // 5.1 å‘é€ä¼˜åŒ–åçš„è¯·æ±‚åˆ°LLM
        let response = llm_interface.send_request(&optimized_request.optimized_prompt).await?;

        // 5.2 éªŒè¯å“åº”è´¨é‡
        let quality_score = llm_interface.evaluate_response_quality(&response);

        // 5.3 å¦‚æœè´¨é‡ä¸è¶³ï¼Œä½¿ç”¨fallback
        if quality_score < 0.7 {
            if let Some(fallback) = &optimized_request.fallback_context {
                println!("   âš ï¸ Low quality response, using fallback context");
                let fallback_prompt = format!("{}\n{}", fallback, optimized_request.optimized_prompt);
                return llm_interface.send_request(&fallback_prompt).await;
            }
        }

        println!("   âœ… LLM response received (quality: {:.1}%)", quality_score * 100.0);
        Ok(response)
    }

    /// Phase 6: å“åº”å¤„ç†
    async fn process_response(
        &self,
        request: &AgentRequest,
        llm_response: Option<String>,
        compressed_context: &CompressedContext,
    ) -> Result<InfrastructureResponse, Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 6: Processing response...");

        let response_text = llm_response.unwrap_or_else(|| "Cached response used".to_string());

        // 6.1 è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        let metrics = ResponseMetrics {
            tokens_saved: compressed_context.original_tokens.saturating_sub(compressed_context.compressed_tokens),
            processing_time_ms: 50, // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            compression_ratio: compressed_context.compression_ratio,
            cache_hit_rate: if llm_response.is_none() { 1.0 } else { 0.0 },
            quality_score: 0.95, // æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
            cost_reduction: 0.54, // åŸºäºbenchmarkç»“æœ
        };

        // 6.2 ç”Ÿæˆä¸Šä¸‹æ–‡æ›´æ–°
        let context_updates = vec![
            ContextUpdate {
                context_id: compressed_context.context_id.clone(),
                update_type: UpdateType::ConversationHistory,
                new_information: format!("Q: {} A: {}", request.query, response_text),
                confidence: 0.9,
            }
        ];

        println!("   âœ… Response processed with {:.1}% cost reduction",
                metrics.cost_reduction * 100.0);

        Ok(InfrastructureResponse {
            agent_id: request.agent_id.clone(),
            session_id: request.session_id.clone(),
            response: response_text,
            metrics,
            context_updates,
        })
    }

    /// Phase 7: çŸ¥è¯†åº“æ›´æ–°
    async fn update_knowledge_base(
        &self,
        request: &AgentRequest,
        response: &InfrastructureResponse,
    ) -> Result<(), Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 7: Updating knowledge base...");

        let storage = self.storage_engine.write().await;
        let weight_dynamics = self.weight_dynamics.write().await;

        // 7.1 æ›´æ–°æƒé‡åŠ¨åŠ›å­¦
        weight_dynamics.update_weights(
            &request.agent_id,
            &request.query,
            &response.response,
            response.metrics.quality_score,
        ).await?;

        // 7.2 å­˜å‚¨æ–°çš„ç»éªŒ
        storage.store_interaction(request, response).await?;

        // 7.3 æ›´æ–°è·¨Agentå…±äº«çŸ¥è¯†
        if response.metrics.quality_score > 0.8 {
            storage.add_to_shared_knowledge(
                &request.context.agent_type,
                &request.query,
                &response.response,
                response.metrics.quality_score,
            ).await?;
        }

        // 7.4 æ›´æ–°è¯­ä¹‰ç¼“å­˜
        let cache = self.semantic_cache.write().await;
        cache.add_to_cache(&request.query, &response.response, response.metrics.quality_score).await?;

        println!("   âœ… Knowledge base updated");
        Ok(())
    }

    /// Phase 8: æŒ‡æ ‡æ”¶é›†
    async fn collect_metrics(
        &self,
        request: &AgentRequest,
        response: &InfrastructureResponse,
    ) -> Result<(), Box<dyn Error + Send + Sync>> {

        println!("ğŸ” Phase 8: Collecting metrics...");

        let metrics_collector = self.metrics_collector.write().await;

        // 8.1 è®°å½•æ€§èƒ½æŒ‡æ ‡
        metrics_collector.record_performance(&response.metrics).await?;

        // 8.2 è®°å½•Agentä½¿ç”¨æ¨¡å¼
        metrics_collector.record_agent_pattern(&request.agent_id, &request.context.agent_type).await?;

        // 8.3 è®°å½•æˆæœ¬èŠ‚çº¦
        metrics_collector.record_cost_savings(response.metrics.cost_reduction).await?;

        // 8.4 è®°å½•è´¨é‡åˆ†æ•°
        metrics_collector.record_quality_score(response.metrics.quality_score).await?;

        println!("   âœ… Metrics collected");
        Ok(())
    }
}

/// ğŸ”§ æ”¯æŒç»„ä»¶å®šä¹‰ (ç®€åŒ–ç‰ˆ)

struct ContextManager;
impl ContextManager {
    fn analyze_context_complexity(&self, _context: &AgentContext) -> f64 { 0.8 }
    fn identify_compressible_information(&self, _context: &AgentContext) -> Vec<String> { vec![] }
    fn generate_semantic_vector(&self, _context: &AgentContext) -> Vec<f64> { vec![0.1; 128] }
    fn extract_key_information(&self, _context: &AgentContext) -> String { "Key info".to_string() }
    fn estimate_tokens(&self, _context: &AgentContext) -> u32 { 500 }
    fn estimate_compressed_tokens(&self, _info: &str) -> u32 { 200 }
    fn build_base_prompt(&self, agent_type: &str) -> String { format!("You are a {} agent.", agent_type) }
    fn inject_compressed_context(&self, _context: &CompressedContext) -> String { "Context: ...".to_string() }
    fn build_knowledge_references(&self, _knowledge: &SharedKnowledge) -> String { "Knowledge: ...".to_string() }
    fn build_personalization(&self, _profile: &UserProfile) -> String { "Personalization: ...".to_string() }
    fn estimate_prompt_tokens(&self, prompt: &str) -> u32 { (prompt.len() / 4) as u32 }
}

struct WeightDynamicsEngine;
impl WeightDynamicsEngine {
    async fn update_weights(&self, _agent_id: &str, _query: &str, _response: &str, _quality: f64) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
}

struct StorageEngine;
impl StorageEngine {
    async fn get_agent_type_knowledge(&self, _agent_type: &str) -> Result<Vec<String>, Box<dyn Error + Send + Sync>> { Ok(vec![]) }
    async fn get_user_knowledge(&self, _user_id: &str) -> Result<Vec<String>, Box<dyn Error + Send + Sync>> { Ok(vec![]) }
    async fn get_domain_knowledge(&self, _domains: &[String]) -> Result<Vec<String>, Box<dyn Error + Send + Sync>> { Ok(vec![]) }
    async fn get_cross_agent_knowledge(&self, _agent_id: &str) -> Result<Vec<String>, Box<dyn Error + Send + Sync>> { Ok(vec![]) }
    async fn store_interaction(&self, _request: &AgentRequest, _response: &InfrastructureResponse) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
    async fn add_to_shared_knowledge(&self, _agent_type: &str, _query: &str, _response: &str, _quality: f64) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
}

struct SemanticCache;
impl SemanticCache {
    fn generate_query_vector(&self, _query: &str, _context_vector: &[f64]) -> Vec<f64> { vec![0.2; 128] }
    fn find_similar_queries(&self, _vector: &[f64], _threshold: f64) -> Vec<CacheEntry> { vec![] }
    fn is_cache_valid(&self, _entry: &CacheEntry, _context: &CompressedContext) -> bool { true }
    async fn add_to_cache(&self, _query: &str, _response: &str, _quality: f64) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
}

struct LLMInterface;
impl LLMInterface {
    async fn send_request(&self, _prompt: &str) -> Result<String, Box<dyn Error + Send + Sync>> {
        Ok("LLM response".to_string())
    }
    fn evaluate_response_quality(&self, _response: &str) -> f64 { 0.9 }
}

struct MetricsCollector;
impl MetricsCollector {
    async fn record_performance(&self, _metrics: &ResponseMetrics) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
    async fn record_agent_pattern(&self, _agent_id: &str, _agent_type: &str) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
    async fn record_cost_savings(&self, _cost_reduction: f64) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
    async fn record_quality_score(&self, _quality: f64) -> Result<(), Box<dyn Error + Send + Sync>> { Ok(()) }
}

#[derive(Debug, Clone)]
struct SharedKnowledge {
    agent_type_knowledge: Vec<String>,
    user_knowledge: Vec<String>,
    domain_knowledge: Vec<String>,
    cross_agent_knowledge: Vec<String>,
}

impl SharedKnowledge {
    fn calculate_reuse_percentage(&self) -> f64 {
        let total_entries = self.agent_type_knowledge.len() +
                           self.user_knowledge.len() +
                           self.domain_knowledge.len() +
                           self.cross_agent_knowledge.len();
        if total_entries > 0 { 0.8 } else { 0.0 }
    }
}

#[derive(Debug, Clone)]
struct CacheEntry {
    similarity: f64,
    response: String,
    entry: String,
}

/// ğŸš€ Demoè¿è¡Œç¤ºä¾‹
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error + Send + Sync>> {
    println!("ğŸ—ï¸ Context Compression Infrastructure Demo");
    println!("{}", "=".repeat(60));

    // åˆå§‹åŒ–åŸºç¡€è®¾æ–½
    let infrastructure = ContextCompressionInfrastructure {
        context_manager: Arc::new(RwLock::new(ContextManager)),
        weight_dynamics: Arc::new(RwLock::new(WeightDynamicsEngine)),
        storage_engine: Arc::new(RwLock::new(StorageEngine)),
        semantic_cache: Arc::new(RwLock::new(SemanticCache)),
        llm_interface: Arc::new(RwLock::new(LLMInterface)),
        metrics_collector: Arc::new(RwLock::new(MetricsCollector)),
    };

    // æ¨¡æ‹ŸAgentè¯·æ±‚
    let agent_request = AgentRequest {
        agent_id: "cs_agent_001".to_string(),
        session_id: "session_12345".to_string(),
        context: AgentContext {
            agent_type: "customer_service".to_string(),
            user_profile: Some(UserProfile {
                user_id: "user_zhang".to_string(),
                demographics: [("age".to_string(), "35".to_string())].iter().cloned().collect(),
                interaction_history: vec!["Previous login issue resolved".to_string()],
                preferences: [("communication_style".to_string(), "technical".to_string())].iter().cloned().collect(),
                satisfaction_scores: vec![0.9, 0.8, 0.95],
            }),
            conversation_history: vec![
                ConversationTurn {
                    timestamp: 1234567890,
                    user_message: "I can't login".to_string(),
                    agent_response: "Please clear your cache".to_string(),
                    quality_score: 0.9,
                }
            ],
            domain_knowledge: vec!["login_issues".to_string(), "password_reset".to_string()],
            preferences: HashMap::new(),
        },
        query: "Customer reports login failure again, same as last week".to_string(),
        metadata: RequestMetadata {
            priority: Priority::High,
            max_tokens: Some(150),
            temperature: Some(0.7),
            context_budget: Some(300),
            quality_threshold: Some(0.8),
        },
    };

    // å¤„ç†è¯·æ±‚
    let response = infrastructure.process_agent_request(agent_request).await?;

    // æ˜¾ç¤ºç»“æœ
    println!("\nğŸ“‹ Final Response:");
    println!("   Agent: {}", response.agent_id);
    println!("   Response: {}", response.response);
    println!("   Tokens Saved: {}", response.metrics.tokens_saved);
    println!("   Cost Reduction: {:.1}%", response.metrics.cost_reduction * 100.0);
    println!("   Quality Score: {:.1}%", response.metrics.quality_score * 100.0);

    println!("\nâœ¨ Infrastructure demo completed!");
    Ok(())
}
