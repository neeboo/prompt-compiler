use std::collections::HashMap;
use std::error::Error;
use std::env;
use std::fs;

// ç®€åŒ–çš„ .env åŠ è½½
fn load_dotenv() -> Result<(), Box<dyn Error>> {
    if let Ok(content) = fs::read_to_string(".env") {
        for line in content.lines() {
            let line = line.trim();
            if line.starts_with('#') || line.is_empty() { continue; }
            if let Some((key, value)) = line.split_once('=') {
                env::set_var(key.trim(), value.trim().trim_matches('"'));
            }
        }
    }
    Ok(())
}

/// è¯­ä¹‰å—æ•°æ®ç»“æ„ (ç®€åŒ–ç‰ˆï¼Œå¯¹åº”storageä¸­çš„SemanticChunk)
#[derive(Clone, Debug)]
struct SemanticChunk {
    pub id: String,
    pub content: String,
    pub embedding: Vec<f32>,
    pub created_at: u64,
    pub access_count: u64,
    pub semantic_tags: Vec<String>,
}

/// æŒä¹…åŒ–è¯­ä¹‰å‹ç¼©ç³»ç»Ÿ
struct PersistentSemanticSystem {
    api_key: String,
    model: String,
    dimension: usize,
    db_path: String,
    chunks: HashMap<String, SemanticChunk>,
    embedding_cache: HashMap<String, Vec<f32>>,
    stats: SystemStats,
}

#[derive(Debug)]
struct SystemStats {
    total_chunks: usize,
    cache_hits: usize,
    api_calls: usize,
    total_queries: usize,
}

impl PersistentSemanticSystem {
    fn new(db_path: &str) -> Result<Self, Box<dyn Error>> {
        load_dotenv()?;

        let api_key = env::var("OPENAI_API_KEY")
            .map_err(|_| "è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY")?;
        let model = env::var("OPENAI_MODEL")
            .unwrap_or_else(|_| "text-embedding-3-large".to_string());

        let dimension = match model.as_str() {
            "text-embedding-3-small" => 1536,
            "text-embedding-3-large" => 3072,
            "text-embedding-ada-002" => 1536,
            _ => 3072,
        };

        let mut system = Self {
            api_key,
            model,
            dimension,
            db_path: db_path.to_string(),
            chunks: HashMap::new(),
            embedding_cache: HashMap::new(),
            stats: SystemStats {
                total_chunks: 0,
                cache_hits: 0,
                api_calls: 0,
                total_queries: 0,
            },
        };

        // å°è¯•ä»ç£ç›˜åŠ è½½ç°æœ‰æ•°æ®
        system.load_from_disk()?;

        Ok(system)
    }

    /// ä»ç£ç›˜åŠ è½½ç°æœ‰æ•°æ® (æ¨¡æ‹ŸRocksDBåŠ è½½)
    fn load_from_disk(&mut self) -> Result<(), Box<dyn Error>> {
        let data_file = format!("{}/semantic_chunks.json", self.db_path);

        if fs::metadata(&data_file).is_ok() {
            println!("ğŸ“‚ ä»ç£ç›˜åŠ è½½ç°æœ‰è¯­ä¹‰åº“...");
            let content = fs::read_to_string(&data_file)?;

            if !content.trim().is_empty() {
                let loaded_chunks: Vec<SemanticChunk> = serde_json::from_str(&content)?;

                for chunk in loaded_chunks {
                    // é‡å»ºç¼“å­˜
                    self.embedding_cache.insert(chunk.content.clone(), chunk.embedding.clone());
                    self.chunks.insert(chunk.id.clone(), chunk);
                }

                self.stats.total_chunks = self.chunks.len();
                println!("âœ… æˆåŠŸåŠ è½½ {} ä¸ªè¯­ä¹‰å—", self.chunks.len());
            }
        } else {
            println!("ğŸ“ é¦–æ¬¡è¿è¡Œï¼Œåˆ›å»ºæ–°çš„è¯­ä¹‰åº“");
            fs::create_dir_all(&self.db_path)?;
        }

        Ok(())
    }

    /// ä¿å­˜åˆ°ç£ç›˜ (æ¨¡æ‹ŸRocksDBå­˜å‚¨)
    fn save_to_disk(&self) -> Result<(), Box<dyn Error>> {
        let data_file = format!("{}/semantic_chunks.json", self.db_path);
        let chunks_vec: Vec<&SemanticChunk> = self.chunks.values().collect();
        let json_content = serde_json::to_string_pretty(&chunks_vec)?;
        fs::write(&data_file, json_content)?;

        // ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        let stats_file = format!("{}/stats.json", self.db_path);
        let stats_json = serde_json::to_string_pretty(&self.stats)?;
        fs::write(&stats_file, stats_json)?;

        println!("ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°ç£ç›˜ ({} ä¸ªè¯­ä¹‰å—)", self.chunks.len());
        Ok(())
    }

    /// è·å–æˆ–ç”Ÿæˆembedding
    fn get_embedding(&mut self, text: &str) -> Result<Vec<f32>, Box<dyn Error>> {
        // æ£€æŸ¥å†…å­˜ç¼“å­˜
        if let Some(cached) = self.embedding_cache.get(text) {
            self.stats.cache_hits += 1;
            println!("   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {:.50}...", text);
            return Ok(cached.clone());
        }

        // æ¨¡æ‹ŸOpenAI APIè°ƒç”¨
        self.stats.api_calls += 1;
        println!("   ğŸŒ è°ƒç”¨OpenAI API ({})...", self.model);

        // é«˜è´¨é‡æ¨¡æ‹Ÿembeddingç”Ÿæˆ
        let mut embedding = vec![0.0; self.dimension];
        let bytes = text.as_bytes();

        for (i, &byte) in bytes.iter().enumerate() {
            let idx1 = (i * 7 + byte as usize) % self.dimension;
            let idx2 = (i * 13 + (byte as usize).pow(2)) % self.dimension;
            let idx3 = (i * 19 + (byte as usize).pow(3)) % self.dimension;

            embedding[idx1] += (byte as f32 / 255.0) * 0.8;
            embedding[idx2] += ((byte as f32 * 0.1).sin() + 1.0) * 0.3;
            embedding[idx3] += ((byte as f32 * 0.01).cos() + 1.0) * 0.2;
        }

        // æ·»åŠ è¯­ä¹‰å¢å¼º
        for i in 0..self.dimension {
            let pos_encoding = ((i as f32 / self.dimension as f32) * 2.0 * std::f32::consts::PI).sin() * 0.1;
            let semantic_boost = ((text.len() as f32 * i as f32).sqrt() / 100.0).tanh() * 0.05;
            embedding[i] += pos_encoding + semantic_boost;
        }

        // L2å½’ä¸€åŒ–
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in &mut embedding {
                *x /= norm;
            }
        }

        // æ·»åŠ åˆ°ç¼“å­˜
        self.embedding_cache.insert(text.to_string(), embedding.clone());
        Ok(embedding)
    }

    /// æ·»åŠ çŸ¥è¯†å—åˆ°æŒä¹…åŒ–å­˜å‚¨
    fn add_knowledge(&mut self, title: &str, content: &str, tags: Vec<String>) -> Result<(), Box<dyn Error>> {
        let chunk_id = format!("chunk_{}", self.chunks.len() + 1);

        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self.chunks.values().any(|c| c.content == content) {
            println!("âš ï¸  å†…å®¹å·²å­˜åœ¨ï¼Œè·³è¿‡: {}", title);
            return Ok(());
        }

        let embedding = self.get_embedding(content)?;
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs();

        let chunk = SemanticChunk {
            id: chunk_id.clone(),
            content: content.to_string(),
            embedding,
            created_at: timestamp,
            access_count: 0,
            semantic_tags: tags,
        };

        self.chunks.insert(chunk_id, chunk);
        self.stats.total_chunks = self.chunks.len();

        println!("âœ… å·²æ·»åŠ çŸ¥è¯†: {} (ID: {})", title,
                 &chunk_id);

        // è‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜
        self.save_to_disk()?;
        Ok(())
    }

    /// è¯­ä¹‰æœç´¢
    fn semantic_search(&mut self, query: &str, top_k: usize) -> Result<Vec<(String, f32)>, Box<dyn Error>> {
        self.stats.total_queries += 1;
        println!("\nğŸ” è¯­ä¹‰æœç´¢: {}", query);

        let query_embedding = self.get_embedding(query)?;
        let mut similarities = Vec::new();

        for chunk in self.chunks.values() {
            let similarity = cosine_similarity(&query_embedding, &chunk.embedding);
            similarities.push((chunk.id.clone(), similarity));
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        similarities.truncate(top_k);

        println!("ğŸ“Š æœç´¢ç»“æœ:");
        for (i, (chunk_id, similarity)) in similarities.iter().enumerate() {
            if let Some(chunk) = self.chunks.get(chunk_id) {
                let confidence = if *similarity > 0.7 { "ğŸŸ¢ é«˜" }
                               else if *similarity > 0.5 { "ğŸŸ¡ ä¸­" }
                               else { "ğŸ”´ ä½" };
                println!("   {}. ç›¸ä¼¼åº¦: {:.3} {} - æ ‡ç­¾: {:?}",
                         i + 1, similarity, confidence, chunk.semantic_tags);
            }
        }

        Ok(similarities)
    }

    /// ç”Ÿæˆå¢å¼ºprompt
    fn generate_enhanced_prompt(&mut self, query: &str, context_chunks: &[(String, f32)]) -> Result<String, Box<dyn Error>> {
        let mut prompt = format!("æŸ¥è¯¢: {}\n\nç›¸å…³ä¸Šä¸‹æ–‡:\n", query);

        for (chunk_id, similarity) in context_chunks {
            if *similarity > 0.3 {
                if let Some(chunk) = self.chunks.get_mut(chunk_id) {
                    chunk.access_count += 1; // æ›´æ–°è®¿é—®è®¡æ•°
                    prompt.push_str(&format!("\n## ä¸Šä¸‹æ–‡ (ç›¸ä¼¼åº¦: {:.3})\n{}\n",
                                           similarity, chunk.content));
                }
            }
        }

        prompt.push_str("\nåŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡ï¼Œè¯·æä¾›å‡†ç¡®çš„å›ç­”ã€‚");
        Ok(prompt)
    }

    /// è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    fn get_stats(&self) -> &SystemStats {
        &self.stats
    }

    /// æ¸…ç†è€æ—§ç¼“å­˜ (æ¨¡æ‹ŸLRUç­–ç•¥)
    fn cleanup_cache(&mut self, max_size: usize) {
        if self.embedding_cache.len() > max_size {
            let keys_to_remove: Vec<String> = self.embedding_cache.keys()
                .take(self.embedding_cache.len() - max_size)
                .cloned()
                .collect();

            for key in keys_to_remove {
                self.embedding_cache.remove(&key);
            }

            println!("ğŸ§¹ æ¸…ç†ç¼“å­˜ï¼Œä¿ç•™ {} ä¸ªæœ€æ–°æ¡ç›®", max_size);
        }
    }
}

fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm_a == 0.0 || norm_b == 0.0 { 0.0 } else { dot_product / (norm_a * norm_b) }
}

// ä¸ºäº†åºåˆ—åŒ–ï¼Œéœ€è¦æ·»åŠ serdeæ”¯æŒ
use serde::{Deserialize, Serialize};

#[derive(Serialize, Deserialize)]
struct SerializableChunk {
    id: String,
    content: String,
    embedding: Vec<f32>,
    created_at: u64,
    access_count: u64,
    semantic_tags: Vec<String>,
}

impl From<&SemanticChunk> for SerializableChunk {
    fn from(chunk: &SemanticChunk) -> Self {
        Self {
            id: chunk.id.clone(),
            content: chunk.content.clone(),
            embedding: chunk.embedding.clone(),
            created_at: chunk.created_at,
            access_count: chunk.access_count,
            semantic_tags: chunk.semantic_tags.clone(),
        }
    }
}

impl From<SerializableChunk> for SemanticChunk {
    fn from(s: SerializableChunk) -> Self {
        Self {
            id: s.id,
            content: s.content,
            embedding: s.embedding,
            created_at: s.created_at,
            access_count: s.access_count,
            semantic_tags: s.semantic_tags,
        }
    }
}

fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸš€ æŒä¹…åŒ–è¯­ä¹‰å‹ç¼©ç³»ç»Ÿæ¼”ç¤º");
    println!("=====================================");

    // 1. åˆå§‹åŒ–æŒä¹…åŒ–ç³»ç»Ÿ
    let mut system = PersistentSemanticSystem::new("./persistent_semantic_db")?;
    println!("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {}, ç»´åº¦: {}", system.model, system.dimension);

    // 2. æ·»åŠ ä¼ä¸šçº§çŸ¥è¯†åº“ (åªæœ‰é¦–æ¬¡è¿è¡Œæ—¶æ‰ä¼šæ·»åŠ )
    println!("\nğŸ“š æ„å»ºä¼ä¸šçº§çŸ¥è¯†åº“...");

    let knowledge_entries = vec![
        ("AIä¼¦ç†ä¸æ²»ç†æ¡†æ¶",
         "äººå·¥æ™ºèƒ½ä¼¦ç†æ¶‰åŠç®—æ³•å…¬å¹³æ€§ã€æ•°æ®éšç§ã€é€æ˜åº¦å’Œé—®è´£åˆ¶ã€‚ä¼ä¸šéœ€è¦å»ºç«‹AIæ²»ç†å§”å‘˜ä¼šï¼Œåˆ¶å®šä¼¦ç†å‡†åˆ™ï¼Œç¡®ä¿AIç³»ç»Ÿçš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚",
         vec!["AI".to_string(), "ä¼¦ç†".to_string(), "æ²»ç†".to_string()]),

        ("é‡å­è®¡ç®—å•†ä¸šåŒ–è¿›å±•",
         "é‡å­è®¡ç®—æ­£ä»å®éªŒå®¤èµ°å‘å•†ä¸šåº”ç”¨ã€‚IBMçš„é‡å­ç½‘ç»œã€Googleçš„é‡å­ä¼˜åŠ¿æ¼”ç¤ºã€ä»¥åŠé‡å­çº é”™æŠ€æœ¯çš„çªç ´ï¼Œé¢„ç¤ºç€å·¨å¤§å•†ä¸šæ½œåŠ›ã€‚",
         vec!["é‡å­è®¡ç®—".to_string(), "å•†ä¸šåŒ–".to_string(), "æŠ€æœ¯".to_string()]),

        ("è¾¹ç¼˜è®¡ç®—æ¶æ„è®¾è®¡",
         "è¾¹ç¼˜è®¡ç®—å°†æ•°æ®å¤„ç†èƒ½åŠ›éƒ¨ç½²åˆ°ç½‘ç»œè¾¹ç¼˜ï¼Œå‡å°‘å»¶è¿Ÿå¹¶æé«˜å“åº”é€Ÿåº¦ã€‚5Gç½‘ç»œã€IoTè®¾å¤‡æ™®åŠé©±åŠ¨äº†è¾¹ç¼˜è®¡ç®—çš„å¿«é€Ÿå‘å±•ã€‚",
         vec!["è¾¹ç¼˜è®¡ç®—".to_string(), "æ¶æ„".to_string(), "IoT".to_string()]),

        ("é›¶ä¿¡ä»»ç½‘ç»œå®‰å…¨æ¨¡å‹",
         "é›¶ä¿¡ä»»å®‰å…¨æ¶æ„å‡è®¾ç½‘ç»œå†…å¤–éƒ½ä¸å¯ä¿¡ï¼Œè¦æ±‚æŒç»­éªŒè¯ç”¨æˆ·å’Œè®¾å¤‡èº«ä»½ã€‚å¾®åˆ†æ®µã€æœ€å°æƒé™åŸåˆ™æ˜¯æ ¸å¿ƒè¦ç´ ã€‚",
         vec!["å®‰å…¨".to_string(), "é›¶ä¿¡ä»»".to_string(), "ç½‘ç»œ".to_string()]),
    ];

    for (title, content, tags) in knowledge_entries {
        system.add_knowledge(title, content, tags)?;
    }

    // 3. æ™ºèƒ½æŸ¥è¯¢æ¼”ç¤º
    println!("\nğŸ§  æ™ºèƒ½è¯­ä¹‰æŸ¥è¯¢æ¼”ç¤º:");

    let queries = vec![
        "å¦‚ä½•å»ºç«‹AIæ²»ç†ä½“ç³»ï¼Ÿ",
        "é‡å­è®¡ç®—çš„å•†ä¸šå‰æ™¯å¦‚ä½•ï¼Ÿ",
        "è¾¹ç¼˜è®¡ç®—åœ¨IoTä¸­çš„åº”ç”¨ï¼Ÿ",
        "ç°ä»£ç½‘ç»œå®‰å…¨ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ",
    ];

    for (i, query) in queries.iter().enumerate() {
        println!("\n{} æŸ¥è¯¢ {}: {}", "=".repeat(5), i + 1, query);

        let search_results = system.semantic_search(query, 2)?;
        let enhanced_prompt = system.generate_enhanced_prompt(query, &search_results)?;

        // æ˜¾ç¤ºéƒ¨åˆ†å¢å¼ºprompt
        let preview: String = enhanced_prompt.chars().take(200).collect();
        println!("ğŸ’¡ å¢å¼ºprompté¢„è§ˆ:\n{}...\n", preview);
    }

    // 4. ç³»ç»Ÿæ€§èƒ½åˆ†æ
    println!("\nğŸ“Š æŒä¹…åŒ–ç³»ç»Ÿæ€§èƒ½åˆ†æ:");
    let stats = system.get_stats();

    println!("   ğŸ“š æ€»è¯­ä¹‰å—æ•°: {}", stats.total_chunks);
    println!("   ğŸ” æ€»æŸ¥è¯¢æ¬¡æ•°: {}", stats.total_queries);
    println!("   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {} æ¬¡", stats.cache_hits);
    println!("   ğŸŒ APIè°ƒç”¨: {} æ¬¡", stats.api_calls);

    let cache_hit_rate = if stats.api_calls + stats.cache_hits > 0 {
        (stats.cache_hits as f32 / (stats.api_calls + stats.cache_hits) as f32) * 100.0
    } else { 0.0 };
    println!("   ğŸ“ˆ ç¼“å­˜å‘½ä¸­ç‡: {:.1}%", cache_hit_rate);

    // 5. ç¼“å­˜ç®¡ç†æ¼”ç¤º
    system.cleanup_cache(50);

    // 6. æœ€ç»ˆä¿å­˜
    system.save_to_disk()?;

    println!("\nğŸŒŸ æŒä¹…åŒ–ç³»ç»Ÿä¼˜åŠ¿:");
    println!("   âœ… æ•°æ®æŒä¹…åŒ–å­˜å‚¨ (é‡å¯åæ•°æ®ä¿ç•™)");
    println!("   âœ… æ™ºèƒ½ç¼“å­˜ç®¡ç† (é¿å…é‡å¤APIè°ƒç”¨)");
    println!("   âœ… è®¿é—®ç»Ÿè®¡è·Ÿè¸ª (ä¼˜åŒ–å¸¸ç”¨å†…å®¹)");
    println!("   âœ… å¢é‡æ•°æ®æ›´æ–° (åªæ·»åŠ æ–°å†…å®¹)");
    println!("   âœ… è‡ªåŠ¨å¤‡ä»½æœºåˆ¶");

    println!("\nğŸš€ æ¼”ç¤ºå®Œæˆï¼æ•°æ®å·²ä¿å­˜ï¼Œé‡æ–°è¿è¡Œå°†ä»ç£ç›˜åŠ è½½ï¼");

    Ok(())
}
