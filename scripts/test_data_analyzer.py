#!/usr/bin/env python3
"""
æ•°æ®åˆ†æå™¨ - ç»¼åˆåˆ†ææµ‹è¯•ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
"""

import json
import time
from typing import List, Dict, Any
import numpy as np
from datetime import datetime
from utils.performance_metrics import MetricsCalculator
from utils.chart_generator import ChartGenerator


class TestDataAnalyzer:
    def __init__(self):
        self.chart_generator = ChartGenerator()

    def analyze_comprehensive_results(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“æµ‹è¯•ç»“æœ"""

        print("ğŸ“Š Analyzing comprehensive test results...")

        # æå–å…³é”®æŒ‡æ ‡
        analysis = {
            "test_summary": {
                "single_agent": self._extract_summary_metrics(single_agent_results),
                "multi_agent": self._extract_summary_metrics(multi_agent_results)
            },
            "performance_insights": self._generate_performance_insights(
                single_agent_results, multi_agent_results
            ),
            "cost_analysis": self._calculate_comprehensive_cost_analysis(
                single_agent_results, multi_agent_results
            ),
            "recommendations": self._generate_recommendations(
                single_agent_results, multi_agent_results
            ),
            "scalability_analysis": self._analyze_scalability(
                single_agent_results, multi_agent_results
            )
        }

        # ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾è¡¨
        chart_path = self._generate_comprehensive_chart(
            single_agent_results, multi_agent_results
        )
        analysis["comprehensive_chart"] = chart_path

        return analysis

    def _extract_summary_metrics(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æµ‹è¯•ç»“æœçš„å…³é”®æŒ‡æ ‡"""
        if not test_results or "comparison" not in test_results:
            return {}

        comparison = test_results["comparison"]
        scenarios = comparison["scenarios"]
        improvements = comparison.get("improvements", {})  # ä½¿ç”¨getæ–¹æ³•å®‰å…¨è®¿é—®

        return {
            "without_context_sharing": {
                "avg_tokens": scenarios.get("Without Context Sharing", {}).get("avg_tokens", 0),
                "total_tokens": scenarios.get("Without Context Sharing", {}).get("total_tokens", 0),
                "avg_response_time": scenarios.get("Without Context Sharing", {}).get("avg_response_time", 0)
            },
            "with_context_sharing": {
                "avg_tokens": scenarios.get("With Context Sharing", {}).get("avg_tokens", 0),
                "total_tokens": scenarios.get("With Context Sharing", {}).get("total_tokens", 0),
                "avg_response_time": scenarios.get("With Context Sharing", {}).get("avg_response_time", 0)
            },
            "improvements": {
                "token_efficiency": improvements.get("token_efficiency", 0),
                "token_savings": improvements.get("token_savings", 0),
                "response_time_change": improvements.get("response_time", 0)
            }
        }

    def _generate_performance_insights(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        insights = {
            "context_sharing_effectiveness": {
                "single_agent_efficiency": single_summary.get("improvements", {}).get("token_efficiency", 0),
                "multi_agent_efficiency": multi_summary.get("improvements", {}).get("token_efficiency", 0),
                "scalability_factor": 0
            },
            "complexity_impact": {
                "single_agent_avg_tokens": single_summary.get("with_context_sharing", {}).get("avg_tokens", 0),
                "multi_agent_avg_tokens": multi_summary.get("with_context_sharing", {}).get("avg_tokens", 0),
                "complexity_overhead": 0
            }
        }

        # è®¡ç®—å¯æ‰©å±•æ€§å› å­
        single_efficiency = insights["context_sharing_effectiveness"]["single_agent_efficiency"]
        multi_efficiency = insights["context_sharing_effectiveness"]["multi_agent_efficiency"]

        if single_efficiency > 0:
            insights["context_sharing_effectiveness"]["scalability_factor"] = multi_efficiency / single_efficiency

        # è®¡ç®—å¤æ‚åº¦å¼€é”€
        single_tokens = insights["complexity_impact"]["single_agent_avg_tokens"]
        multi_tokens = insights["complexity_impact"]["multi_agent_avg_tokens"]

        if single_tokens > 0:
            insights["complexity_impact"]["complexity_overhead"] = (multi_tokens - single_tokens) / single_tokens * 100

        return insights

    def _calculate_comprehensive_cost_analysis(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæˆæœ¬åˆ†æ"""

        cost_per_1k = 0.002  # GPT-3.5-turbo pricing

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        # å•æ™ºèƒ½ä½“æˆæœ¬
        single_cost_without = (single_summary.get("without_context_sharing", {}).get("total_tokens", 0) / 1000) * cost_per_1k
        single_cost_with = (single_summary.get("with_context_sharing", {}).get("total_tokens", 0) / 1000) * cost_per_1k
        single_savings = single_cost_without - single_cost_with

        # å¤šæ™ºèƒ½ä½“æˆæœ¬
        multi_cost_without = (multi_summary.get("without_context_sharing", {}).get("total_tokens", 0) / 1000) * cost_per_1k
        multi_cost_with = (multi_summary.get("with_context_sharing", {}).get("total_tokens", 0) / 1000) * cost_per_1k
        multi_savings = multi_cost_without - multi_cost_with

        return {
            "single_agent": {
                "cost_without_context": single_cost_without,
                "cost_with_context": single_cost_with,
                "savings_usd": single_savings,
                "savings_percentage": (single_savings / single_cost_without * 100) if single_cost_without > 0 else 0
            },
            "multi_agent": {
                "cost_without_context": multi_cost_without,
                "cost_with_context": multi_cost_with,
                "savings_usd": multi_savings,
                "savings_percentage": (multi_savings / multi_cost_without * 100) if multi_cost_without > 0 else 0
            },
            "total_savings": {
                "usd": single_savings + multi_savings,
                "percentage": ((single_savings + multi_savings) / (single_cost_without + multi_cost_without) * 100) if (single_cost_without + multi_cost_without) > 0 else 0
            }
        }

    def _generate_recommendations(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, List[str]]:
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        recommendations = {
            "when_to_use_context_sharing": [],
            "performance_optimization": [],
            "cost_optimization": [],
            "architecture_considerations": []
        }

        # åŸºäºæ€§èƒ½æ•°æ®ç”Ÿæˆå»ºè®®
        single_efficiency = single_summary.get("improvements", {}).get("token_efficiency", 0)
        multi_efficiency = multi_summary.get("improvements", {}).get("token_efficiency", 0)

        if single_efficiency > 20:
            recommendations["when_to_use_context_sharing"].append(
                f"âœ… å•æ™ºèƒ½ä½“åœºæ™¯æ˜¾ç¤º {single_efficiency:.1f}% çš„Tokenæ•ˆç‡æå‡ï¼Œæ¨èä½¿ç”¨"
            )

        if multi_efficiency > 30:
            recommendations["when_to_use_context_sharing"].append(
                f"âœ… å¤šæ™ºèƒ½ä½“åœºæ™¯æ˜¾ç¤º {multi_efficiency:.1f}% çš„Tokenæ•ˆç‡æå‡ï¼Œå¼ºçƒˆæ¨èä½¿ç”¨"
            )

        if multi_efficiency > single_efficiency * 1.2:
            recommendations["architecture_considerations"].append(
                "ğŸ—ï¸  Context Sharingåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¡¨ç°æ›´ä¼˜ï¼Œé€‚åˆåä½œå‹åº”ç”¨"
            )

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if single_efficiency > 0:
            recommendations["performance_optimization"].append(
                "âš¡ Context Sharingæœ‰æ•ˆå‡å°‘Tokenä½¿ç”¨ï¼Œæå‡å“åº”æ•ˆç‡"
            )

        # æˆæœ¬ä¼˜åŒ–å»ºè®®
        recommendations["cost_optimization"].append(
            "ğŸ’° é€šè¿‡Context Sharingå¯æ˜¾è‘—é™ä½APIè°ƒç”¨æˆæœ¬"
        )

        return recommendations

    def _analyze_scalability(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æå¯æ‰©å±•æ€§"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        # è®¡ç®—æ‰©å±•æ•ˆç‡
        single_tokens = single_summary.get("with_context_sharing", {}).get("avg_tokens", 0)
        multi_tokens = multi_summary.get("with_context_sharing", {}).get("avg_tokens", 0)

        scalability_efficiency = 0
        if single_tokens > 0:
            scalability_efficiency = (1 - (multi_tokens / single_tokens)) * 100

        return {
            "scaling_efficiency": scalability_efficiency,
            "single_agent_baseline": single_tokens,
            "multi_agent_performance": multi_tokens,
            "scalability_rating": self._rate_scalability(scalability_efficiency),
            "recommendations": self._scalability_recommendations(scalability_efficiency)
        }

    def _rate_scalability(self, efficiency: float) -> str:
        """è¯„çº§å¯æ‰©å±•æ€§"""
        if efficiency >= 20:
            return "â­â­â­â­â­ ä¼˜ç§€"
        elif efficiency >= 10:
            return "â­â­â­â­ è‰¯å¥½"
        elif efficiency >= 0:
            return "â­â­â­ ä¸€èˆ¬"
        else:
            return "â­â­ éœ€è¦ä¼˜åŒ–"

    def _scalability_recommendations(self, efficiency: float) -> List[str]:
        """å¯æ‰©å±•æ€§å»ºè®®"""
        if efficiency >= 15:
            return ["ğŸš€ Context Sharingå±•ç°å‡ºè‰²çš„æ‰©å±•æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²"]
        elif efficiency >= 5:
            return ["ğŸ“ˆ Context Sharingå…·å¤‡è‰¯å¥½æ‰©å±•æ½œåŠ›ï¼Œå»ºè®®åœ¨å¤æ‚åœºæ™¯ä¸­ä½¿ç”¨"]
        else:
            return ["ğŸ”§ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–Context Sharingç®—æ³•ä»¥æå‡æ‰©å±•æ•ˆç‡"]

    def _generate_comprehensive_chart(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾è¡¨"""

        # è¿™é‡Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç»¼åˆçš„ä»ªè¡¨æ¿å›¾è¡¨
        # æš‚æ—¶è¿”å›è·¯å¾„ï¼Œå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„å¯è§†åŒ–
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"comprehensive_analysis_{timestamp}.png"

    def generate_markdown_report(
        self,
        analysis: Dict[str, Any],
        output_file: str = None
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"pc_node_analysis_report_{timestamp}.md"

        report_content = self._build_markdown_content(analysis)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return output_file

    def _build_markdown_content(self, analysis: Dict[str, Any]) -> str:
        """æ„å»ºMarkdownæŠ¥å‘Šå†…å®¹"""

        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

        content = f"""# PC Node æ€§èƒ½åˆ†ææŠ¥å‘Š

*ç”Ÿæˆæ—¶é—´: {timestamp}*

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ

### å•æ™ºèƒ½ä½“æµ‹è¯•ç»“æœ
- **Tokenæ•ˆç‡æå‡**: {analysis['test_summary']['single_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%
- **TokenèŠ‚çœ**: {analysis['test_summary']['single_agent'].get('improvements', {}).get('token_savings', 0)} tokens
- **å“åº”æ—¶é—´å˜åŒ–**: {analysis['test_summary']['single_agent'].get('improvements', {}).get('response_time_change', 0):.1f}%

### å¤šæ™ºèƒ½ä½“æµ‹è¯•ç»“æœ
- **Tokenæ•ˆç‡æå‡**: {analysis['test_summary']['multi_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%
- **TokenèŠ‚çœ**: {analysis['test_summary']['multi_agent'].get('improvements', {}).get('token_savings', 0)} tokens
- **å“åº”æ—¶é—´å˜åŒ–**: {analysis['test_summary']['multi_agent'].get('improvements', {}).get('response_time_change', 0):.1f}%

## ğŸ’¡ æ€§èƒ½æ´å¯Ÿ

### Context Sharingæ•ˆæœ
- **å•æ™ºèƒ½ä½“æ•ˆç‡**: {analysis['performance_insights']['context_sharing_effectiveness']['single_agent_efficiency']:.1f}%
- **å¤šæ™ºèƒ½ä½“æ•ˆç‡**: {analysis['performance_insights']['context_sharing_effectiveness']['multi_agent_efficiency']:.1f}%
- **å¯æ‰©å±•æ€§å› å­**: {analysis['performance_insights']['context_sharing_effectiveness']['scalability_factor']:.2f}

## ğŸ’° æˆæœ¬åˆ†æ

### å•æ™ºèƒ½ä½“åœºæ™¯
- **ä¸ä½¿ç”¨Context Sharing**: ${analysis['cost_analysis']['single_agent']['cost_without_context']:.4f}
- **ä½¿ç”¨Context Sharing**: ${analysis['cost_analysis']['single_agent']['cost_with_context']:.4f}
- **èŠ‚çœ**: ${analysis['cost_analysis']['single_agent']['savings_usd']:.4f} ({analysis['cost_analysis']['single_agent']['savings_percentage']:.1f}%)

### å¤šæ™ºèƒ½ä½“åœºæ™¯
- **ä¸ä½¿ç”¨Context Sharing**: ${analysis['cost_analysis']['multi_agent']['cost_without_context']:.4f}
- **ä½¿ç”¨Context Sharing**: ${analysis['cost_analysis']['multi_agent']['cost_with_context']:.4f}
- **èŠ‚çœ**: ${analysis['cost_analysis']['multi_agent']['savings_usd']:.4f} ({analysis['cost_analysis']['multi_agent']['savings_percentage']:.1f}%)

## ğŸ¯ ä½¿ç”¨å»ºè®®

### ä½•æ—¶ä½¿ç”¨Context Sharing
"""

        for recommendation in analysis['recommendations']['when_to_use_context_sharing']:
            content += f"- {recommendation}\n"

        content += "\n### æ¶æ„è€ƒè™‘\n"
        for recommendation in analysis['recommendations']['architecture_considerations']:
            content += f"- {recommendation}\n"

        content += f"""
## ğŸ“ˆ å¯æ‰©å±•æ€§åˆ†æ

- **æ‰©å±•æ•ˆç‡**: {analysis['scalability_analysis']['scaling_efficiency']:.1f}%
- **å¯æ‰©å±•æ€§è¯„çº§**: {analysis['scalability_analysis']['scalability_rating']}

### æ‰©å±•å»ºè®®
"""

        for recommendation in analysis['scalability_analysis']['recommendations']:
            content += f"- {recommendation}\n"

        content += f"""
## ğŸ† æ€»ç»“

åŸºäºæµ‹è¯•ç»“æœï¼ŒPC Nodeçš„Context SharingåŠŸèƒ½åœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼š

1. **æ˜¾è‘—çš„Tokenæ•ˆç‡æå‡** - åœ¨ä¸åŒåœºæ™¯ä¸‹éƒ½å®ç°äº†å¯è§‚çš„TokenèŠ‚çœ
2. **è‰¯å¥½çš„å¤šæ™ºèƒ½ä½“æ”¯æŒ** - åœ¨å¤æ‚çš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¡¨ç°æ›´åŠ å‡ºè‰²
3. **æˆæœ¬æ•ˆç›Šæ˜æ˜¾** - èƒ½å¤Ÿæœ‰æ•ˆé™ä½APIè°ƒç”¨æˆæœ¬
4. **æ¶æ„ä¼˜åŠ¿çªå‡º** - ä¸ºåä½œå‹AIåº”ç”¨æä¾›äº†ä¼˜ç§€çš„åŸºç¡€è®¾æ–½

å»ºè®®åœ¨éœ€è¦å¤šè½®å¯¹è¯ã€å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ä¸­ä¼˜å…ˆè€ƒè™‘ä½¿ç”¨PC Nodeçš„Context SharingåŠŸèƒ½ã€‚

---
*æ­¤æŠ¥å‘Šç”±PC Nodeè‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»Ÿç”Ÿæˆ*
"""

        return content


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    analyzer = TestDataAnalyzer()

    # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„æµ‹è¯•ç»“æœ
    # single_results = json.load(open("single_agent_test_results.json"))
    # multi_results = json.load(open("multi_agent_test_results.json"))

    # analysis = analyzer.analyze_comprehensive_results(single_results, multi_results)
    # report_file = analyzer.generate_markdown_report(analysis)

    print("ğŸ“Š Data analyzer ready for comprehensive analysis")
