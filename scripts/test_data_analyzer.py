#!/usr/bin/env python3
"""
æ•°æ®åˆ†æå™¨ - ç»¼åˆåˆ†ææµ‹è¯•ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
"""

import json
import time
from typing import List, Dict, Any
import numpy as np
from datetime import datetime
from utils.performance_metrics import MetricsCalculator
from utils.chart_generator import ChartGenerator


class TestDataAnalyzer:
    def __init__(self):
        self.chart_generator = ChartGenerator()

    def analyze_comprehensive_results(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“æµ‹è¯•ç»“æœ"""

        print("ğŸ“Š Analyzing comprehensive test results...")

        # ğŸ” æ·»åŠ è°ƒè¯•ï¼šç¡®è®¤ä¼ å…¥çš„å‚æ•°ç±»å‹
        print(f"ğŸ” Received single_agent type: {single_agent_results.get('test_type', 'unknown')}")
        print(f"ğŸ” Received multi_agent type: {multi_agent_results.get('test_type', 'unknown')}")

        # æå–å…³é”®æŒ‡æ ‡
        print("ğŸ” Extracting single agent summary...")
        single_agent_summary = self._extract_summary_metrics(single_agent_results)

        print("ğŸ” Extracting multi agent summary...")
        multi_agent_summary = self._extract_summary_metrics(multi_agent_results)

        analysis = {
            "test_summary": {
                "single_agent": single_agent_summary,
                "multi_agent": multi_agent_summary
            },
            "performance_insights": self._generate_performance_insights(
                single_agent_results, multi_agent_results
            ),
            "cost_analysis": self._calculate_comprehensive_cost_analysis(
                single_agent_results, multi_agent_results
            ),
            "recommendations": self._generate_recommendations(
                single_agent_results, multi_agent_results
            ),
            "scalability_analysis": self._analyze_scalability(
                single_agent_results, multi_agent_results
            )
        }

        # ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾è¡¨
        chart_path = self._generate_comprehensive_chart(
            single_agent_results, multi_agent_results
        )
        analysis["comprehensive_chart"] = chart_path

        return analysis

    def _extract_summary_metrics(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æµ‹è¯•ç»“æœçš„å…³é”®æŒ‡æ ‡"""
        if not test_results:
            return {
                "without_context_sharing": {"avg_tokens": 0, "total_tokens": 0, "avg_response_time": 0},
                "with_context_sharing": {"avg_tokens": 0, "total_tokens": 0, "avg_response_time": 0},
                "improvements": {"token_efficiency": 0, "token_savings": 0, "response_time_change": 0}
            }

        # ğŸ” å¤„ç†ä¸åŒç±»å‹çš„æµ‹è¯•ç»“æœæ•°æ®ç»“æ„
        test_type = test_results.get("test_type", "unknown")
        print(f"ğŸ” Processing {test_type}")

        # æ ¹æ®æµ‹è¯•ç±»å‹é€‰æ‹©æ­£ç¡®çš„æ•°æ®å­—æ®µ
        if test_type == "multi_agent_comparison":
            # å¤šæ™ºèƒ½ä½“ä½¿ç”¨ overall_comparison å­—æ®µ
            comparison = test_results.get("overall_comparison", {})
        else:
            # å•æ™ºèƒ½ä½“ä½¿ç”¨ comparison å­—æ®µ
            comparison = test_results.get("comparison", {})

        if not comparison:
            return {
                "without_context_sharing": {"avg_tokens": 0, "total_tokens": 0, "avg_response_time": 0},
                "with_context_sharing": {"avg_tokens": 0, "total_tokens": 0, "avg_response_time": 0},
                "improvements": {"token_efficiency": 0, "token_savings": 0, "response_time_change": 0}
            }

        scenarios = comparison.get("scenarios", {})
        improvements = comparison.get("improvements", {})

        print(f"ğŸ” {test_type} - scenarios keys: {list(scenarios.keys())}")
        print(f"ğŸ” {test_type} - token_efficiency: {improvements.get('token_efficiency', 0):.1f}%")

        # æå–åœºæ™¯æ•°æ®
        without_data = scenarios.get("Without Context Sharing", {})
        with_data = scenarios.get("With Context Sharing", {})

        return {
            "without_context_sharing": {
                "avg_tokens": without_data.get("avg_tokens", 0),
                "total_tokens": without_data.get("total_tokens", 0),
                "avg_response_time": without_data.get("avg_response_time", 0)
            },
            "with_context_sharing": {
                "avg_tokens": with_data.get("avg_tokens", 0),
                "total_tokens": with_data.get("total_tokens", 0),
                "avg_response_time": with_data.get("avg_response_time", 0)
            },
            "improvements": {
                "token_efficiency": improvements.get("token_efficiency", 0),
                "token_savings": improvements.get("token_savings", 0),
                "response_time_change": improvements.get("response_time", 0)
            }
        }

    def _generate_performance_insights(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        insights = {
            "context_sharing_effectiveness": {
                "single_agent_efficiency": single_summary.get("improvements", {}).get("token_efficiency", 0),
                "multi_agent_efficiency": multi_summary.get("improvements", {}).get("token_efficiency", 0),
                "scalability_factor": 0
            },
            "complexity_impact": {
                "single_agent_avg_tokens": single_summary.get("with_context_sharing", {}).get("avg_tokens", 0),
                "multi_agent_avg_tokens": multi_summary.get("with_context_sharing", {}).get("avg_tokens", 0),
                "complexity_overhead": 0
            }
        }

        # è®¡ç®—å¯æ‰©å±•æ€§å› å­
        single_efficiency = insights["context_sharing_effectiveness"]["single_agent_efficiency"]
        multi_efficiency = insights["context_sharing_effectiveness"]["multi_agent_efficiency"]

        if single_efficiency > 0:
            insights["context_sharing_effectiveness"]["scalability_factor"] = multi_efficiency / single_efficiency

        # è®¡ç®—å¤æ‚åº¦å¼€é”€
        single_tokens = insights["complexity_impact"]["single_agent_avg_tokens"]
        multi_tokens = insights["complexity_impact"]["multi_agent_avg_tokens"]

        if single_tokens > 0:
            insights["complexity_impact"]["complexity_overhead"] = (multi_tokens - single_tokens) / single_tokens * 100

        return insights

    def _calculate_comprehensive_cost_analysis(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæˆæœ¬åˆ†æ - åŸºäºTokenèŠ‚çœé‡"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        # å•æ™ºèƒ½ä½“TokenèŠ‚çœ
        single_tokens_without = single_summary.get("without_context_sharing", {}).get("total_tokens", 0)
        single_tokens_with = single_summary.get("with_context_sharing", {}).get("total_tokens", 0)
        single_token_savings = single_tokens_without - single_tokens_with

        # å¤šæ™ºèƒ½ä½“TokenèŠ‚çœ
        multi_tokens_without = multi_summary.get("without_context_sharing", {}).get("total_tokens", 0)
        multi_tokens_with = multi_summary.get("with_context_sharing", {}).get("total_tokens", 0)
        multi_token_savings = multi_tokens_without - multi_tokens_with

        # è®¡ç®—æ¯è½®TokenèŠ‚çœ
        single_rounds = 19  # å•æ™ºèƒ½ä½“æµ‹è¯•è½®æ•°
        multi_rounds = 20   # å¤šæ™ºèƒ½ä½“æµ‹è¯•è½®æ•°

        single_per_round_token_savings = single_token_savings / single_rounds if single_rounds > 0 else 0
        multi_per_round_token_savings = multi_token_savings / multi_rounds if multi_rounds > 0 else 0
        average_per_round_token_savings = (single_per_round_token_savings + multi_per_round_token_savings) / 2

        return {
            "single_agent": {
                "tokens_without_context": single_tokens_without,
                "tokens_with_context": single_tokens_with,
                "token_savings": single_token_savings,
                "savings_percentage": (single_token_savings / single_tokens_without * 100) if single_tokens_without > 0 else 0,
                "per_round_token_savings": single_per_round_token_savings
            },
            "multi_agent": {
                "tokens_without_context": multi_tokens_without,
                "tokens_with_context": multi_tokens_with,
                "token_savings": multi_token_savings,
                "savings_percentage": (multi_token_savings / multi_tokens_without * 100) if multi_tokens_without > 0 else 0,
                "per_round_token_savings": multi_per_round_token_savings
            },
            "total_savings": {
                "tokens": single_token_savings + multi_token_savings,
                "percentage": ((single_token_savings + multi_token_savings) / (single_tokens_without + multi_tokens_without) * 100) if (single_tokens_without + multi_tokens_without) > 0 else 0
            },
            "average_per_round_token_savings": average_per_round_token_savings
        }

    def _generate_recommendations(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, List[str]]:
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        recommendations = {
            "when_to_use_context_sharing": [],
            "performance_optimization": [],
            "cost_optimization": [],
            "architecture_considerations": []
        }

        # åŸºäºæ€§èƒ½æ•°æ®ç”Ÿæˆå»ºè®®
        single_efficiency = single_summary.get("improvements", {}).get("token_efficiency", 0)
        multi_efficiency = multi_summary.get("improvements", {}).get("token_efficiency", 0)

        if single_efficiency > 20:
            recommendations["when_to_use_context_sharing"].append(
                f"âœ… å•æ™ºèƒ½ä½“åœºæ™¯æ˜¾ç¤º {single_efficiency:.1f}% çš„Tokenæ•ˆç‡æå‡ï¼Œæ¨èä½¿ç”¨"
            )

        if multi_efficiency > 30:
            recommendations["when_to_use_context_sharing"].append(
                f"âœ… å¤šæ™ºèƒ½ä½“åœºæ™¯æ˜¾ç¤º {multi_efficiency:.1f}% çš„Tokenæ•ˆç‡æå‡ï¿½ï¿½ï¿½å¼ºçƒˆæ¨èä½¿ç”¨"
            )

        if multi_efficiency > single_efficiency * 1.2:
            recommendations["architecture_considerations"].append(
                "ğŸ—ï¸  Context Sharingåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¡¨ç°æ›´ä¼˜ï¼Œé€‚åˆåä½œå‹åº”ç”¨"
            )

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if single_efficiency > 0:
            recommendations["performance_optimization"].append(
                "âš¡ Context Sharingæœ‰æ•ˆå‡å°‘Tokenä½¿ç”¨ï¼Œæå‡å“åº”æ•ˆç‡"
            )

        # æˆæœ¬ä¼˜åŒ–å»ºè®®
        recommendations["cost_optimization"].append(
            "ğŸ’° é€šè¿‡Context Sharingå¯æ˜¾è‘—é™ä½APIè°ƒç”¨æˆæœ¬"
        )

        return recommendations

    def _analyze_scalability(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æå¯æ‰©å±•æ€§"""

        single_summary = self._extract_summary_metrics(single_agent_results)
        multi_summary = self._extract_summary_metrics(multi_agent_results)

        # è®¡ç®—æ‰©å±•æ•ˆç‡
        single_tokens = single_summary.get("with_context_sharing", {}).get("avg_tokens", 0)
        multi_tokens = multi_summary.get("with_context_sharing", {}).get("avg_tokens", 0)

        scalability_efficiency = 0
        if single_tokens > 0:
            scalability_efficiency = (1 - (multi_tokens / single_tokens)) * 100

        return {
            "scaling_efficiency": scalability_efficiency,
            "single_agent_baseline": single_tokens,
            "multi_agent_performance": multi_tokens,
            "scalability_rating": self._rate_scalability(scalability_efficiency),
            "recommendations": self._scalability_recommendations(scalability_efficiency)
        }

    def _rate_scalability(self, efficiency: float) -> str:
        """è¯„çº§å¯æ‰©å±•æ€§"""
        if efficiency >= 20:
            return "â­â­â­â­â­ ä¼˜ç§€"
        elif efficiency >= 10:
            return "â­â­â­â­ è‰¯å¥½"
        elif efficiency >= 0:
            return "â­â­â­ ä¸€èˆ¬"
        else:
            return "â­â­ éœ€è¦ä¼˜åŒ–"

    def _scalability_recommendations(self, efficiency: float) -> List[str]:
        """å¯æ‰©å±•æ€§å»ºè®®"""
        if efficiency >= 15:
            return ["ğŸš€ Context Sharingå±•ç°å‡ºè‰²çš„æ‰©å±•æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²"]
        elif efficiency >= 5:
            return ["ğŸ“ˆ Context Sharingå…·å¤‡è‰¯å¥½æ‰©å±•æ½œåŠ›ï¼Œå»ºè®®åœ¨å¤æ‚åœºæ™¯ä¸­ä½¿ç”¨"]
        else:
            return ["ğŸ”§ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–Context Sharingç®—æ³•ä»¥æå‡æ‰©å±•æ•ˆç‡"]

    def _generate_comprehensive_chart(
        self,
        single_agent_results: Dict[str, Any],
        multi_agent_results: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾è¡¨"""

        # è¿™é‡Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç»¼åˆçš„ä»ªè¡¨æ¿å›¾è¡¨
        # æš‚æ—¶è¿”å›è·¯å¾„ï¼Œå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„å¯è§†åŒ–
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"comprehensive_analysis_{timestamp}.png"

    def generate_markdown_report(
        self,
        analysis: Dict[str, Any],
        output_file: str = None
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"pc_node_analysis_report_{timestamp}.md"

        report_content = self._build_markdown_content(analysis)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return output_file

    def _build_markdown_content(self, analysis: Dict[str, Any]) -> str:
        """æ„å»ºMarkdownæŠ¥å‘Šå†…å®¹"""

        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

        content = f"""# PC Node æ€§èƒ½åˆ†ææŠ¥å‘Š

*ç”Ÿæˆæ—¶é—´: {timestamp}*

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ

### å•æ™ºèƒ½ä½“æµ‹è¯•ç»“æœ
- **Tokenæ•ˆç‡æå‡**: {analysis['test_summary']['single_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%
- **TokenèŠ‚çœ**: {analysis['test_summary']['single_agent'].get('improvements', {}).get('token_savings', 0)} tokens

![å•æ™ºèƒ½ä½“æ€§èƒ½å¯¹æ¯”](images/single_agent_comparison.png)

### å¤šæ™ºèƒ½ä½“æµ‹è¯•ç»“æœ
- **Tokenæ•ˆç‡æå‡**: {analysis['test_summary']['multi_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%
- **TokenèŠ‚çœ**: {analysis['test_summary']['multi_agent'].get('improvements', {}).get('token_savings', 0)} tokens

![å¤šæ™ºèƒ½ä½“æ€§èƒ½å¯¹æ¯”](images/multi_agent_comparison.png)

## ğŸ’¡ æ€§èƒ½æ´å¯Ÿ

### Context Sharingæ•ˆæœ
- **å•æ™ºèƒ½ä½“æ•ˆç‡**: {analysis['performance_insights']['context_sharing_effectiveness']['single_agent_efficiency']:.1f}%
- **å¤šæ™ºèƒ½ä½“æ•ˆç‡**: {analysis['performance_insights']['context_sharing_effectiveness']['multi_agent_efficiency']:.1f}%
- **å¯æ‰©å±•æ€§å› å­**: {analysis['performance_insights']['context_sharing_effectiveness']['scalability_factor']:.2f}

### å¤æ‚åº¦å½±å“
- **å•æ™ºèƒ½ä½“å¹³å‡Token**: {analysis['performance_insights']['complexity_impact']['single_agent_avg_tokens']:.0f} tokens
- **å¤šæ™ºèƒ½ä½“å¹³å‡Token**: {analysis['performance_insights']['complexity_impact']['multi_agent_avg_tokens']:.0f} tokens
- **å¤æ‚åº¦å¼€é”€**: {analysis['performance_insights']['complexity_impact']['complexity_overhead']:.1f}%

## ğŸ’° TokenèŠ‚çœåˆ†æ

### å•æ™ºèƒ½ä½“åœºæ™¯
- **ä¸ä½¿ç”¨Context Sharing**: {analysis['cost_analysis']['single_agent']['tokens_without_context']:,.0f} tokens
- **ä½¿ç”¨Context Sharing**: {analysis['cost_analysis']['single_agent']['tokens_with_context']:,.0f} tokens
- **èŠ‚çœ**: {analysis['cost_analysis']['single_agent']['token_savings']:,.0f} tokens ({analysis['cost_analysis']['single_agent']['savings_percentage']:.1f}%)
- **æ¯è½®èŠ‚çœ**: {analysis['cost_analysis']['single_agent']['per_round_token_savings']:.0f} tokens

### å¤šæ™ºèƒ½ä½“åœºæ™¯
- **ä¸ä½¿ç”¨Context Sharing**: {analysis['cost_analysis']['multi_agent']['tokens_without_context']:,.0f} tokens
- **ä½¿ç”¨Context Sharing**: {analysis['cost_analysis']['multi_agent']['tokens_with_context']:,.0f} tokens
- **èŠ‚çœ**: {analysis['cost_analysis']['multi_agent']['token_savings']:,.0f} tokens ({analysis['cost_analysis']['multi_agent']['savings_percentage']:.1f}%)
- **æ¯è½®èŠ‚çœ**: {analysis['cost_analysis']['multi_agent']['per_round_token_savings']:.0f} tokens

### æ€»ä½“èŠ‚çœ
- **æ€»TokenèŠ‚çœ**: {analysis['cost_analysis']['total_savings']['tokens']:,.0f} tokens
- **æ€»èŠ‚çœæ¯”ä¾‹**: {analysis['cost_analysis']['total_savings']['percentage']:.1f}%
- **å¹³å‡æ¯è½®èŠ‚çœ**: {analysis['cost_analysis']['average_per_round_token_savings']:.0f} tokens

## ğŸ¯ ä½¿ç”¨å»ºè®®

### ä½•æ—¶ä½¿ç”¨Context Sharing
"""

        # æ·»åŠ å»ºè®®å†…å®¹
        for recommendation in analysis['recommendations']['when_to_use_context_sharing']:
            content += f"- {recommendation}\n"

        content += "\n### æ€§èƒ½ä¼˜åŒ–å»ºè®®\n"
        for recommendation in analysis['recommendations']['performance_optimization']:
            content += f"- {recommendation}\n"

        content += "\n### æˆæœ¬ä¼˜åŒ–å»ºè®®\n"
        for recommendation in analysis['recommendations']['cost_optimization']:
            content += f"- {recommendation}\n"

        content += "\n### æ¶æ„è€ƒè™‘\n"
        for recommendation in analysis['recommendations']['architecture_considerations']:
            content += f"- {recommendation}\n"

        # æ·»åŠ æ€»ç»“ï¼Œä½¿ç”¨tokensè¡¨ç¤º
        content += f"""
## ğŸ“‹ æ€»ç»“

æœ¬æ¬¡æµ‹è¯•éªŒè¯äº†PC Nodeåœ¨Context Sharingæ–¹é¢çš„æ€§èƒ½è¡¨ç°ï¼š

1. **å•æ™ºèƒ½ä½“åœºæ™¯**: Context Sharingå¸¦æ¥äº†{analysis['test_summary']['single_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%çš„Tokenæ•ˆç‡æå‡
2. **å¤šæ™ºèƒ½ä½“åœºæ™¯**: Context Sharingå¸¦æ¥äº†{analysis['test_summary']['multi_agent'].get('improvements', {}).get('token_efficiency', 0):.1f}%çš„Tokenæ•ˆç‡æå‡
3. **TokenèŠ‚çœ**: å¹³å‡æ¯è½®å¯¹è¯èŠ‚çœ{analysis['cost_analysis']['average_per_round_token_savings']:.0f} tokens
4. **è§„æ¨¡æ•ˆåº”**: æ¯1000è½®å¯¹è¯èŠ‚çœ{analysis['cost_analysis']['average_per_round_token_savings'] * 1000:.0f} tokens

---
*æŠ¥å‘Šç”±PC Nodeè‡ªåŠ¨ç”Ÿæˆ | æ•°æ®æ¥æº: ç»¼åˆæ€§èƒ½æµ‹è¯•*
"""

        return content


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    analyzer = TestDataAnalyzer()

    # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„æµ‹è¯•ç»“æœ
    # single_results = json.load(open("single_agent_test_results.json"))
    # multi_results = json.load(open("multi_agent_test_results.json"))

    # analysis = analyzer.analyze_comprehensive_results(single_results, multi_results)
    # report_file = analyzer.generate_markdown_report(analysis)

    print("ğŸ“Š Data analyzer ready for comprehensive analysis")
