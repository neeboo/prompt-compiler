#!/usr/bin/env python3
"""
å•æ™ºèƒ½ä½“å¤šè½®å¯¹è¯æµ‹è¯•
æ¯”è¾ƒå¼€å¯å’Œå…³é—­Context Sharingçš„æ€§èƒ½å·®å¼‚
"""

import os
import json
import time
from typing import List, Dict, Any
from datetime import datetime
from utils.pc_client import PCNodeClient, ConversationResult
from utils.performance_metrics import MetricsCalculator
from utils.chart_generator import ChartGenerator


class SingleAgentTester:
    def __init__(self, config_path: str = None):
        if config_path is None:
            # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç›¸å¯¹è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(script_dir, "configs", "single_agent_config.json")

        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        self.client = PCNodeClient(self.config['test_config']['base_url'])
        self.chart_generator = ChartGenerator()

    def test_without_context_sharing(self) -> List[ConversationResult]:
        """æµ‹è¯•ä¸å¼€å¯Context Sharingçš„åœºæ™¯"""
        print("ğŸ” Testing WITHOUT Context Sharing...")

        conversation = self.config['conversation_scenarios']['web_scraping']
        results = []

        # ä¸ä½¿ç”¨context sharingï¼Œæ¯æ¬¡éƒ½ä¼ é€’å®Œæ•´çš„æ¶ˆæ¯å†å²
        messages = []

        for i, message in enumerate(conversation):
            print(f"   Turn {i+1}/{len(conversation)}: {message[:50]}...")

            messages.append({"role": "user", "content": message})

            result = self.client.chat_completion(
                messages=messages,
                context_sharing=False,
                model=self.config['test_config']['model'],
                temperature=self.config['test_config']['temperature'],
                max_tokens=self.config['test_config']['max_tokens']
            )

            results.append(result)

            if result.content:
                messages.append({"role": "assistant", "content": result.content})

            time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«

        return results

    def test_with_context_sharing(self) -> List[ConversationResult]:
        """æµ‹è¯•å¼€å¯Context Sharingçš„åœºæ™¯"""
        print("ğŸ” Testing WITH Context Sharing...")

        conversation = self.config['conversation_scenarios']['web_scraping']
        results = []
        agent_id = "single_agent_tester"

        for i, message in enumerate(conversation):
            print(f"   Turn {i+1}/{len(conversation)}: {message[:50]}...")

            # ä½¿ç”¨context sharingï¼Œåªéœ€è¦å‘é€å½“å‰æ¶ˆæ¯
            result = self.client.chat_completion(
                messages=[{"role": "user", "content": message}],
                agent_id=agent_id,
                context_sharing=True,
                model=self.config['test_config']['model'],
                temperature=self.config['test_config']['temperature'],
                max_tokens=self.config['test_config']['max_tokens']
            )

            results.append(result)
            time.sleep(0.1)

        return results

    def run_comparison_test(self) -> Dict[str, Any]:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ Starting Single Agent Comparison Test...")

        # å¥åº·æ£€æŸ¥
        if not self.client.health_check():
            print("âŒ PC Node is not healthy, aborting test")
            return {}

        # è¿è¡Œä¸¤ä¸ªåœºæ™¯
        start_time = time.time()

        results_without = self.test_without_context_sharing()
        results_with = self.test_with_context_sharing()

        total_time = time.time() - start_time

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        comparison = MetricsCalculator.compare_scenarios(
            results_without,
            results_with,
            "Without Context Sharing",
            "With Context Sharing"
        )

        # ç”Ÿæˆå›¾è¡¨
        chart_path = self.chart_generator.generate_comparison_chart(
            results_without,
            results_with,
            "Without Context Sharing",
            "With Context Sharing",
            "Single Agent Performance Comparison",
            "single_agent_comparison.png"
        )

        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            "test_type": "single_agent_comparison",
            "total_test_time": total_time,
            "conversation_rounds": len(results_without),
            "comparison": comparison,
            "chart_path": chart_path,
            "raw_results": {
                "without_context_sharing": [
                    {
                        "turn": i+1,
                        "tokens": r.tokens,
                        "response_time": r.response_time,
                        "compression_ratio": r.compression_ratio,
                        "context_size": r.context_size
                    }
                    for i, r in enumerate(results_without)
                ],
                "with_context_sharing": [
                    {
                        "turn": i+1,
                        "tokens": r.tokens,
                        "response_time": r.response_time,
                        "compression_ratio": r.compression_ratio,
                        "context_size": r.context_size
                    }
                    for i, r in enumerate(results_with)
                ]
            }
        }

        self._print_summary(comparison)

        return result

    def _print_summary(self, comparison: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“Š SINGLE AGENT TEST SUMMARY")
        print("="*60)

        scenarios = comparison["scenarios"]
        improvements = comparison.get("improvements", {})  # ä½¿ç”¨å®‰å…¨è®¿é—®

        print(f"ğŸ”¸ Without Context Sharing:")
        print(f"   Average Tokens: {scenarios.get('Without Context Sharing', {}).get('avg_tokens', 0):.1f}")
        print(f"   Total Tokens: {scenarios.get('Without Context Sharing', {}).get('total_tokens', 0)}")
        print(f"   Average Response Time: {scenarios.get('Without Context Sharing', {}).get('avg_response_time', 0):.3f}s")

        print(f"\nğŸ”¸ With Context Sharing:")
        print(f"   Average Tokens: {scenarios.get('With Context Sharing', {}).get('avg_tokens', 0):.1f}")
        print(f"   Total Tokens: {scenarios.get('With Context Sharing', {}).get('total_tokens', 0)}")
        print(f"   Average Response Time: {scenarios.get('With Context Sharing', {}).get('avg_response_time', 0):.3f}s")

        print(f"\nğŸ’¡ Performance Improvements:")
        print(f"   Token Efficiency: {improvements.get('token_efficiency', 0):.1f}%")
        print(f"   Token Savings: {improvements.get('token_savings', 0)} tokens")
        print(f"   Response Time Change: {improvements.get('response_time', 0):.1f}%")

        better_scenario = comparison["summary"]["better_scenario"]
        improvement_pct = comparison["summary"]["token_improvement_pct"]

        if better_scenario == "With Context Sharing":
            print(f"\nâœ… Context Sharing shows {improvement_pct:.1f}% better token efficiency!")
        else:
            print(f"\nâš ï¸  Traditional approach shows {improvement_pct:.1f}% better performance")

        print("="*60)


if __name__ == "__main__":
    tester = SingleAgentTester()
    results = tester.run_comparison_test()

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"single_agent_test_results_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ Results saved to: {output_file}")
