#!/usr/bin/env python3
"""
ä¸»æµ‹è¯•è¿è¡Œå™¨ - åè°ƒæ‰€æœ‰æµ‹è¯•å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import json
import time
import os
import sys
from datetime import datetime
from typing import Dict, Any
from test_single_agent import SingleAgentTester
from test_multi_agent import MultiAgentTester
from test_data_analyzer import TestDataAnalyzer


class TestRunner:
    def __init__(self):
        self.results_dir = "test_results"
        os.makedirs(self.results_dir, exist_ok=True)

        self.single_agent_tester = SingleAgentTester()
        self.multi_agent_tester = MultiAgentTester()
        self.data_analyzer = TestDataAnalyzer()

    def run_all_tests(self, skip_single=False, skip_multi=False) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""

        print("ğŸš€ Starting PC Node Comprehensive Performance Testing")
        print("=" * 60)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results = {
            "test_run_id": f"pc_node_test_{timestamp}",
            "start_time": datetime.now().isoformat(),
            "tests_completed": [],
            "errors": []
        }

        # è¿è¡Œå•æ™ºèƒ½ä½“æµ‹è¯•
        if not skip_single:
            try:
                print("\nğŸ¤– Running Single Agent Tests...")
                single_results = self.single_agent_tester.run_comparison_test()
                results["single_agent_results"] = single_results
                results["tests_completed"].append("single_agent")

                # ä¿å­˜å•ç‹¬çš„ç»“æœæ–‡ä»¶
                single_file = os.path.join(self.results_dir, f"single_agent_{timestamp}.json")
                with open(single_file, 'w', encoding='utf-8') as f:
                    json.dump(single_results, f, ensure_ascii=False, indent=2)

                print(f"âœ… Single agent test completed - saved to {single_file}")

            except Exception as e:
                error_msg = f"Single agent test failed: {str(e)}"
                print(f"âŒ {error_msg}")
                results["errors"].append(error_msg)

        # è¿è¡Œå¤šæ™ºèƒ½ä½“æµ‹è¯•
        if not skip_multi:
            try:
                print("\nğŸ‘¥ Running Multi-Agent Tests...")
                multi_results = self.multi_agent_tester.run_comparison_test()
                results["multi_agent_results"] = multi_results
                results["tests_completed"].append("multi_agent")

                # ä¿å­˜å•ç‹¬çš„ç»“æœæ–‡ä»¶
                multi_file = os.path.join(self.results_dir, f"multi_agent_{timestamp}.json")
                with open(multi_file, 'w', encoding='utf-8') as f:
                    json.dump(multi_results, f, ensure_ascii=False, indent=2)

                print(f"âœ… Multi-agent test completed - saved to {multi_file}")

            except Exception as e:
                error_msg = f"Multi-agent test failed: {str(e)}"
                print(f"âŒ {error_msg}")
                results["errors"].append(error_msg)

        # ç”Ÿæˆç»¼åˆåˆ†æ
        if "single_agent" in results["tests_completed"] and "multi_agent" in results["tests_completed"]:
            try:
                print("\nğŸ“Š Generating Comprehensive Analysis...")

                # ğŸ” è°ƒè¯•ï¼šæ‰“å°ä¼ é€’ç»™åˆ†æå™¨çš„ï¿½ï¿½ï¿½æ®ç±»å‹
                print(f"ğŸ” Single agent test type: {results['single_agent_results'].get('test_type', 'unknown')}")
                print(f"ğŸ” Multi agent test type: {results['multi_agent_results'].get('test_type', 'unknown')}")

                analysis = self.data_analyzer.analyze_comprehensive_results(
                    results["single_agent_results"],
                    results["multi_agent_results"]
                )
                results["comprehensive_analysis"] = analysis

                # ç”ŸæˆMarkdownæŠ¥å‘Š
                report_file = os.path.join(self.results_dir, f"analysis_report_{timestamp}.md")
                self.data_analyzer.generate_markdown_report(analysis, report_file)

                print(f"âœ… Comprehensive analysis completed - report saved to {report_file}")

            except Exception as e:
                error_msg = f"Comprehensive analysis failed: {str(e)}"
                print(f"âŒ {error_msg}")
                results["errors"].append(error_msg)

        results["end_time"] = datetime.now().isoformat()
        results["total_duration"] = (datetime.fromisoformat(results["end_time"]) -
                                   datetime.fromisoformat(results["start_time"])).total_seconds()

        # ä¿å­˜å®Œæ•´çš„æµ‹è¯•ç»“æœ
        complete_results_file = os.path.join(self.results_dir, f"complete_test_results_{timestamp}.json")
        with open(complete_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        self._print_final_summary(results)

        return results

    def _print_final_summary(self, results: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæµ‹è¯•æ€»ç»“"""

        print("\n" + "="*80)
        print("ğŸ† PC NODE COMPREHENSIVE TEST SUMMARY")
        print("="*80)

        print(f"ğŸ“… Test Run ID: {results['test_run_id']}")
        print(f"â±ï¸  Total Duration: {results['total_duration']:.2f} seconds")
        print(f"âœ… Tests Completed: {', '.join(results['tests_completed'])}")

        if results['errors']:
            print(f"âŒ Errors: {len(results['errors'])}")
            for error in results['errors']:
                print(f"   - {error}")

        # å¦‚æœæœ‰ç»¼åˆåˆ†æï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        if "comprehensive_analysis" in results:
            analysis = results["comprehensive_analysis"]

            print("\nğŸ“Š KEY PERFORMANCE INDICATORS:")

            if "test_summary" in analysis:
                single_efficiency = analysis["test_summary"]["single_agent"].get("improvements", {}).get("token_efficiency", 0)
                multi_efficiency = analysis["test_summary"]["multi_agent"].get("improvements", {}).get("token_efficiency", 0)

                print(f"ğŸ¤– Single Agent Token Efficiency: {single_efficiency:.1f}%")
                print(f"ğŸ‘¥ Multi-Agent Token Efficiency: {multi_efficiency:.1f}%")

            if "cost_analysis" in analysis:
                total_savings = analysis["cost_analysis"]["total_savings"]["percentage"]
                print(f"ğŸ’° Overall Cost Savings: {total_savings:.1f}%")

            if "scalability_analysis" in analysis:
                scalability_rating = analysis["scalability_analysis"]["scalability_rating"]
                print(f"ğŸ“ˆ Scalability Rating: {scalability_rating}")

        print("\nğŸ¯ FINAL VERDICT:")
        if results["tests_completed"] and not results["errors"]:
            if "comprehensive_analysis" in results:
                single_eff = results["comprehensive_analysis"]["test_summary"]["single_agent"].get("improvements", {}).get("token_efficiency", 0)
                multi_eff = results["comprehensive_analysis"]["test_summary"]["multi_agent"].get("improvements", {}).get("token_efficiency", 0)

                if single_eff > 20 and multi_eff > 20:
                    print("ğŸŒŸ EXCELLENT: PC Node Context Sharing shows outstanding performance!")
                    print("   Recommended for production use in both single and multi-agent scenarios.")
                elif single_eff > 10 or multi_eff > 10:
                    print("âœ… GOOD: PC Node Context Sharing demonstrates clear benefits.")
                    print("   Recommended for scenarios with moderate to high conversation complexity.")
                else:
                    print("âš ï¸  MIXED: PC Node Context Sharing shows some benefits but needs optimization.")
            else:
                print("âœ… Tests completed successfully but comprehensive analysis unavailable.")
        else:
            print("âŒ INCOMPLETE: Some tests failed or were skipped. Review errors above.")

        print("="*80)
        print(f"ğŸ“ All results saved in: {self.results_dir}/")
        print("="*80)


def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""

    import argparse

    parser = argparse.ArgumentParser(description="PC Node Performance Test Runner")
    parser.add_argument("--skip-single", action="store_true",
                       help="Skip single agent tests")
    parser.add_argument("--skip-multi", action="store_true",
                       help="Skip multi-agent tests")
    parser.add_argument("--quick", action="store_true",
                       help="Run quick tests (reduced conversation rounds)")

    args = parser.parse_args()

    if args.quick:
        print("âš¡ Quick test mode enabled - using reduced conversation rounds")
        # å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹é…ç½®ä»¥å‡å°‘æµ‹è¯•è½®æ•°

    runner = TestRunner()

    try:
        results = runner.run_all_tests(
            skip_single=args.skip_single,
            skip_multi=args.skip_multi
        )

        # æ ¹æ®æµ‹è¯•ç»“æœç¡®å®šé€€å‡ºä»£ç 
        if results["errors"]:
            sys.exit(1)  # æœ‰é”™è¯¯æ—¶é€€å‡ºä»£ç ä¸º1
        else:
            sys.exit(0)  # æˆåŠŸæ—¶é€€å‡ºä»£ç ä¸º0

    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
