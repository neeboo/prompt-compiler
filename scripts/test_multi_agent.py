#!/usr/bin/env python3
"""
å¤šæ™ºèƒ½ä½“å¤šè½®å¯¹è¯æµ‹è¯•
æ¯”è¾ƒå¼€å¯å’Œå…³é—­Context Sharingåœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸‹çš„æ€§èƒ½å·®å¼‚
"""

import os
import json
import time
from typing import List, Dict, Any
from utils.pc_client import PCNodeClient, ConversationResult
from utils.performance_metrics import MetricsCalculator
from utils.chart_generator import ChartGenerator


class MultiAgentTester:
    def __init__(self, config_path: str = None):
        if config_path is None:
            # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç›¸å¯¹è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(script_dir, "configs", "multi_agent_config.json")

        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        self.client = PCNodeClient(self.config['test_config']['base_url'])
        self.chart_generator = ChartGenerator()

    def test_without_context_sharing(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¸å¼€å¯Context Sharingçš„å¤šæ™ºèƒ½ä½“åœºæ™¯"""
        print("ğŸ” Testing Multi-Agent WITHOUT Context Sharing...")

        conversation = self.config['conversation_scenarios']['enterprise_project']
        agent_results = {}
        # ğŸ” æ–°å¢ï¼šè®°å½•è‡ªç„¶å¯¹è¯é¡ºåºçš„æ•°æ®
        conversation_timeline = []

        # è®°å½•æ‰€æœ‰å¯¹è¯å†å²ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½éœ€è¦å®Œæ•´å†å²
        full_conversation_history = []

        for i, turn in enumerate(conversation):
            agent_id = turn['agent']
            message = turn['message']

            print(f"   Turn {i+1}/{len(conversation)} - {self.config['agents'][agent_id]['name']}: {message[:50]}...")

            # æ¯æ¬¡éƒ½ä¼ é€’å®Œæ•´çš„å¯¹è¯å†å²
            full_conversation_history.append({"role": "user", "content": message})

            # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯æ¬¡ä¼ é€’çš„æ¶ˆæ¯æ•°é‡å’Œæ€»é•¿åº¦
            total_chars = sum(len(msg['content']) for msg in full_conversation_history)
            print(f"      ï¿½ï¿½ï¿½ å‘é€ {len(full_conversation_history)} æ¡æ¶ˆæ¯ï¼Œæ€»é•¿åº¦: {total_chars} å­—ç¬¦")

            result = self.client.chat_completion(
                messages=full_conversation_history.copy(),
                context_sharing=False,
                model=self.config['test_config']['model'],
                temperature=self.config['test_config']['temperature'],
                max_tokens=self.config['test_config']['max_tokens']
            )

            # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„ç»“æœ
            if agent_id not in agent_results:
                agent_results[agent_id] = []
            agent_results[agent_id].append(result)

            # ğŸ” æ–°å¢ï¼šè®°å½•åˆ°å¯¹è¯æ—¶é—´çº¿
            conversation_timeline.append({
                "global_turn": i + 1,
                "agent_id": agent_id,
                "agent_name": self.config['agents'][agent_id]['name'],
                "tokens": result.tokens,
                "response_time": result.response_time,
                "compression_ratio": result.compression_ratio,
                "context_size": result.context_size,
                "messages_sent": len(full_conversation_history),
                "total_chars": total_chars
            })

            # å°†å›å¤æ·»åŠ åˆ°å†å²ä¸­
            if result.content:
                full_conversation_history.append({"role": "assistant", "content": result.content})
                # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ·»åŠ å›å¤åçš„æ¶ˆæ¯æ•°é‡
                print(f"      âœ… æ”¶åˆ°å›å¤ï¼Œæ·»åŠ åå…± {len(full_conversation_history)} æ¡æ¶ˆæ¯ï¼Œtokens: {result.tokens}")
            else:
                print(f"      âŒ å›å¤ä¸ºï¿½ï¿½ï¼æ²¡æœ‰æ·»åŠ åˆ°å†å²è®°å½•ä¸­")

            time.sleep(0.1)

        return {"agent_results": agent_results, "conversation_timeline": conversation_timeline}

    def test_with_context_sharing(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼€å¯Context Sharingçš„å¤šæ™ºèƒ½ä½“åœºæ™¯"""
        print("ğŸ” Testing Multi-Agent WITH Context Sharing...")

        conversation = self.config['conversation_scenarios']['enterprise_project']
        agent_results = {}
        # ğŸ” æ–°å¢ï¼šè®°å½•è‡ªç„¶å¯¹è¯é¡ºåºçš„æ•°æ®
        conversation_timeline = []

        for i, turn in enumerate(conversation):
            agent_id = turn['agent']
            message = turn['message']

            print(f"   Turn {i+1}/{len(conversation)} - {self.config['agents'][agent_id]['name']}: {message[:50]}...")

            # ä½¿ç”¨context sharingï¼Œåªéœ€è¦å‘é€å½“å‰æ¶ˆæ¯
            result = self.client.chat_completion(
                messages=[{"role": "user", "content": message}],
                agent_id=agent_id,
                context_sharing=True,
                model=self.config['test_config']['model'],
                temperature=self.config['test_config']['temperature'],
                max_tokens=self.config['test_config']['max_tokens']
            )

            # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„ç»“æœ
            if agent_id not in agent_results:
                agent_results[agent_id] = []
            agent_results[agent_id].append(result)

            # ğŸ” æ–°å¢ï¼šè®°å½•åˆ°å¯¹è¯æ—¶é—´çº¿
            conversation_timeline.append({
                "global_turn": i + 1,
                "agent_id": agent_id,
                "agent_name": self.config['agents'][agent_id]['name'],
                "tokens": result.tokens,
                "response_time": result.response_time,
                "compression_ratio": result.compression_ratio,
                "context_size": result.context_size,
                "messages_sent": 1,  # Context sharing åªå‘é€1æ¡æ¶ˆæ¯
                "total_chars": len(message)
            })

            time.sleep(0.1)

        return {"agent_results": agent_results, "conversation_timeline": conversation_timeline}

    def run_comparison_test(self) -> Dict[str, Any]:
        """è¿è¡Œå¤šæ™ºèƒ½ä½“å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ Starting Multi-Agent Comparison Test...")

        # å¥åº·æ£€æŸ¥
        if not self.client.health_check():
            print("âŒ PC Node is not healthy, aborting test")
            return {}

        # è¿è¡Œä¸¤ä¸ªåœºæ™¯
        start_time = time.time()

        results_without = self.test_without_context_sharing()
        results_with = self.test_with_context_sharing()

        total_time = time.time() - start_time

        # æå–agent_resultså’Œconversation_timeline
        agent_results_without = results_without["agent_results"]
        agent_results_with = results_with["agent_results"]
        timeline_without = results_without["conversation_timeline"]
        timeline_with = results_with["conversation_timeline"]

        # å°†å¤šæ™ºèƒ½ä½“ç»“æœåˆå¹¶ä¸ºå•ä¸€åˆ—è¡¨è¿›è¡Œå¯¹æ¯”
        # ğŸ” ä¿®æ­£ï¼šæŒ‰ç…§è‡ªç„¶å¯¹è¯é¡ºåºåˆå¹¶ï¼Œè€Œä¸æ˜¯æŒ‰agentåˆ†ç»„
        all_results_without = []
        all_results_with = []

        # ä»å¯¹è¯ï¿½ï¿½ï¿½é—´çº¿ä¸­æŒ‰ç…§è‡ªç„¶é¡ºåºæå–ConversationResultå¯¹è±¡
        for turn_data in timeline_without:
            agent_id = turn_data["agent_id"]
            global_turn = turn_data["global_turn"]
            # æ‰¾åˆ°å¯¹åº”agentåœ¨è¯¥ï¿½ï¿½ï¿½æ¬¡çš„ConversationResult
            agent_turn_index = sum(1 for t in timeline_without[:global_turn-1] if t["agent_id"] == agent_id)
            result = agent_results_without[agent_id][agent_turn_index]
            all_results_without.append(result)

        for turn_data in timeline_with:
            agent_id = turn_data["agent_id"]
            global_turn = turn_data["global_turn"]
            # æ‰¾åˆ°å¯¹åº”agentåœ¨è¯¥è½®æ¬¡çš„ConversationResult
            agent_turn_index = sum(1 for t in timeline_with[:global_turn-1] if t["agent_id"] == agent_id)
            result = agent_results_with[agent_id][agent_turn_index]
            all_results_with.append(result)

        # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        overall_comparison = MetricsCalculator.compare_scenarios(
            all_results_without,
            all_results_with,
            "Without Context Sharing",
            "With Context Sharing"
        )

        # åªç”Ÿæˆä¸»è¦çš„å¯¹æ¯”å›¾è¡¨ï¼Œç§»é™¤å“åº”æ—¶é—´ç»´åº¦
        overall_chart_path = self.chart_generator.generate_comparison_chart(
            all_results_without,
            all_results_with,
            "Without Context Sharing",
            "With Context Sharing",
            "Multi-Agent Performance Comparison (Token Efficiency Focus)",
            "multi_agent_comparison.png"
        )

        # è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“çš„æ€§èƒ½æŒ‡æ ‡
        agent_metrics = {}
        for agent_id in self.config['agents']:
            agent_name = self.config['agents'][agent_id]['name']

            if agent_id in agent_results_without and agent_id in agent_results_with:
                agent_comparison = MetricsCalculator.compare_scenarios(
                    agent_results_without[agent_id],
                    agent_results_with[agent_id],
                    f"{agent_name} (Without)",
                    f"{agent_name} (With)"
                )
                agent_metrics[agent_id] = {
                    "name": agent_name,
                    "comparison": agent_comparison,
                    "turns_without": len(agent_results_without[agent_id]),
                    "turns_with": len(agent_results_with[agent_id])
                }

        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            "test_type": "multi_agent_comparison",
            "total_test_time": total_time,
            "total_conversation_turns": len(self.config['conversation_scenarios']['enterprise_project']),
            "participating_agents": len(self.config['agents']),
            "overall_comparison": overall_comparison,
            "agent_specific_metrics": agent_metrics,
            "chart_paths": {
                "overall_comparison": overall_chart_path
            },
            # ğŸ” æ–°ï¿½ï¿½ï¼šåŒ…å«å¯¹è¯æ—¶é—´çº¿æ•°æ®
            "conversation_timelines": {
                "without_context_sharing": timeline_without,
                "with_context_sharing": timeline_with
            },
            "raw_results": {
                "without_context_sharing": {
                    agent_id: [
                        {
                            "turn": i+1,
                            "tokens": r.tokens,
                            "response_time": r.response_time,
                            "compression_ratio": r.compression_ratio,
                            "context_size": r.context_size
                        }
                        for i, r in enumerate(results)
                    ]
                    for agent_id, results in agent_results_without.items()
                },
                "with_context_sharing": {
                    agent_id: [
                        {
                            "turn": i+1,
                            "tokens": r.tokens,
                            "response_time": r.response_time,
                            "compression_ratio": r.compression_ratio,
                            "context_size": r.context_size
                        }
                        for i, r in enumerate(results)
                    ]
                    for agent_id, results in agent_results_with.items()
                }
            }
        }

        self._print_summary(overall_comparison, agent_metrics)

        return result

    def _print_summary(self, overall_comparison: Dict[str, Any], agent_metrics: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*70)
        print("ğŸ“Š MULTI-AGENT TEST SUMMARY")
        print("="*70)

        # æ•´ä½“æ€§èƒ½å¯¹æ¯”
        scenarios = overall_comparison["scenarios"]
        improvements = overall_comparison["improvements"]

        print(f"ğŸ”¸ Overall Performance (Without Context Sharing):")
        print(f"   Average Tokens: {scenarios['Without Context Sharing']['avg_tokens']:.1f}")
        print(f"   Total Tokens: {scenarios['Without Context Sharing']['total_tokens']}")
        print(f"   Average Response Time: {scenarios['Without Context Sharing']['avg_response_time']:.3f}s")

        print(f"\nğŸ”¸ Overall Performance (With Context Sharing):")
        print(f"   Average Tokens: {scenarios['With Context Sharing']['avg_tokens']:.1f}")
        print(f"   Total Tokens: {scenarios['With Context Sharing']['total_tokens']}")
        print(f"   Average Response Time: {scenarios['With Context Sharing']['avg_response_time']:.3f}s")

        print(f"\nğŸ’¡ Overall Improvements:")
        print(f"   Token Efficiency: {improvements['token_efficiency']:.1f}%")
        print(f"   Token Savings: {improvements['token_savings']} tokens")
        print(f"   Response Time Change: {improvements['response_time']:.1f}%")

        # å„æ™ºèƒ½ä½“è¯¦ç»†è¡¨ç°
        print(f"\nğŸ‘¥ Agent-Specific Performance:")
        for agent_id, metrics in agent_metrics.items():
            agent_improvements = metrics['comparison']['improvements']
            print(f"   ğŸ“‹ {metrics['name']}:")
            print(f"      Token Efficiency: {agent_improvements['token_efficiency']:.1f}%")
            print(f"      Token Savings: {agent_improvements['token_savings']} tokens")
            print(f"      Turns: {metrics['turns_with']}")

        better_scenario = overall_comparison["summary"]["better_scenario"]
        improvement_pct = overall_comparison["summary"]["token_improvement_pct"]

        if better_scenario == "With Context Sharing":
            print(f"\nâœ… Multi-Agent Context Sharing shows {improvement_pct:.1f}% better token efficiency!")
            print("ğŸ¤ Successful cross-agent knowledge sharing demonstrated!")
        else:
            print(f"\nâš ï¸  Traditional approach shows {improvement_pct:.1f}% better performance")

        print("="*70)


if __name__ == "__main__":
    tester = MultiAgentTester()
    results = tester.run_comparison_test()

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"multi_agent_test_results_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ Results saved to: {output_file}")
