//! State database - RocksDB storage for compiled states

use crate::{Result, StorageError};
use prompt_compiler_crypto::{Hash, Signature};
use rocksdb::{ColumnFamilyDescriptor, Options, DB};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// è¯­ä¹‰å‹ç¼©çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SemanticChunk {
    pub id: String,
    pub content_hash: String, // æ”¹ä¸ºStringç±»å‹é¿å…Hashç±»å‹é—®é¢˜
    pub compressed_embedding: Vec<f32>, // å‹ç¼©åçš„è¯­ä¹‰è¡¨ç¤º
    pub original_size: usize,
    pub compressed_size: usize,
    pub compression_ratio: f32,
    pub access_count: u64,
    pub last_accessed: u64,
    pub semantic_tags: Vec<String>,
}

/// ä¸Šä¸‹æ–‡æ³¨å…¥ç­–ç•¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ContextInjectionStrategy {
    /// ç›´æ¥å‘é€ç»™LLM
    DirectSend { max_tokens: usize },
    /// æ³¨å…¥åˆ°è¯­ä¹‰ç©ºé—´
    SemanticInject { similarity_threshold: f32 },
    /// æ··åˆç­–ç•¥
    Hybrid {
        direct_ratio: f32,
        semantic_ratio: f32
    },
}

/// Compiled state for storage
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StoredState {
    pub version: String,
    pub content: Vec<u8>,
    pub created_at: u64,
    pub metadata: HashMap<String, String>,
    /// å…³è”çš„è¯­ä¹‰å—ID
    pub semantic_chunks: Vec<String>,
    /// æ³¨å…¥ç­–ç•¥
    pub injection_strategy: ContextInjectionStrategy,
}

/// Signed compiled state for storage
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SignedStoredState {
    pub state: StoredState,
    pub hash: Hash,
    pub signature: Option<Signature>,
}

/// Compilation statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompilationStats {
    pub total_compilations: u64,
    pub avg_compilation_time_ms: f64,
    pub avg_weight_updates_per_prompt: f32,
    pub most_common_targets: Vec<String>,
    pub convergence_rate: f32,
    /// è¯­ä¹‰å‹ç¼©ç»Ÿè®¡
    pub semantic_compression_ratio: f32,
    pub avg_chunk_reuse_rate: f32,
    pub context_injection_success_rate: f32,
}

/// RocksDB state database
pub struct StateDB {
    db: DB,
    cf_handles: HashMap<String, rocksdb::ColumnFamily>,
}

impl StateDB {
    /// Create new state database
    pub fn new(path: &str) -> Result<Self> {
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.create_missing_column_families(true);

        let cf_descriptors = vec![
            ColumnFamilyDescriptor::new("states", Options::default()),
            ColumnFamilyDescriptor::new("versions", Options::default()),
            ColumnFamilyDescriptor::new("semantic_chunks", Options::default()),
            ColumnFamilyDescriptor::new("embeddings", Options::default()),
            ColumnFamilyDescriptor::new("stats", Options::default()),
        ];

        let db = DB::open_cf_descriptors(&opts, path, cf_descriptors)?;

        let cf_handles = HashMap::new();

        Ok(StateDB { db, cf_handles })
    }

    /// å­˜å‚¨è¯­ä¹‰å—
    pub fn store_semantic_chunk(&self, chunk: &SemanticChunk) -> Result<()> {
        let cf = self.db.cf_handle("semantic_chunks")
            .ok_or_else(|| StorageError::ColumnFamilyNotFound("semantic_chunks".to_string()))?;

        let serialized = bincode::serialize(chunk)?;

        self.db.put_cf(&cf, &chunk.id, &serialized)?;

        println!("ğŸ“¦ å­˜å‚¨è¯­ä¹‰å—: {} (å‹ç¼©æ¯”: {:.2}%)",
                chunk.id, chunk.compression_ratio * 100.0);
        Ok(())
    }

    /// æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ä¸Šä¸‹æ–‡
    pub fn retrieve_by_semantic_similarity(
        &self,
        query_embedding: &[f32],
        threshold: f32,
        limit: usize
    ) -> Result<Vec<SemanticChunk>> {
        let cf = self.db.cf_handle("semantic_chunks")
            .ok_or_else(|| StorageError::ColumnFamilyNotFound("semantic_chunks".to_string()))?;

        let mut results = Vec::new();
        let iter = self.db.iterator_cf(&cf, rocksdb::IteratorMode::Start);

        for item in iter {
            let (_, value) = item?;
            let chunk: SemanticChunk = bincode::deserialize(&value)?;

            // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            let similarity = Self::cosine_similarity(query_embedding, &chunk.compressed_embedding);

            if similarity >= threshold {
                results.push((chunk, similarity));
            }
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        results.truncate(limit);

        Ok(results.into_iter().map(|(chunk, _)| chunk).collect())
    }

    /// å®æ–½ä¸Šä¸‹æ–‡æ³¨å…¥ç­–ç•¥
    pub fn inject_context(
        &self,
        base_prompt: &str,
        strategy: &ContextInjectionStrategy,
        query_embedding: &[f32]
    ) -> Result<String> {
        match strategy {
            ContextInjectionStrategy::DirectSend { max_tokens } => {
                // ç›´æ¥æ‹¼æ¥åˆ°promptä¸­
                let chunks = self.retrieve_by_semantic_similarity(query_embedding, 0.7, 5)?;
                let context = self.compile_context_for_direct_send(&chunks, *max_tokens)?;
                Ok(format!("{}\n\n# ç›¸å…³ä¸Šä¸‹æ–‡:\n{}\n\n# æŸ¥è¯¢:\n{}",
                          context, self.format_chunks_for_llm(&chunks), base_prompt))
            },

            ContextInjectionStrategy::SemanticInject { similarity_threshold } => {
                // æ³¨å…¥åˆ°è¯­ä¹‰ç©ºé—´ï¼ˆæ¨¡æ‹Ÿï¼‰
                let chunks = self.retrieve_by_semantic_similarity(
                    query_embedding,
                    *similarity_threshold,
                    10
                )?;

                let semantic_context = self.create_semantic_injection(&chunks)?;
                Ok(format!("{}âš¡è¯­ä¹‰æ³¨å…¥: {} ä¸ªç›¸å…³å—âš¡\n{}",
                          semantic_context, chunks.len(), base_prompt))
            },

            ContextInjectionStrategy::Hybrid { direct_ratio, semantic_ratio: _ } => {
                // æ··åˆç­–ç•¥
                let chunks = self.retrieve_by_semantic_similarity(query_embedding, 0.6, 8)?;
                let direct_count = (chunks.len() as f32 * direct_ratio) as usize;

                let direct_chunks = &chunks[..direct_count.min(chunks.len())];
                let semantic_chunks = &chunks[direct_count..];

                let direct_context = self.format_chunks_for_llm(direct_chunks);
                let semantic_injection = self.create_semantic_injection(semantic_chunks)?;

                Ok(format!("{}ğŸ”€æ··åˆæ³¨å…¥ğŸ”€\n# ç›´æ¥ä¸Šä¸‹æ–‡:\n{}\n\n# æŸ¥è¯¢:\n{}",
                          semantic_injection, direct_context, base_prompt))
            }
        }
    }

    /// å‹ç¼©å¹¶å­˜å‚¨æ–°çš„ä¸Šä¸‹æ–‡
    pub fn compress_and_store_context(
        &self,
        content: &str,
        embedding: Vec<f32>
    ) -> Result<SemanticChunk> {
        let original_size = content.len();

        // ç®€å•çš„å‹ç¼©æ¨¡æ‹Ÿï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„è¯­ä¹‰å‹ç¼©ï¼‰
        let compressed_embedding = Self::compress_embedding(&embedding, 128)?;
        let compressed_size = compressed_embedding.len() * 4; // f32 = 4 bytes

        let chunk = SemanticChunk {
            id: format!("chunk_{}", chrono::Utc::now().timestamp_millis()),
            content_hash: format!("{:x}", md5::compute(content.as_bytes())), // ä½¿ç”¨md5ä½œä¸ºç®€å•hash
            compressed_embedding,
            original_size,
            compressed_size,
            compression_ratio: compressed_size as f32 / original_size as f32,
            access_count: 0,
            last_accessed: chrono::Utc::now().timestamp() as u64,
            semantic_tags: Self::extract_semantic_tags(content),
        };

        self.store_semantic_chunk(&chunk)?;
        Ok(chunk)
    }

    /// æ›´æ–°ç¼–è¯‘ç»Ÿè®¡
    pub fn update_compilation_stats(&self, stats: &CompilationStats) -> Result<()> {
        let cf = self.db.cf_handle("stats")
            .ok_or_else(|| StorageError::ColumnFamilyNotFound("stats".to_string()))?;

        let serialized = bincode::serialize(stats)?;

        self.db.put_cf(&cf, b"compilation_stats", &serialized)?;

        println!("ğŸ“Š æ›´æ–°ç¼–è¯‘ç»Ÿè®¡: æ”¶æ•›ç‡ {:.2}%, å‹ç¼©æ¯” {:.2}%",
                stats.convergence_rate * 100.0,
                stats.semantic_compression_ratio * 100.0);
        Ok(())
    }

    /// å­˜å‚¨ç¼–è¯‘çŠ¶æ€ï¼ˆä¸ºSDKå…¼å®¹æ€§æ·»åŠ ï¼‰
    pub fn store_state(&self, hash: &str, state: &StoredState) -> Result<()> {
        let cf = self.db.cf_handle("states")
            .ok_or_else(|| StorageError::ColumnFamilyNotFound("states".to_string()))?;

        let serialized = bincode::serialize(state)?;
        self.db.put_cf(&cf, hash.as_bytes(), &serialized)?;

        println!("ğŸ’¾ å­˜å‚¨ç¼–è¯‘çŠ¶æ€: {}", hash);
        Ok(())
    }

    /// åˆ—å‡ºæ‰€æœ‰å“ˆå¸Œå€¼ï¼ˆä¸ºSDKå…¼å®¹æ€§æ·»åŠ ï¼‰
    pub fn list_all_hashes(&self) -> Result<Vec<String>> {
        let cf = self.db.cf_handle("states")
            .ok_or_else(|| StorageError::ColumnFamilyNotFound("states".to_string()))?;

        let mut hashes = Vec::new();
        let iter = self.db.iterator_cf(&cf, rocksdb::IteratorMode::Start);

        for item in iter {
            let (key, _) = item?;
            if let Ok(hash_str) = String::from_utf8(key.to_vec()) {
                hashes.push(hash_str);
            }
        }

        Ok(hashes)
    }

    // è¾…åŠ©æ–¹æ³•
    fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
        let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
        let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

        if norm_a == 0.0 || norm_b == 0.0 {
            0.0
        } else {
            dot_product / (norm_a * norm_b)
        }
    }

    fn compress_embedding(embedding: &[f32], target_dim: usize) -> Result<Vec<f32>> {
        // ç®€å•çš„ç»´åº¦å‹ç¼©ï¼ˆPCAçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
        if embedding.len() <= target_dim {
            return Ok(embedding.to_vec());
        }

        let chunk_size = embedding.len() / target_dim;
        let compressed: Vec<f32> = embedding
            .chunks(chunk_size)
            .map(|chunk| chunk.iter().sum::<f32>() / chunk.len() as f32)
            .collect();

        Ok(compressed)
    }

    fn extract_semantic_tags(content: &str) -> Vec<String> {
        // ç®€å•çš„å…³é”®è¯æå–
        content
            .split_whitespace()
            .filter(|word| word.len() > 4)
            .take(5)
            .map(|s| s.to_lowercase())
            .collect()
    }

    fn format_chunks_for_llm(&self, chunks: &[SemanticChunk]) -> String {
        chunks.iter()
            .enumerate()
            .map(|(i, chunk)| {
                format!("## ä¸Šä¸‹æ–‡ç‰‡æ®µ {}\næ ‡ç­¾: {:?}\nä½¿ç”¨æ¬¡æ•°: {}\n",
                       i + 1, chunk.semantic_tags, chunk.access_count)
            })
            .collect::<Vec<_>>()
            .join("\n")
    }

    fn create_semantic_injection(&self, chunks: &[SemanticChunk]) -> Result<String> {
        let total_compression = chunks.iter()
            .map(|c| c.compression_ratio)
            .sum::<f32>() / chunks.len() as f32;

        Ok(format!("ğŸ§ [è¯­ä¹‰ç©ºé—´æ³¨å…¥: {} å—, å¹³å‡å‹ç¼©æ¯” {:.1}%]",
                  chunks.len(), total_compression * 100.0))
    }

    fn compile_context_for_direct_send(&self, chunks: &[SemanticChunk], max_tokens: usize) -> Result<String> {
        let mut context = String::new();
        let mut token_count = 0;

        for chunk in chunks {
            let chunk_info = format!("å‹ç¼©æ¯”: {:.1}% | æ ‡ç­¾: {:?}",
                                   chunk.compression_ratio * 100.0,
                                   chunk.semantic_tags);

            if token_count + chunk_info.len() > max_tokens {
                break;
            }

            context.push_str(&chunk_info);
            context.push('\n');
            token_count += chunk_info.len();
        }

        Ok(context)
    }
}
