use crate::enhanced::{EnhancedPromptAnalyzer, AdvancedAnalyzerConfig, DetailedConvergenceAnalysis, ConvergenceType};
use crate::storage::{PromptAnalysisStorage, AnalysisRecord, OptimizationRecord};
use crate::test_data::TEST_CASES;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::time::{SystemTime, UNIX_EPOCH};

/// Prompt åŸºå‡†æµ‹è¯•å¥—ä»¶
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptBenchmark {
    pub name: String,
    pub prompt: String,
    pub task: String,
    pub expected_quality: QualityLevel,
    pub category: PromptCategory,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QualityLevel {
    Excellent,  // é¢„æœŸå¿«é€Ÿæ”¶æ•›
    Good,       // é¢„æœŸå¹³ç¨³æ”¶æ•›
    Fair,       // é¢„æœŸç¼“æ…¢æ”¶æ•›
    Poor,       // é¢„æœŸä¸æ”¶æ•›æˆ–å‘æ•£
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PromptCategory {
    Simple,           // ç®€å•æŒ‡ä»¤
    Structured,       // ç»“æ„åŒ–æŒ‡ä»¤
    Professional,     // ä¸“ä¸šè§’è‰²è®¾å®š
    Complex,          // å¤æ‚å¤šæ­¥éª¤
    Creative,         // åˆ›æ„ç±»ä»»åŠ¡
    Analytical,       // åˆ†æç±»ä»»åŠ¡
}

/// åŸºå‡†æµ‹è¯•ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResult {
    pub benchmark: PromptBenchmark,
    pub analysis: DetailedConvergenceAnalysis,
    pub quality_score: f32,
    pub performance_rating: String,
    pub recommendations: Vec<String>,
    pub timestamp: u64,
}

/// Prompt è´¨é‡è¯„ä¼°å™¨
pub struct PromptQualityAssessor {
    storage: Option<PromptAnalysisStorage>,
    benchmarks: Vec<PromptBenchmark>,
}

impl PromptQualityAssessor {
    /// åˆ›å»ºæ–°çš„è´¨é‡è¯„ä¼°å™¨
    pub fn new(db_path: Option<&str>) -> Result<Self, Box<dyn std::error::Error>> {
        let storage = if let Some(path) = db_path {
            Some(PromptAnalysisStorage::new(path)?)
        } else {
            None
        };

        let benchmarks = Self::load_default_benchmarks();

        Ok(Self {
            storage,
            benchmarks,
        })
    }

    /// åŠ è½½é»˜è®¤åŸºå‡†æµ‹è¯•ç”¨ä¾‹
    fn load_default_benchmarks() -> Vec<PromptBenchmark> {
        vec![
            // ç®€å•æŒ‡ä»¤ç±»
            PromptBenchmark {
                name: "simple_analysis".to_string(),
                prompt: "åˆ†ææ•°æ®".to_string(),
                task: "æ•°æ®åˆ†æ".to_string(),
                expected_quality: QualityLevel::Poor,
                category: PromptCategory::Simple,
            },
            PromptBenchmark {
                name: "basic_request".to_string(),
                prompt: "å†™ä»£ç ".to_string(),
                task: "ç¼–ç¨‹ä»»åŠ¡".to_string(),
                expected_quality: QualityLevel::Poor,
                category: PromptCategory::Simple,
            },

            // ç»“æ„åŒ–æŒ‡ä»¤ç±»
            PromptBenchmark {
                name: "structured_analysis".to_string(),
                prompt: "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š1) ç†è§£é—®é¢˜ 2) æ”¶é›†æ•°æ® 3) å¾—å‡ºç»“è®º".to_string(),
                task: "æ•°æ®åˆ†æ".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Structured,
            },
            PromptBenchmark {
                name: "formatted_response".to_string(),
                prompt: "è¯·æŒ‰æ­¤æ ¼å¼å›ç­”ï¼š\né—®é¢˜ï¼š[é‡è¿°é—®é¢˜]\nåˆ†æï¼š[è¯¦ç»†åˆ†æ]\nç»“è®ºï¼š[æ˜ç¡®ç»“è®º]".to_string(),
                task: "é—®é¢˜è§£ç­”".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Structured,
            },

            // ä¸“ä¸šè§’è‰²ç±»
            PromptBenchmark {
                name: "professional_analyst".to_string(),
                prompt: "ä½œä¸ºä¸“ä¸šæ•°æ®åˆ†æå¸ˆï¼Œè¯·è¯¦ç»†åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼å¹¶æä¾›å¯æ‰§è¡Œå»ºè®®".to_string(),
                task: "ç”¨æˆ·è¡Œä¸ºåˆ†æ".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Professional,
            },
            PromptBenchmark {
                name: "expert_consultant".to_string(),
                prompt: "ä½œä¸ºèµ„æ·±æŠ€æœ¯é¡¾é—®ï¼Œè¯·è¯„ä¼°æŠ€æœ¯æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®".to_string(),
                task: "æŠ€æœ¯å’¨è¯¢".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Professional,
            },

            // å¤æ‚å¤šæ­¥éª¤ç±»
            PromptBenchmark {
                name: "complex_workflow".to_string(),
                prompt: "è¯·ä½œä¸ºä¸“ä¸šåˆ†æå¸ˆï¼Œè¯¦ç»†åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿ï¼Œè¯„ä¼°æ½œåœ¨é£é™©ï¼Œå¹¶åˆ¶å®šå…·ä½“çš„ä¼˜åŒ–ç­–ç•¥å’Œå®æ–½è®¡åˆ’".to_string(),
                task: "ç»¼åˆåˆ†æ".to_string(),
                expected_quality: QualityLevel::Fair,
                category: PromptCategory::Complex,
            },

            // æ¥è‡ªç°æœ‰æµ‹è¯•ç”¨ä¾‹çš„ä¼˜è´¨prompt
            PromptBenchmark {
                name: "test_case_structured".to_string(),
                prompt: TEST_CASES[0].good_prompt.to_string(),
                task: TEST_CASES[0].task.to_string(),
                expected_quality: QualityLevel::Excellent,
                category: PromptCategory::Structured,
            },

            // æ–°å¢ï¼šå®é™…ä¸šåŠ¡åœºæ™¯çš„æµ‹è¯•ç”¨ä¾‹
            PromptBenchmark {
                name: "customer_service_excellent".to_string(),
                prompt: "ä½œä¸ºä¸“ä¸šå®¢æœä»£è¡¨ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†å®¢æˆ·å’¨è¯¢ï¼š1) ä¸»åŠ¨é—®å€™å¹¶ç¡®è®¤é—®é¢˜ 2) è¯¦ç»†äº†è§£å®¢æˆ·éœ€æ±‚å’ŒèƒŒæ™¯ 3) æä¾›ä¸ªæ€§åŒ–è§£å†³æ–¹æ¡ˆ 4) ç¡®è®¤å®¢æˆ·æ»¡æ„åº¦å¹¶è®°å½•åé¦ˆã€‚å¤„ç†è¿‡ç¨‹ä¸­è¯·ä¿æŒè€å¿ƒã€ä¸“ä¸šå’ŒåŒç†å¿ƒã€‚".to_string(),
                task: "å®¢æˆ·æœåŠ¡".to_string(),
                expected_quality: QualityLevel::Excellent,
                category: PromptCategory::Professional,
            },

            PromptBenchmark {
                name: "creative_writing_good".to_string(),
                prompt: "è¯·åˆ›ä½œä¸€ç¯‡å¼•äººå…¥èƒœçš„æ•…äº‹ï¼ŒåŒ…å«ï¼š1) æ¸…æ™°çš„ä¸»è§’è®¾å®š 2) å¼•äººå…¥èƒœçš„å¼€å¤´ 3) åˆç†çš„æƒ…èŠ‚å‘å±• 4) æ„å¤–çš„è½¬æŠ˜ 5) ä»¤äººæ»¡æ„çš„ç»“å±€ã€‚æ•…äº‹é£æ ¼è¦ç”ŸåŠ¨æœ‰è¶£ï¼Œå­—æ•°æ§åˆ¶åœ¨800å­—ä»¥å†…ã€‚".to_string(),
                task: "åˆ›æ„å†™ä½œ".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Creative,
            },

            PromptBenchmark {
                name: "code_review_excellent".to_string(),
                prompt: "ä½œä¸ºèµ„æ·±å¼€å‘å·¥ç¨‹å¸ˆï¼Œè¯·æŒ‰ä»¥ä¸‹æ ‡å‡†è¿›è¡Œä»£ç å®¡æŸ¥ï¼š1) æ£€æŸ¥ä»£ç é€»è¾‘æ­£ç¡®æ€§ 2) è¯„ä¼°æ€§èƒ½å’Œå®‰å…¨æ€§ 3) éªŒè¯ä»£ç è§„èŒƒå’Œå¯è¯»æ€§ 4) æå‡ºå…·ä½“æ”¹è¿›å»ºè®® 5) ç»™å‡ºæ€»ä½“è¯„åˆ†å’Œç†ç”±ã€‚è¯·ç”¨ä¸“ä¸šæœ¯è¯­å¹¶æä¾›å¯æ‰§è¡Œçš„æ”¹è¿›æ–¹æ¡ˆã€‚".to_string(),
                task: "ä»£ç å®¡æŸ¥".to_string(),
                expected_quality: QualityLevel::Excellent,
                category: PromptCategory::Professional,
            },

            PromptBenchmark {
                name: "analytical_thinking_good".to_string(),
                prompt: "è¯·è¿ç”¨æ‰¹åˆ¤æ€§æ€ç»´åˆ†æç»™å®šé—®é¢˜ï¼š1) è¯†åˆ«æ ¸å¿ƒé—®é¢˜å’Œå…³é”®å‡è®¾ 2) æ”¶é›†ç›¸å…³æ•°æ®å’Œè¯æ® 3) è€ƒè™‘å¤šä¸ªè§†è§’å’Œæ½œåœ¨åè§ 4) é€»è¾‘æ¨ç†å¾—å‡ºç»“è®º 5) è¯„ä¼°ç»“è®ºçš„å¯é æ€§å’Œå±€é™æ€§ã€‚".to_string(),
                task: "æ‰¹åˆ¤æ€§åˆ†æ".to_string(),
                expected_quality: QualityLevel::Good,
                category: PromptCategory::Analytical,
            },

            PromptBenchmark {
                name: "poor_quality_example".to_string(),
                prompt: "åšä¸€ä¸‹".to_string(),
                task: "æœªæŒ‡å®šä»»åŠ¡".to_string(),
                expected_quality: QualityLevel::Poor,
                category: PromptCategory::Simple,
            },
        ]
    }

    /// è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•
    pub fn run_full_benchmark(&mut self) -> Result<Vec<BenchmarkResult>, Box<dyn std::error::Error>> {
        println!("ğŸš€ å¼€å§‹è¿è¡Œ Prompt è´¨é‡åŸºå‡†æµ‹è¯•");
        println!("æµ‹è¯•ç”¨ä¾‹æ•°é‡: {}", self.benchmarks.len());
        println!("{}", "=".repeat(60));

        let mut results = Vec::new();

        // å…‹éš†åŸºå‡†æµ‹è¯•ä»¥é¿å…å€Ÿç”¨å†²çª
        let benchmarks = self.benchmarks.clone();

        for (i, benchmark) in benchmarks.iter().enumerate() {
            println!("\nğŸ“Š è¿è¡ŒåŸºå‡†æµ‹è¯• {}/{}: {}", i + 1, benchmarks.len(), benchmark.name);
            println!("ç±»åˆ«: {:?} | é¢„æœŸè´¨é‡: {:?}", benchmark.category, benchmark.expected_quality);

            let result = self.evaluate_prompt(benchmark)?;
            results.push(result);

            // ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if let Some(ref _storage) = self.storage {
                // è¿™é‡Œå¯ä»¥ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
                println!("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“");
            }
        }

        self.print_benchmark_summary(&results);
        Ok(results)
    }

    /// è¯„ä¼°å•ä¸ª prompt
    fn evaluate_prompt(&mut self, benchmark: &PromptBenchmark) -> Result<BenchmarkResult, Box<dyn std::error::Error>> {
        // é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡åŸºäº prompt å¤æ‚åº¦
        let learning_rate = match benchmark.category {
            PromptCategory::Simple => 0.03,
            PromptCategory::Structured => 0.05,
            PromptCategory::Professional => 0.08,
            PromptCategory::Complex => 0.1,
            PromptCategory::Creative => 0.06,
            PromptCategory::Analytical => 0.07,
        };

        let config = AdvancedAnalyzerConfig {
            learning_rate,
            regularization_strength: 0.05,
            max_iterations: 30,
            convergence_threshold: 0.01,
            adaptive_learning_rate: true,
        };

        let mut analyzer = EnhancedPromptAnalyzer::new(config)?;
        let analysis = analyzer.deep_convergence_analysis(&benchmark.prompt, &benchmark.task)?;

        // è®¡ç®—è´¨é‡å¾—åˆ†
        let quality_score = self.calculate_quality_score(&analysis);
        let performance_rating = self.rate_performance(&analysis, &benchmark.expected_quality);
        let recommendations = self.generate_recommendations(&analysis, benchmark);

        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();

        Ok(BenchmarkResult {
            benchmark: benchmark.clone(),
            analysis,
            quality_score,
            performance_rating,
            recommendations,
            timestamp,
        })
    }

    /// è®¡ç®—ç»¼åˆè´¨é‡å¾—åˆ†
    fn calculate_quality_score(&self, analysis: &DetailedConvergenceAnalysis) -> f32 {
        let mut score = 0.0;

        // 1. æ”¶æ•›æˆåŠŸå¥–åŠ± (35%)
        if analysis.converged {
            score += 0.35;
        } else if analysis.final_convergence_rate > 0.3 {
            score += 0.25; // éƒ¨åˆ†æ”¶æ•›å¥–åŠ±
        } else if analysis.final_convergence_rate > 0.1 {
            score += 0.15; // è½»å¾®æ”¶æ•›å¥–åŠ±
        }

        // 2. æ”¶æ•›ç±»å‹è¯„åˆ† (30%) - è°ƒæ•´æƒé‡åˆ†é…
        let convergence_bonus = match analysis.convergence_type {
            ConvergenceType::Rapid => 0.30,      // ç«‹å³æ”¶æ•›æ˜¯å¥½çš„
            ConvergenceType::Steady => 0.28,     // å¹³ç¨³æ”¶æ•›ä¹Ÿå¾ˆå¥½
            ConvergenceType::Slow => 0.22,       // ç¼“æ…¢æ”¶æ•›å¯æ¥å—
            ConvergenceType::Oscillating => 0.15, // éœ‡è¡æ”¶æ•›ä¸€èˆ¬
            ConvergenceType::Stable => 0.10,     // ç¨³å®šä½†æœªæ”¶æ•›è¾ƒå·®
            ConvergenceType::Diverging => 0.0,   // å‘æ•£æœ€å·®
        };
        score += convergence_bonus;

        // 3. æ•ˆæœå¾—åˆ†å³°å€¼ (20%) - æ›´é‡è§†æœ€é«˜æ•ˆæœ
        if let Some(max_score) = analysis.effectiveness_scores.iter().max_by(|a, b| a.partial_cmp(b).unwrap()) {
            let normalized_effectiveness = (max_score * 2.5).min(0.20); // æé«˜æ•ˆæœæƒé‡
            score += normalized_effectiveness;
        }

        // 4. æ”¶æ•›é€Ÿåº¦å¥–åŠ± (10%) - æ–°å¢é€Ÿåº¦è¯„åˆ†
        if let Some(convergence_steps) = analysis.convergence_steps {
            let speed_bonus = match convergence_steps {
                1..=3 => 0.10,    // å¿«é€Ÿæ”¶æ•›
                4..=10 => 0.08,   // ä¸­ç­‰é€Ÿåº¦
                11..=20 => 0.05,  // è¾ƒæ…¢
                _ => 0.02,        // å¾ˆæ…¢
            };
            score += speed_bonus;
        }

        // 5. æ¢¯åº¦ç¨³å®šæ€§è¯„åˆ† (5%) - é™ä½æƒé‡ä½†ä¿ç•™
        if analysis.gradient_norms.len() > 3 {
            let variance = self.calculate_variance(&analysis.gradient_norms);
            if variance < 0.001 {
                score += 0.05; // éå¸¸ç¨³å®š
            } else if variance < 0.01 {
                score += 0.03; // è¾ƒç¨³å®š
            } else if variance < 0.05 {
                score += 0.01; // ä¸€èˆ¬ç¨³å®š
            }
        }

        // ç¡®ä¿å¾—åˆ†åœ¨åˆç†èŒƒå›´å†…
        score.min(1.0).max(0.0)
    }

    /// è¯„ä¼°æ€§èƒ½ç­‰çº§ - è°ƒæ•´é˜ˆå€¼
    fn rate_performance(&self, analysis: &DetailedConvergenceAnalysis, expected: &QualityLevel) -> String {
        let score = self.calculate_quality_score(analysis);

        // è°ƒæ•´é˜ˆå€¼ï¼Œä½¿åˆ†çº§æ›´åˆç†
        let actual_rating = if score >= 0.75 {
            "Excellent"
        } else if score >= 0.55 {
            "Good"
        } else if score >= 0.35 {
            "Fair"
        } else {
            "Poor"
        };

        let expected_str = format!("{:?}", expected);
        let matches_expectation = match (expected, actual_rating) {
            (QualityLevel::Excellent, "Excellent") => true,
            (QualityLevel::Good, "Good") | (QualityLevel::Good, "Excellent") => true,
            (QualityLevel::Fair, "Fair") | (QualityLevel::Fair, "Good") | (QualityLevel::Fair, "Excellent") => true,
            (QualityLevel::Poor, _) => true, // Poorçº§åˆ«åªè¦ä¸æ˜¯æœ€å·®å°±ç®—ç¬¦åˆ
            _ => false,
        };

        if matches_expectation {
            format!("{} âœ… (ç¬¦åˆé¢„æœŸ: {})", actual_rating, expected_str)
        } else {
            format!("{} âš ï¸ (é¢„æœŸ: {})", actual_rating, expected_str)
        }
    }

    /// ç”Ÿæˆæ”¹è¿›å»ºè®®
    fn generate_recommendations(&self, analysis: &DetailedConvergenceAnalysis, benchmark: &PromptBenchmark) -> Vec<String> {
        let mut recommendations = Vec::new();

        match analysis.convergence_type {
            ConvergenceType::Diverging => {
                recommendations.push("ğŸ”¥ å‘æ•£é—®é¢˜ï¼šå»ºè®®ç®€åŒ– promptï¼Œå‡å°‘å¤æ‚æ€§".to_string());
                recommendations.push("ğŸ“‰ é™ä½å­¦ä¹ ç‡ï¼Œå¢å¼ºæ­£åˆ™åŒ–".to_string());
            },
            ConvergenceType::Stable => {
                recommendations.push("â±ï¸ æœªå……åˆ†æ”¶æ•›ï¼šå¢åŠ è¿­ä»£æ¬¡æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡".to_string());
            },
            ConvergenceType::Slow => {
                recommendations.push("ğŸŒ æ”¶æ•›ç¼“æ…¢ï¼šè€ƒè™‘ä¼˜åŒ– prompt ç»“æ„".to_string());
            },
            ConvergenceType::Oscillating => {
                recommendations.push("ğŸŒŠ éœ‡è¡æ”¶æ•›ï¼šå»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢å¼ºæ­£åˆ™åŒ–".to_string());
            },
            _ => {
                recommendations.push("âœ… æ”¶æ•›è¡¨ç°è‰¯å¥½ï¼Œprompt è´¨é‡è¾ƒé«˜".to_string());
            }
        }

        // åŸºäºç±»åˆ«çš„ç‰¹å®šå»ºè®®
        match benchmark.category {
            PromptCategory::Simple => {
                recommendations.push("ğŸ’¡ ç®€å•æŒ‡ä»¤ï¼šè€ƒè™‘æ·»åŠ æ›´å¤šç»†èŠ‚å’Œç»“æ„".to_string());
            },
            PromptCategory::Complex => {
                recommendations.push("ğŸ”§ å¤æ‚æŒ‡ä»¤ï¼šå¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªç®€å•æ­¥éª¤".to_string());
            },
            _ => {}
        }

        recommendations
    }

    /// è®¡ç®—æ–¹å·®
    fn calculate_variance(&self, values: &[f32]) -> f32 {
        let mean = values.iter().sum::<f32>() / values.len() as f32;
        values.iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f32>() / values.len() as f32
    }

    /// æ‰“å°åŸºå‡†æµ‹è¯•æ€»ç»“
    fn print_benchmark_summary(&self, results: &[BenchmarkResult]) {
        println!("\nğŸ“ˆ === åŸºå‡†æµ‹è¯•æ€»ç»“ ===");

        let mut category_stats: HashMap<String, Vec<f32>> = HashMap::new();
        let mut total_score = 0.0;
        let mut excellent_count = 0;
        let mut good_count = 0;

        for result in results {
            let category = format!("{:?}", result.benchmark.category);
            category_stats.entry(category).or_insert_with(Vec::new).push(result.quality_score);
            total_score += result.quality_score;

            if result.quality_score >= 0.8 {
                excellent_count += 1;
            } else if result.quality_score >= 0.6 {
                good_count += 1;
            }

            println!("{}: {} (å¾—åˆ†: {:.3})",
                result.benchmark.name,
                result.performance_rating,
                result.quality_score
            );
        }

        println!("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:");
        for (category, scores) in category_stats {
            let avg_score = scores.iter().sum::<f32>() / scores.len() as f32;
            println!("  {}: å¹³å‡å¾—åˆ† {:.3} ({} ä¸ªæ ·æœ¬)", category, avg_score, scores.len());
        }

        println!("\nğŸ¯ æ€»ä½“è¡¨ç°:");
        println!("  å¹³å‡è´¨é‡å¾—åˆ†: {:.3}", total_score / results.len() as f32);
        println!("  ä¼˜ç§€ prompt æ•°é‡: {} ({:.1}%)", excellent_count, excellent_count as f32 / results.len() as f32 * 100.0);
        println!("  è‰¯å¥½ prompt æ•°é‡: {} ({:.1}%)", good_count, good_count as f32 / results.len() as f32 * 100.0);
    }
}
