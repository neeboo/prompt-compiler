use anyhow::Result;
use std::sync::{Arc, Mutex}; // æ·»åŠ Mutexæ”¯æŒ
use tracing::{info, debug, warn}; // æ·»åŠ warnå¯¼å…¥
use prompt_compiler_embeddings::EmbeddingProvider;
use prompt_compiler_weights::{ImplicitDynamics, DynamicsConfig}; // æ·»åŠ DynamicsConfigå¯¼å…¥
use regex; // æ·»åŠ regexä¾èµ–
use serde::{Deserialize, Serialize}; // æ·»åŠ Azure APIéœ€è¦çš„åºåˆ—åŒ–æ”¯æŒ
use reqwest; // æ·»åŠ HTTPå®¢æˆ·ç«¯

use crate::{ChatCompletionRequest, ChatCompletionResponse, ChatMessage, ProcessedRequest};
use crate::storage::NodeStorage;

// Azure Text Analytics API æ•°æ®ç»“æ„
#[derive(Debug, Serialize)]
struct AzureNerRequest {
    #[serde(rename = "analysisInput")]
    analysis_input: AzureAnalysisInput,
    #[serde(rename = "tasks")]
    tasks: Vec<AzureTask>,
}

#[derive(Debug, Serialize)]
struct AzureAnalysisInput {
    documents: Vec<AzureDocument>,
}

#[derive(Debug, Serialize)]
struct AzureDocument {
    id: String,
    language: String,
    text: String,
}

#[derive(Debug, Serialize)]
struct AzureTask {
    kind: String,
    #[serde(rename = "taskName")]
    task_name: String,
}

#[derive(Debug, Deserialize)]
struct AzureNerResponse {
    tasks: AzureTasks,
}

#[derive(Debug, Deserialize)]
struct AzureTasks {
    items: Vec<AzureTaskItem>,
}

#[derive(Debug, Deserialize)]
struct AzureTaskItem {
    results: AzureResults,
}

#[derive(Debug, Deserialize)]
struct AzureResults {
    documents: Vec<AzureResultDocument>,
}

#[derive(Debug, Deserialize)]
struct AzureResultDocument {
    entities: Vec<AzureEntity>,
}

#[derive(Debug, Deserialize)]
struct AzureEntity {
    text: String,
    category: String,
    #[serde(rename = "subcategory")]
    subcategory: Option<String>,
    #[serde(rename = "confidenceScore")]
    confidence_score: f32,
}

pub struct ContextEngine {
    embedding_provider: Mutex<EmbeddingProvider>, // ç”¨MutexåŒ…è£…
    dynamics: ImplicitDynamics,
    storage: Arc<NodeStorage>,
}

impl ContextEngine {
    pub async fn new(storage: Arc<NodeStorage>) -> Result<Self> {
        info!("Initializing Context Engine with weight dynamics...");

        // Initialize weight dynamics with paper-compliant configuration
        let config = DynamicsConfig {
            learning_rate: 0.1,
            use_skip_connections: false,
            regularization_strength: 0.001,
        };
        let dynamics = ImplicitDynamics::new(384, 256, config)?;

        // Initialize embedding provider
        let embedding_provider = Mutex::new(EmbeddingProvider::new(
            prompt_compiler_embeddings::EmbeddingConfig::default()
        )?);

        info!("âœ… Context Engine initialized successfully");

        Ok(Self {
            storage,
            dynamics,
            embedding_provider,
        })
    }

    // ğŸ”§ ç®€åŒ–ç‰ˆæœ¬ï¼šAzure Text Analytics NER è°ƒç”¨ï¼ˆå¸¦æ›´å¥½çš„é”™è¯¯å¤„ç†ï¼‰
    async fn call_azure_ner(&self, texts: &[String]) -> Result<Vec<AzureEntity>> {
        // ğŸ”§ é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦é…ç½®
        let endpoint = match std::env::var("AZURE_TEXT_ANALYTICS_ENDPOINT") {
            Ok(val) if !val.is_empty() => val,
            _ => {
                debug!("Azure Text Analytics endpoint not configured, skipping NER");
                return Err(anyhow::anyhow!("Azure endpoint not configured"));
            }
        };

        let api_key = match std::env::var("AZURE_TEXT_ANALYTICS_KEY") {
            Ok(val) if !val.is_empty() => val,
            _ => {
                debug!("Azure Text Analytics key not configured, skipping NER");
                return Err(anyhow::anyhow!("Azure key not configured"));
            }
        };

        // ğŸ”§ ä½¿ç”¨æ›´å…¼å®¹çš„ REST API æ ¼å¼
        // å‚è€ƒï¼šhttps://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking
        #[derive(Serialize)]
        struct SimpleDocument {
            id: String,
            text: String,
            language: String,
        }

        #[derive(Serialize)]
        struct SimpleRequest {
            documents: Vec<SimpleDocument>,
        }

        let documents: Vec<SimpleDocument> = texts
            .iter()
            .enumerate()
            .map(|(i, text)| SimpleDocument {
                id: format!("doc_{}", i),
                text: text.clone(),
                language: "en".to_string(),
            })
            .collect();

        let request = SimpleRequest { documents };

        let client = reqwest::Client::new();
        // ğŸ”§ ä½¿ç”¨ç¨³å®šçš„ v3.1 API
        let url = format!("{}/text/analytics/v3.1/entities/recognition/general", endpoint.trim_end_matches('/'));

        debug!("Calling Azure Text Analytics v3.1: {}", url);

        let response = client
            .post(&url)
            .header("Ocp-Apim-Subscription-Key", &api_key)
            .header("Content-Type", "application/json")
            .json(&request)
            .timeout(std::time::Duration::from_secs(10)) // ğŸ”§ æ·»åŠ è¶…æ—¶
            .send()
            .await?;

        if !response.status().is_success() {
            let status_code = response.status();
            let error_text = response.text().await.unwrap_or_default();
            warn!("Azure API error: {} - {}", status_code, &error_text[..std::cmp::min(200, error_text.len())]);
            return Err(anyhow::anyhow!("Azure API error: {}", status_code));
        }

        // ğŸ”§ ï¿½ï¿½ï¿½åŒ–çš„å“åº”è§£æ
        #[derive(Deserialize)]
        struct SimpleResponse {
            documents: Vec<SimpleDocumentResponse>,
        }

        #[derive(Deserialize)]
        struct SimpleDocumentResponse {
            entities: Vec<AzureEntity>,
        }

        let azure_response: SimpleResponse = response.json().await
            .map_err(|e| anyhow::anyhow!("Failed to parse Azure response: {}", e))?;

        // æå–æ‰€æœ‰å®ä½“
        let mut all_entities = Vec::new();
        for document in azure_response.documents {
            all_entities.extend(document.entities);
        }

        debug!("Azure NER found {} entities", all_entities.len());
        Ok(all_entities)
    }

    // ğŸ”§ æ–°å¢ï¼šä½¿ç”¨ Azure NER å¢å¼ºçš„ç»¼åˆä¿¡æ¯æå–
    async fn extract_comprehensive_key_information_with_ner(&self, contexts: &[SimilarContext]) -> Result<Vec<String>> {
        // å‡†å¤‡æ–‡æœ¬ç”¨äº NER åˆ†æ
        let texts: Vec<String> = contexts.iter()
            .take(3)  // é™åˆ¶å¤„ç†çš„ä¸Šä¸‹æ–‡æ•°é‡ä»¥æ§åˆ¶æˆæœ¬
            .map(|ctx| ctx.content.clone())
            .collect();

        if texts.is_empty() {
            return Ok(Vec::new());
        }

        let mut key_info = Vec::new();

        // ğŸ”§ æ˜ç¡®æ—¥å¿—ï¼šæ˜¾ç¤ºæ˜¯å¦å°è¯•ä½¿ç”¨Azure NER
        info!("ğŸ” Attempting to use Azure NER for comprehensive key information extraction...");

        // å°è¯•è°ƒç”¨ Azure NER
        match self.call_azure_ner(&texts).await {
            Ok(entities) => {
                info!("âœ… SUCCESS: Azure NER extracted {} entities successfully!", entities.len());
                debug!("Azure NER entities: {:?}", entities.iter().map(|e| &e.text).collect::<Vec<_>>());

                // å°† Azure å®ä½“è½¬æ¢ä¸ºä¸šåŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯
                for entity in entities {
                    if entity.confidence_score >= 0.7 {  // åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„å®ä½“
                        let key_item = self.convert_azure_entity_to_business_info(&entity);
                        if !key_item.is_empty() && !key_info.contains(&key_item) {
                            key_info.push(key_item);
                        }
                    }
                }

                info!("ğŸ“Š Azure NER result: {} high-confidence business entities extracted", key_info.len());

                // å¦‚æœ NER ç»“æœä¸å¤Ÿä¸°å¯Œï¼Œè¡¥å……ç¡¬ç¼–ç çš„æå–ç»“æœ
                if key_info.len() < 3 {
                    info!("ğŸ“ Azure NER results insufficient, supplementing with local extraction...");
                    let fallback_info = self.extract_comprehensive_key_information(contexts);
                    for item in fallback_info {
                        if !key_info.contains(&item) && key_info.len() < 6 {
                            key_info.push(item);
                        }
                    }
                }
            }
            Err(e) => {
                warn!("âŒ FALLBACK: Azure NER failed, using local extraction only: {}", e);
                info!("ğŸ”„ Falling back to hardcoded entity extraction methods");
                // å®Œå…¨é™çº§åˆ°åŸæœ‰çš„ç¡¬ç¼–ç æ–¹æ³•
                return Ok(self.extract_comprehensive_key_information(contexts));
            }
        }

        info!("ğŸ¯ Final result: {} key information items extracted (Azure NER + Local)", key_info.len());
        Ok(key_info)
    }

    // ğŸ”§ æ–°å¢ï¼šå°† Azure å®ä½“è½¬æ¢ä¸ºä¸šåŠ¡ç›¸å…³ä¿¡æ¯
    fn convert_azure_entity_to_business_info(&self, entity: &AzureEntity) -> String {
        match entity.category.as_str() {
            "Person" => {
                format!("Contact: {}", entity.text)
            }
            "Organization" => {
                format!("Company: {}", entity.text)
            }
            "Location" => {
                format!("Location: {}", entity.text)
            }
            "DateTime" => {
                format!("Timeline: {}", entity.text)
            }
            "Quantity" => {
                if entity.text.contains("thousand") || entity.text.contains("million") {
                    format!("Scale: {}", entity.text)
                } else {
                    format!("Metric: {}", entity.text)
                }
            }
            "PersonType" => {
                format!("Role: {}", entity.text)
            }
            "Product" => {
                format!("Product: {}", entity.text)
            }
            "Event" => {
                format!("Event: {}", entity.text)
            }
            _ => {
                // ï¿½ï¿½ï¿½äºå…¶ä»–ç±»å‹ï¼Œæ ¹æ®å­ç±»åˆ«è¿›ä¸€æ­¥åˆ†ç±»
                if let Some(subcategory) = &entity.subcategory {
                    format!("{}: {}", subcategory, entity.text)
                } else {
                    format!("Entity: {}", entity.text)
                }
            }
        }
    }

    // ğŸ”§ æ–°å¢ï¼šä½¿ç”¨ Azure NER å¢å¼ºçš„ç”¨æˆ·èº«ä»½æå–
    async fn extract_persistent_user_identity_with_ner(&self, contexts: &[SimilarContext]) -> Result<String> {
        // å‡†å¤‡æ–‡æœ¬ç”¨äº NER åˆ†æ
        let texts: Vec<String> = contexts.iter()
            .take(2)  // ç”¨æˆ·èº«ä»½ä¿¡æ¯é€šå¸¸åœ¨å‰å‡ è½®å¯¹è¯ä¸­
            .map(|ctx| ctx.content.clone())
            .collect();

        if texts.is_empty() {
            return Ok(String::new());
        }

        let mut identity_parts = Vec::new();

        // ğŸ”§ æ˜ç¡®æ—¥å¿—ï¼šæ˜¾ç¤ºæ˜¯å¦å°è¯•ä½¿ç”¨Azure NERè¿›è¡Œèº«ä»½æå–
        info!("ğŸ‘¤ Attempting to use Azure NER for user identity extraction...");

        // å°è¯•è°ƒç”¨ Azure NER
        match self.call_azure_ner(&texts).await {
            Ok(entities) => {
                info!("âœ… SUCCESS: Azure NER found {} entities for identity analysis!", entities.len());
                debug!("Azure NER identity entities: {:?}", entities.iter().map(|e| format!("{}({})", e.text, e.category)).collect::<Vec<_>>());

                // æå–èº«ä»½ç›¸å…³çš„å®ä½“
                for entity in entities {
                    if entity.confidence_score >= 0.8 {  // èº«ä»½ä¿¡æ¯è¦æ±‚æ›´é«˜çš„ç½®ä¿¡åº¦
                        match entity.category.as_str() {
                            "Person" => {
                                let name_info = format!("Name: {}", entity.text);
                                if !identity_parts.contains(&name_info) {
                                    identity_parts.push(name_info);
                                    info!("ğŸ·ï¸  Azure NER extracted person name: {}", entity.text);
                                }
                            }
                            "PersonType" => {
                                let role_info = format!("Role: {}", entity.text);
                                if !identity_parts.contains(&role_info) {
                                    identity_parts.push(role_info);
                                    info!("ğŸ’¼ Azure NER extracted role: {}", entity.text);
                                }
                            }
                            "Organization" => {
                                let org_info = format!("Company: {}", entity.text);
                                if !identity_parts.contains(&org_info) {
                                    identity_parts.push(org_info);
                                    info!("ğŸ¢ Azure NER extracted organization: {}", entity.text);
                                }
                            }
                            "Product" | "Event" => {
                                let project_info = format!("Project: {}", entity.text);
                                if !identity_parts.contains(&project_info) {
                                    identity_parts.push(project_info);
                                    info!("ğŸš€ Azure NER extracted project/event: {}", entity.text);
                                }
                            }
                            _ => {}
                        }
                    }
                }

                info!("ğŸ“‹ Azure NER identity result: {} identity components extracted", identity_parts.len());

                // å¦‚æœ NER ç»“æœä¸å¤Ÿï¼Œè¡¥å……ç¡¬ç¼–ç çš„æå–ç»“æœ
                if identity_parts.len() < 2 {
                    info!("ğŸ“ Azure NER identity results insufficient, supplementing with local extraction...");
                    let fallback_identity = self.extract_persistent_user_identity(contexts);
                    if !fallback_identity.is_empty() && !identity_parts.iter().any(|part| part.contains(&fallback_identity)) {
                        identity_parts.push(fallback_identity);
                    }
                }
            }
            Err(e) => {
                warn!("âŒ FALLBACK: Azure NER failed for identity extraction, using local methods: {}", e);
                info!("ğŸ”„ Falling back to hardcoded identity extraction methods");
                // é™çº§åˆ°åŸæœ‰æ–¹æ³•
                return Ok(self.extract_persistent_user_identity(contexts));
            }
        }

        let final_identity = if identity_parts.is_empty() {
            String::new()
        } else {
            identity_parts.join(", ")
        };

        if final_identity.is_empty() {
            info!("ğŸš« No user identity information extracted");
        } else {
            info!("ğŸ¯ Final identity result: {} (Azure NER + Local)", final_identity);
        }

        Ok(final_identity)
    }

    pub async fn process_request_with_group(
        &self,
        request: &ChatCompletionRequest,
        agent_id: Option<&str>,
        shared_context_group: Option<&str>,
    ) -> Result<ProcessedRequest> {
        debug!("Processing request with context sharing for agent: {:?}, group: {:?}", agent_id, shared_context_group);

        // Extract context from messages
        let conversation_context = self.extract_conversation_context(&request.messages).await?;
        debug!("Current conversation context: {}", &conversation_context[..std::cmp::min(100, conversation_context.len())]);

        // ğŸ”§ è·¨Agentä¸Šä¸‹æ–‡å…±äº«é€»è¾‘ - è¿›ä¸€æ­¥ä¼˜åŒ–è¿‡æ»¤ç­–ç•¥
        let similar_contexts = if let Some(group) = shared_context_group {
            // ä½¿ç”¨ä¸Šä¸‹æ–‡ç»„æŸ¥æ‰¾ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆè·¨Agentå…±äº«ï¼‰
            let context_embedding = {
                let mut provider = self.embedding_provider.lock()
                    .map_err(|e| anyhow::anyhow!("Lock error: {}", e))?;
                provider.encode(&conversation_context)?
            };

            let contexts = self.storage.find_similar_contexts_in_group(group, &context_embedding, 15).await?; // ğŸ”§ å¢åŠ åˆ°15ä¸ªå€™é€‰
            debug!("Found {} similar contexts in group {}", contexts.len(), group);

            // ğŸ”§ æ™ºèƒ½ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼šç¡®ä¿å®¢æˆ·èº«ï¿½ï¿½ä¿¡æ¯å’Œå…³é”®ä¸šåŠ¡ä¿¡æ¯ä¼˜å…ˆä¿ç•™
            let mut filtered_contexts = Vec::new();

            // ğŸ”§ æ”¹è¿›ï¼šé¦–å…ˆæ— æ¡ä»¶æ·»åŠ æ‰€æœ‰åŒ…å«å®¢æˆ·èº«ä»½çš„ä¸Šä¸‹æ–‡
            for ctx in &contexts {
                if self.contains_client_identity(&ctx.content) {
                    filtered_contexts.push(ctx.clone());
                    if filtered_contexts.len() >= 5 { // æœ€å¤š5ä¸ªå®¢æˆ·ç›¸å…³ä¸Šä¸‹æ–‡
                        break;
                    }
                }
            }

            // ç¬¬äºŒè½®ï¼šæ·»åŠ ä¸šåŠ¡å…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
            for ctx in &contexts {
                if !self.contains_client_identity(&ctx.content) &&
                   self.contains_business_info(&ctx.content) &&
                   filtered_contexts.len() < 10 {
                    filtered_contexts.push(ctx.clone());
                }
            }

            // ç¬¬ä¸‰è½®ï¼šæ·»åŠ å…¶ä»–é«˜ç›¸ä¼¼åº¦ä¸Šä¸‹æ–‡
            for ctx in contexts.into_iter() {
                if !self.contains_client_identity(&ctx.content) &&
                   !self.contains_business_info(&ctx.content) &&
                   filtered_contexts.len() < 12 &&
                   ctx.similarity > 0.05 { // ğŸ”§ é™ä½é˜ˆå€¼åˆ°0.05
                    filtered_contexts.push(ctx);
                }
            }

            debug!("Filtered contexts: {} with relaxed criteria", filtered_contexts.len());
            filtered_contexts
        } else if let Some(agent_id) = agent_id {
            // ğŸ”§ å•Agentä¸Šä¸‹æ–‡æŸ¥æ‰¾ - åº”ç”¨å¤šAgentï¿½ï¿½åŠŸç»éªŒ
            let context_embedding = {
                let mut provider = self.embedding_provider.lock()
                    .map_err(|e| anyhow::anyhow!("Lock error: {}", e))?;
                provider.encode(&conversation_context)?
            };

            let contexts = self.storage.find_similar_contexts(agent_id, &context_embedding, 10).await?; // ğŸ”§ å¢åŠ å€™é€‰æ•°é‡
            debug!("Found {} similar contexts for agent {}", contexts.len(), agent_id);

            // ğŸ”§ åº”ç”¨ä¸å¤šAgentç›¸åŒçš„æ™ºèƒ½è¿‡æ»¤ç­–ç•¥
            let mut filtered_contexts = Vec::new();

            // ç¬¬ä¸€è½®ï¼šä¼˜å…ˆæ·»åŠ åŒ…å«ç”¨æˆ·èº«ä»½çš„ä¸Šä¸‹æ–‡ï¼ˆé™ä½é˜ˆå€¼ï¼‰
            for ctx in &contexts {
                if self.contains_user_identity(&ctx.content) && ctx.similarity > 0.05 { // ğŸ”§ é™ä½é˜ˆå€¼
                    filtered_contexts.push(ctx.clone());
                    if filtered_contexts.len() >= 3 {
                        break;
                    }
                }
            }

            // ç¬¬äºŒè½®ï¼šæ·»åŠ å…¶ä»–ç›¸å…³ä¸Šä¸‹æ–‡
            for ctx in contexts.into_iter() {
                if !self.contains_user_identity(&ctx.content) &&
                   filtered_contexts.len() < 8 &&
                   ctx.similarity > 0.05 { // ğŸ”§ ç»Ÿä¸€ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
                    filtered_contexts.push(ctx);
                }
            }

            debug!("Single-agent filtered contexts: {} with relaxed criteria", filtered_contexts.len());
            filtered_contexts
        } else {
            debug!("No agent ID or context group provided, skipping context lookup");
            Vec::new()
        };

        // ğŸ”§ å¢å¼ºçš„ä¸Šä¸‹æ–‡å¤„ç† - ä¼˜å…ˆä¿ç•™å…³é”®ä¿¡æ¯
        let processed_messages = if !similar_contexts.is_empty() {
            debug!("Applying context sharing with {} relevant contexts", similar_contexts.len());

            // ğŸ”§ æ”¹è¿›ï¼šä¼˜å…ˆä½¿ç”¨Azure NERå¢å¼ºçš„ä¿¡æ¯æå–ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
            info!("ğŸ” Starting enhanced context analysis with Azure NER...");
            let key_info = self.extract_comprehensive_key_information_with_ner(&similar_contexts).await
                .unwrap_or_else(|e| {
                    warn!("ğŸ”„ Azure NER unavailable ({}), using local extraction", e);
                    self.extract_comprehensive_key_information(&similar_contexts)
                });

            // ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ Azure NER å¢å¼ºçš„ç”¨æˆ·èº«ä»½æå–ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
            info!("ğŸ‘¤ Starting user identity analysis with Azure NER...");
            let user_identity_info = self.extract_persistent_user_identity_with_ner(&similar_contexts).await
                .unwrap_or_else(|e| {
                    warn!("ğŸ”„ Azure NER identity extraction unavailable ({}), using local methods", e);
                    self.extract_persistent_user_identity(&similar_contexts)
                });

            let enhanced_context = if !key_info.is_empty() {
                let mut context_parts = Vec::new();

                // ğŸ”§ ä¼˜å…ˆæ·»åŠ ç”¨æˆ·èº«ä»½ä¿¡æ¯ï¼ˆç¡®ä¿ä¸ä¸¢å¤±ï¼‰
                if !user_identity_info.is_empty() {
                    context_parts.push(format!("User Identity: {}", user_identity_info));
                }

                // æ·»åŠ å…¶ä»–å…³é”®ä¿¡æ¯
                if !key_info.is_empty() {
                    context_parts.push(format!("Previous Context: {}", key_info.join("; ")));
                }

                format!("IMPORTANT CONTEXT - {}", context_parts.join(" | "))
            } else {
                format!("Previous context: {}",
                       similar_contexts.first()
                           .map(|c| &c.content[..std::cmp::min(300, c.content.len())])  // å¢åŠ åˆ°300å­—ç¬¦
                           .unwrap_or("No context available"))
            };

            // ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆå‹ç¼©ç­–ç•¥ï¼Œåœ¨å‹ç¼©æ—¶å¼ºåˆ¶ä¿ç•™ç”¨æˆ·èº«ä»½ä¿¡æ¯
            let base_messages = if request.messages.len() > 5 {  // ä¿æŒé˜ˆå€¼ä¸º5
                debug!("Long conversation detected ({} messages), applying identity-preserving compression", request.messages.len());
                self.apply_identity_preserving_compression(&request.messages, &user_identity_info).await?
            } else {
                request.messages.clone()
            };

            let mut enhanced = vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: enhanced_context,
                }
            ];
            enhanced.extend(base_messages);

            debug!("Added enhanced context ({} chars) with user identity: {}",
                   enhanced[0].content.len(), user_identity_info);
            enhanced
        } else if request.messages.len() > 5 {
            // å¤šè½®å¯¹è¯åœºæ™¯ï¼šæ™ºèƒ½å‹ç¼©
            debug!("Multi-turn conversation detected ({} messages), applying smart compression", request.messages.len());
            self.apply_smart_compression(&request.messages).await?
        } else {
            debug!("No compression needed");
            request.messages.clone()
        };

        // è®¡ç®—å‹ç¼©æ•ˆæœ
        let original_content_length: usize = request.messages.iter().map(|m| m.content.len()).sum();
        let processed_content_length: usize = processed_messages.iter().map(|m| m.content.len()).sum();

        let compression_ratio = if processed_content_length > original_content_length {
            -((processed_content_length - original_content_length) as f32 / original_content_length as f32)
        } else {
            ((original_content_length - processed_content_length) as f32 / original_content_length as f32)
        };

        Ok(ProcessedRequest {
            messages: processed_messages,
            compression_ratio,
            context_used: !similar_contexts.is_empty(),
        })
    }

    // ä¿æŒåŸæœ‰æ–¹æ³•çš„å‘åå…¼å®¹æ€§
    pub async fn process_request(
        &self,
        request: &ChatCompletionRequest,
        agent_id: Option<&str>,
    ) -> Result<ProcessedRequest> {
        self.process_request_with_group(request, agent_id, None).await
    }

    // ğŸ”§ æ–°å¢ï¼šå®ç°çœŸæ­£çš„è¯­ä¹‰å‹ç¼©
    fn compress_historical_context(&self, contexts: &[SimilarContext]) -> String {
        if contexts.is_empty() {
            return String::new();
        }

        // æå–å…³ï¿½ï¿½ï¿½ï¿½ï¿½æ¯ï¼Œï¿½ï¿½ï¿½ï¿½ï¿½æ˜¯å®Œæ•´å¯¹è¯
        let mut key_facts = Vec::new();

        for context in contexts.iter().take(2) { // åªå–æœ€ç›¸å…³çš„2ä¸ªä¸Šä¸‹æ–‡
            // ä»å†å²å¯¹è¯ä¸­æå–å…³é”®äº‹å®
            let content = &context.content;

            // ç®€å•çš„å…³é”®ä¿¡æ¯æå–é€»è¾‘
            if content.contains("name") || content.contains("åå­—") {
                if let Some(name_info) = self.extract_name_info(content) {
                    key_facts.push(name_info);
                }
            }

            if content.contains("work") || content.contains("job") || content.contains("å·¥ä½œ") {
                if let Some(work_info) = self.extract_work_info(content) {
                    key_facts.push(work_info);
                }
            }

            if content.contains("project") || content.contains("é¡¹ç›®") {
                if let Some(project_info) = self.extract_project_info(content) {
                    key_facts.push(project_info);
                }
            }
        }

        // å»é‡å¹¶ç”Ÿæˆç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦
        key_facts.dedup();
        if key_facts.is_empty() {
            return String::new();
        }

        format!("Context: {}", key_facts.join(", "))
    }

    // ğŸ”§ è¾…åŠ©æ–¹æ³•ï¼šæå–å…³é”®ä¿¡æ¯
    fn extract_name_info(&self, content: &str) -> Option<String> {
        // ç®€å•çš„åå­—æå–é€»è¾‘
        if let Some(start) = content.find("name is ") {
            let after_name = &content[start + 8..];
            if let Some(end) = after_name.find(" ") {
                let name = &after_name[..end];
                if name.len() > 0 && name.len() < 20 {
                    return Some(format!("name: {}", name));
                }
            }
        }
        None
    }

    fn extract_work_info(&self, content: &str) -> Option<String> {
        if content.contains("work at") || content.contains("job") {
            // æå–å·¥ä½œç›¸å…³çš„ç®€çŸ­ä¿¡æ¯
            if content.contains("tech") || content.contains("startup") {
                return Some("work: tech".to_string());
            }
            if content.contains("engineer") {
                return Some("work: engineer".to_string());
            }
        }
        None
    }

    fn extract_project_info(&self, content: &str) -> Option<String> {
        if content.contains("Python") && content.contains("machine learning") {
            return Some("project: Python ML".to_string());
        }
        if content.contains("project") {
            return Some("project: development".to_string());
        }
        None
    }

    // ğŸ”§ æ–°å¢ï¼šä½¿ç”¨Azure NERå¢å¼ºçš„å¯¹è¯å†å²å‹ç¼©
    async fn compress_conversation_history_with_ner(&self, messages: &[ChatMessage]) -> Result<String> {
        if messages.is_empty() {
            return Ok(String::new());
        }

        // å‡†å¤‡æ–‡æœ¬ç”¨äºAzure NERåˆ†æ
        let conversation_text = messages
            .iter()
            .take(5) // åªåˆ†æå‰5æ¡æ¶ˆæ¯ä»¥æ§åˆ¶æˆæœ¬
            .map(|msg| format!("{}: {}", msg.role, msg.content))
            .collect::<Vec<_>>()
            .join("\n");

        info!("ğŸ” Using Azure NER for conversation history compression...");

        // å°è¯•ä½¿ç”¨Azure NERæå–å…³é”®å®ä½“
        match self.call_azure_ner(&[conversation_text]).await {
            Ok(entities) => {
                info!("âœ… Azure NER extracted {} entities from conversation history", entities.len());

                let mut summary_components = Vec::new();
                let mut names = Vec::new();
                let mut organizations = Vec::new();
                let mut topics = Vec::new();
                let mut roles = Vec::new();

                // åˆ†ç±»Azure NERæå–çš„å®ä½“
                for entity in entities {
                    if entity.confidence_score >= 0.6 { // è¾ƒä½çš„ç½®ä¿¡åº¦é˜ˆå€¼ç”¨äºå†å²å‹ç¼©
                        match entity.category.as_str() {
                            "Person" => {
                                if !names.contains(&entity.text) {
                                    names.push(entity.text);
                                }
                            }
                            "Organization" => {
                                if !organizations.contains(&entity.text) {
                                    organizations.push(entity.text);
                                }
                            }
                            "PersonType" => {
                                if !roles.contains(&entity.text) {
                                    roles.push(entity.text);
                                }
                            }
                            "Product" | "Event" => {
                                if !topics.contains(&entity.text) {
                                    topics.push(entity.text);
                                }
                            }
                            _ => {}
                        }
                    }
                }

                // æ„å»ºæ™ºèƒ½æ‘˜è¦
                if !names.is_empty() {
                    summary_components.push(format!("Participants: {}", names.join(", ")));
                }
                if !organizations.is_empty() {
                    summary_components.push(format!("Organizations: {}", organizations.join(", ")));
                }
                if !roles.is_empty() {
                    summary_components.push(format!("Roles: {}", roles.join(", ")));
                }
                if !topics.is_empty() {
                    summary_components.push(format!("Topics: {}", topics.join(", ")));
                }

                if summary_components.is_empty() {
                    // å¦‚æœAzure NERæ²¡æœ‰æå–åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œä½¿ç”¨æœ¬åœ°æ–¹æ³•è¡¥å……
                    info!("ğŸ“ Azure NER results insufficient for history, supplementing with local analysis");
                    Ok(self.compress_conversation_history(messages))
                } else {
                    let azure_summary = summary_components.join("; ");
                    info!("ğŸ¯ Azure NER conversation summary: {} components", summary_components.len());
                    Ok(azure_summary)
                }
            }
            Err(e) => {
                warn!("ğŸ”„ Azure NER unavailable for conversation history ({}), using local compression", e);
                Ok(self.compress_conversation_history(messages))
            }
        }
    }

    // ğŸ”§ æ”¹è¿›ï¼šæ›´æ™ºèƒ½çš„å¯¹è¯å†å²å‹ç¼©ï¼ˆå‡å°‘ç¡¬ç¼–ç ï¼‰
    fn compress_conversation_history(&self, messages: &[ChatMessage]) -> String {
        if messages.is_empty() {
            return String::new();
        }

        // ğŸ”§ ï¿½ï¿½ï¿½ç”¨æ›´æ™ºï¿½ï¿½ï¿½çš„è¯­ä¹‰æå–ï¼Œå‡å°‘ç¡¬ç¼–ç 
        let mut key_entities = Vec::new();
        let mut topics = std::collections::HashSet::new();
        let mut user_attributes = Vec::new();

        for message in messages {
            let content = &message.content;

            // ğŸ”§ æ™ºèƒ½å®ä½“æå–ï¼ˆä½¿ç”¨æ›´çµæ´»çš„æ¨¡å¼ï¼‰
            if message.role == "user" {
                // æå–å¯èƒ½çš„ç”¨æˆ·æ ‡è¯†ç¬¦ï¼ˆå§“åã€è§’è‰²ã€å…¬å¸ç­‰ï¼‰
                if let Some(identity) = self.extract_user_identity_smart(content) {
                    if !user_attributes.iter().any(|attr: &String| attr.contains(&identity)) {
                        user_attributes.push(identity);
                    }
                }

                // ğŸ”§ å…³é”®æ”¹è¿›ï¼šæå–ä¸šåŠ¡å…³é”®ä¿¡æ¯
                if let Some(business_info) = self.extract_business_details(content) {
                    if !key_entities.contains(&business_info) && key_entities.len() < 8 {
                        key_entities.push(business_info);
                    }
                }

                // æå–å…³é”®å®ä½“ï¼ˆä½¿ç”¨NER-likeæ–¹æ³•ï¼‰
                let entities = self.extract_entities_smart(content);
                for entity in entities {
                    if !key_entities.contains(&entity) && key_entities.len() < 8 {
                        key_entities.push(entity);
                    }
                }
            }

            // ğŸ”§ ä¸»é¢˜æå–ï¼ˆåŸºäºå…³é”®è¯èšç±»è€Œéç¡¬ç¼–ç ï¼‰
            let detected_topics = self.extract_topics_smart(content);
            for topic in detected_topics {
                topics.insert(topic);
            }
        }

        // ğŸ”§ æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ - ä¼˜ï¿½ï¿½ï¿½ä¿ç•™ä¸šåŠ¡å…³é”®ä¿¡ï¿½ï¿½ï¿½
        let mut summary_parts = Vec::new();

        if !user_attributes.is_empty() {
            summary_parts.push(format!("User: {}", user_attributes.join(", ")));
        }

        if !key_entities.is_empty() {
            summary_parts.push(format!("Context: {}", key_entities.join(", ")));
        }

        if !topics.is_empty() && topics.len() <= 3 {
            let topic_list: Vec<String> = topics.into_iter().take(3).collect();
            summary_parts.push(format!("Topics: {}", topic_list.join(", ")));
        }

        if summary_parts.is_empty() {
            // ğŸ”§ fallback: ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦è€Œéç®€å•è®¡æ•°
            let total_chars: usize = messages.iter().map(|m| m.content.len()).sum();
            let avg_message_len = total_chars / messages.len().max(1);
            format!("Session: {} exchanges, avg {} chars/msg",
                   messages.len() / 2, avg_message_len)
        } else {
            summary_parts.join("; ")
        }
    }

    async fn store_interaction(
        &self, // æ”¹å›ï¿½ï¿½å¯å˜ï¿½ï¿½ï¿½ï¿½ç”¨
        request: &ChatCompletionRequest,
        response: &ChatCompletionResponse,
        agent_id: Option<&str>,
    ) -> Result<()> {
        if let Some(agent_id) = agent_id {
            debug!("Storing interaction context for agent: {}", agent_id);

            // Create conversation summary
            let mut full_conversation = request.messages.clone();
            if let Some(choice) = response.choices.first() {
                full_conversation.push(choice.message.clone());
            }

            let conversation_text = self.extract_conversation_context(&full_conversation).await?;
            let embedding = {
                let mut provider = self.embedding_provider.lock()
                    .map_err(|e| anyhow::anyhow!("Lock error: {}", e))?;
                provider.encode(&conversation_text)?
            };

            // Store in persistent storage
            self.storage.store_context(agent_id, &conversation_text, &embedding).await?;

            debug!("âœ… Interaction context stored successfully");
        }

        Ok(())
    }

    pub async fn store_interaction_with_group(
        &self,
        request: &ChatCompletionRequest,
        response: &ChatCompletionResponse,
        agent_id: Option<&str>,
        shared_context_group: Option<&str>,
    ) -> Result<()> {
        if let Some(agent_id) = agent_id {
            debug!("Storing interaction context for agent: {} in group: {:?}", agent_id, shared_context_group);

            // Create conversation summary
            let mut full_conversation = request.messages.clone();
            if let Some(choice) = response.choices.first() {
                full_conversation.push(choice.message.clone());
            }

            let conversation_text = self.extract_conversation_context(&full_conversation).await?;
            let embedding = {
                let mut provider = self.embedding_provider.lock()
                    .map_err(|e| anyhow::anyhow!("Lock error: {}", e))?;
                provider.encode(&conversation_text)?
            };

            // Store with or without context group
            if let Some(group) = shared_context_group {
                self.storage.store_context_with_group(agent_id, group, &conversation_text, &embedding).await?;
            } else {
                self.storage.store_context(agent_id, &conversation_text, &embedding).await?;
            }

            debug!("âœ… Interaction context stored successfully");
        }

        Ok(())
    }

    async fn extract_conversation_context(&self, messages: &[ChatMessage]) -> Result<String> {
        // Extract meaningful context from conversation
        let context = messages
            .iter()
            .map(|msg| format!("{}: {}", msg.role, msg.content))
            .collect::<Vec<_>>()
            .join("\n");

        Ok(context)
    }

    async fn apply_weight_dynamics(
        &self,
        original_messages: &[ChatMessage],
        similar_contexts: &[SimilarContext],
    ) -> Result<Vec<ChatMessage>> {
        debug!("Applying weight dynamics with {} similar contexts", similar_contexts.len());

        // ç®€åŒ–çš„å‹ç¼©é€»è¾‘ï¼Œé¿å…å¤æ‚çš„weight dynamicsè°ƒç”¨
        let compressed = self.fallback_compression(original_messages).await?;
        Ok(compressed)
    }

    async fn fallback_compression(&self, messages: &[ChatMessage]) -> Result<Vec<ChatMessage>> {
        // Simple fallback: keep last 3 messages
        let keep_count = std::cmp::min(3, messages.len());
        Ok(messages[messages.len() - keep_count..].to_vec())
    }

    fn calculate_compression_ratio(&self, original: &[ChatMessage], compressed: &[ChatMessage]) -> f32 {
        let original_length: usize = original.iter().map(|m| m.content.len()).sum();
        let compressed_length: usize = compressed.iter().map(|m| m.content.len()).sum();

        if original_length == 0 {
            return 0.0;
        }

        1.0 - (compressed_length as f32 / original_length as f32)
    }

    // ğŸ”§ æ™ºèƒ½ç”¨æˆ·èº«ä»½æå–ï¿½ï¿½å‡å°‘ç¡¬ç¼–ç æ¨¡å¼ï¼‰
    fn extract_user_identity_smart(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // ğŸ”§ æ¨¡å¼1: è‡ªæˆ‘ä»‹ç»æ¨¡å¼
        let intro_patterns = [
            (r"i'?m (.+?)(?:[,.\n]|$)", 1),
            (r"my name is (.+?)(?:[,.\n]|$)", 1),
            (r"this is (.+?)(?:[,.\n]|$)", 1),
            (r"i am (.+?)(?:[,.\n]|$)", 1),
            (r"call me (.+?)(?:[,.\n]|$)", 1),
        ];

        for (pattern, group) in intro_patterns {
            if let Some(captures) = regex::Regex::new(pattern).ok()?.captures(&content_lower) {
                if let Some(matched) = captures.get(group) {
                    let identity = matched.as_str().trim();
                    if identity.len() > 1 && identity.len() < 50 && !identity.contains("assistant") {
                        return Some(identity.to_string());
                    }
                }
            }
        }

        None
    }

    // ğŸ”§ æ™ºèƒ½å®ä½“æå–
    fn extract_entities_smart(&self, content: &str) -> Vec<String> {
        let mut entities = Vec::new();
        let content_lower = content.to_lowercase();

        // ğŸ”§ æŠ€æœ¯æ ˆæ£€æµ‹
        let tech_terms = [
            "python", "javascript", "rust", "java", "typescript",
            "react", "vue", "angular", "tensorflow", "pytorch",
            "kubernetes", "docker", "aws", "azure", "gcp"
        ];

        // ğŸ”§ é¢†åŸŸæ£€æµ‹
        let domain_terms = [
            "machine learning", "ai", "blockchain", "fintech", "healthcare",
            "e-commerce", "gaming", "cybersecurity", "data science"
        ];

        // ğŸ”§ å…¬å¸ç±»å‹æ£€æµ‹
        let company_terms = [
            "startup", "corporation", "enterprise", "consulting", "agency"
        ];

        for term in tech_terms.iter().chain(domain_terms.iter()).chain(company_terms.iter()) {
            if content_lower.contains(term) {
                entities.push(term.to_string());
            }
        }

        entities
    }

    // ğŸ”§ æ™ºèƒ½ä¸»é¢˜æå–
    fn extract_topics_smart(&self, content: &str) -> Vec<String> {
        let mut topics = Vec::new();
        let content_lower = content.to_lowercase();

        // ğŸ”§ é—®é¢˜ç±»å‹æ£€æµ‹
        if content_lower.contains("how") && (content_lower.contains("work") || content_lower.contains("implement")) {
            topics.push("implementation".to_string());
        }

        if content_lower.contains("what") && (content_lower.contains("best") || content_lower.contains("recommend")) {
            topics.push("recommendations".to_string());
        }

        if content_lower.contains("problem") || content_lower.contains("issue") || content_lower.contains("error") {
            topics.push("troubleshooting".to_string());
        }

        // ğŸ”§ æŠ€æœ¯ä¸»é¢˜ï¿½ï¿½ï¿½æµ‹
        let tech_topics = [
            ("algorithm", vec!["algorithm", "sort", "search", "optimize"]),
            ("database", vec!["database", "sql", "nosql", "query"]),
            ("frontend", vec!["frontend", "ui", "ux", "css", "html"]),
            ("backend", vec!["backend", "api", "server", "microservice"]),
            ("devops", vec!["deploy", "ci/cd", "pipeline", "infrastructure"]),
        ];

        for (topic, keywords) in tech_topics {
            if keywords.iter().any(|keyword| content_lower.contains(keyword)) {
                topics.push(topic.to_string());
            }
        }

        topics
    }

    // ğŸ”§ æ–°å¢ï¼šä»å¤šä¸ªä¸Šä¸‹æ–‡ï¿½ï¿½ï¿½æå–å…³é”®ä¿¡æ¯
    fn extract_key_information_from_contexts(&self, contexts: &[SimilarContext]) -> Vec<String> {
        let mut key_info = Vec::new();

        for context in contexts.iter().take(3) { // åªå¤„ç†æœ€ç›¸å…³çš„3ä¸ªä¸Šä¸‹æ–‡
            let content = &context.content;

            // æå–å®¢æˆ·å§“åå’Œå…¬ï¿½ï¿½ä¿¡æ¯ (å¦‚ "Michael Chen from Alpha Corp")
            if let Some(client_info) = self.extract_client_information(content) {
                if !key_info.contains(&client_info) {
                    key_info.push(client_info);
                }
            }

            // æå–é‡è¦çš„ä¸šåŠ¡ç»†èŠ‚
            if let Some(business_detail) = self.extract_business_details(content) {
                if !key_info.contains(&business_detail) {
                    key_info.push(business_detail);
                }
            }

            // æå–è§£å†³æ–¹æ¡ˆä¿¡æ¯
            if let Some(solution_info) = self.extract_solution_information(content) {
                if !key_info.contains(&solution_info) {
                    key_info.push(solution_info);
                }
            }
        }

        key_info
    }

    // ğŸ”§ æå–å®¢æˆ·ä¿¡æ¯ï¼ˆå§“åã€å…¬å¸ç­‰ï¼‰- å¢å¼ºç‰ˆ
    fn extract_client_information(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // ğŸ”§ ï¿½ï¿½ï¿½å…ˆåŒ¹é…å®Œï¿½ï¿½çš„å®¢æˆ·ï¿½ï¿½æ¯ï¿½ï¿½ï¿½å¼
        let client_patterns = [
            // å®Œæ•´å§“å + å…¬å¸æ¨¡å¼
            r"([A-Z][a-z]+\s+[A-Z][a-z]+)\s+from\s+([A-Z][a-zA-Z\s]+(?:Corp|Inc|LLC|Ltd|Corporation))",
            // ï¿½ï¿½åŒ–çš„å§“å + å…¬å¸æ¨¡å¼
            r"([A-Z][a-z]+)\s+from\s+([A-Z][a-zA-Z\s]+)",
            // ç›´æ¥çš„å®¢æˆ·ä»‹ç»æ¨¡å¼
            r"this\s+is\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s+from\s+([A-Z][a-zA-Z\s]+)",
            // Hiå¼€å¤´çš„è‡ªæˆ‘ä»‹ç»
            r"hi,?\s+this\s+is\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)",
        ];

        for pattern in client_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(captures) = re.captures(content) {
                    match captures.len() {
                        3 => {
                            // åŒ…å«å§“åå’Œå…¬å¸
                            let name = captures.get(1).map(|m| m.as_str()).unwrap_or("");
                            let company = captures.get(2).map(|m| m.as_str()).unwrap_or("");
                            if !name.is_empty() && !company.is_empty() {
                                return Some(format!("Client: {} from {}", name, company.trim()));
                            }
                        },
                        2 => {
                            // åªæœ‰å§“å
                            let name = captures.get(1).map(|m| m.as_str()).unwrap_or("");
                            if !name.is_empty() {
                                return Some(format!("Client: {}", name));
                            }
                        },
                        _ => {}
                    }
                }
            }
        }

        // ğŸ”§ å¤‡ç”¨ï¼šå¯»æ‰¾å¸¸è§çš„å®¢æˆ·ä¿¡æ¯å…³é”®è¯
        if content_lower.contains("michael") && content_lower.contains("alpha") {
            return Some("Client: Michael from Alpha Corp".to_string());
        }

        if content_lower.contains("john") && content_lower.contains("techcorp") {
            return Some("Client: John from TechCorp".to_string());
        }

        None
    }

    // ğŸ”§ æå–ä¸šåŠ¡ç»†èŠ‚
    fn extract_business_details(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // è§„æ¨¡ä¿¡æ¯
        if content_lower.contains("inquiries") && content_lower.contains("month") {
            if let Some(volume) = self.extract_volume_info(&content_lower) {
                return Some(format!("Volume: {}", volume));
            }
        }

        // æŠ€æœ¯éœ€æ±‚
        if content_lower.contains("ai-powered") || content_lower.contains("ai powered") {
            return Some("Requirement: AI-powered solution".to_string());
        }

        // è¡Œä¸šä¿¡æ¯
        if content_lower.contains("customer service") {
            return Some("Domain: Customer Service".to_string());
        }

        None
    }

    // ğŸ”§ æå–è§£å†³æ–¹æ¡ˆä¿¡æ¯
    fn extract_solution_information(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        if content_lower.contains("recommend") || content_lower.contains("suggest") {
            // æå–æ¨èçš„è§£å†³æ–¹æ¡ˆ
            if content_lower.contains("enterprise") {
                return Some("Solution: Enterprise package recommended".to_string());
            }
            if content_lower.contains("custom") {
                return Some("Solution: Custom solution".to_string());
            }
        }

        None
    }

    // ğŸ”§ æå–æ•°é‡ä¿¡æ¯
    fn extract_volume_info(&self, content: &str) -> Option<String> {
        // åŒ¹é… "æ•°å­— + thousand/k + inquiries/month" æ¨¡å¼
        let volume_patterns = [
            r"(\d+[,\d]*)\s*(?:thousand|k)\s*inquiries",
            r"(\d+[,\d]*)\s*inquiries\s*per\s*month",
            r"(\d+[,\d]*)\s*monthly\s*inquiries",
        ];

        for pattern in volume_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(captures) = re.captures(content) {
                    if let Some(number) = captures.get(1) {
                        return Some(format!("{} inquiries/month", number.as_str()));
                    }
                }
            }
        }

        None
    }

    // ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½å‹ç¼©æ–¹æ³•
    async fn apply_smart_compression(&self, messages: &[ChatMessage]) -> Result<Vec<ChatMessage>> {
        debug!("Applying smart compression to {} messages", messages.len());

        // ğŸ”§ ç§»é™¤åŒï¿½ï¿½æ£€æŸ¥ - è°ƒç”¨æ–¹å·²ç»ç¡®è®¤éœ€è¦å‹ç¼©
        if messages.len() <= 2 {
            debug!("Too few messages for compression, returning original");
            return Ok(messages.to_vec());
        }

        // ğŸ”§ æ›´ç§¯æçš„å‹ç¼©ç­–ç•¥
        let keep_recent = if messages.len() > 8 {
            2  // é•¿å¯¹è¯åªä¿ç•™æœ€è¿‘2æ¡
        } else if messages.len() > 5 {
            3  // ä¸­ç­‰é•¿åº¦ä¿ç•™3æ¡
        } else {
            messages.len().saturating_sub(1)  // çŸ­å¯¹è¯ä¿ç•™å¤§éƒ¨åˆ†
        };

        let recent_messages = messages.iter().rev().take(keep_recent).rev().cloned().collect::<Vec<_>>();
        let historical_messages = &messages[..messages.len().saturating_sub(keep_recent)];

        // ï¿½ï¿½ ç”Ÿæˆæ›´ç®€æ´çš„å‹ç¼©æ‘˜è¦
        let compressed_summary = if historical_messages.is_empty() {
            String::new()
        } else {
            self.compress_conversation_history(historical_messages)
        };

        let mut result = Vec::new();

        // ğŸ”§ åªæœ‰åœ¨æœ‰å®é™…å†å²å†…å®¹æ—¶æ‰æ·»åŠ æ‘˜è¦
        if !compressed_summary.is_empty() && compressed_summary.len() > 10 {
            result.push(ChatMessage {
                role: "system".to_string(),
                content: format!("Previous conversation: {}", compressed_summary),
            });
        }

        result.extend(recent_messages);

        debug!("âœ… Compressed {} messages to {} (summary: {} chars)",
               messages.len(), result.len(),
               compressed_summary.len());

        Ok(result)
    }

    // ğŸ”§ æ–°å¢ï¼šæ£€æµ‹æ˜¯å¦åŒ…å«ç”¨æˆ·èº«ä»½ä¿¡æ¯
    fn contains_user_identity(&self, content: &str) -> bool {
        let content_lower = content.to_lowercase();

        // ğŸ”§ æ›´ç²¾ç¡®çš„ç”¨æˆ·èº«ä»½æ¨¡å¼æ£€æµ‹
        let identity_patterns = [
            // å§“åæ¨¡å¼
            r"my name is ([a-zA-Z]+)",
            r"i'm ([a-zA-Z]+)",
            r"i am ([a-zA-Z]+)",
            r"this is ([a-zA-Z]+)",
            // å·¥ä½œ/é¡¹ç›®æ¨¡å¼
            r"i'm working on",
            r"i am working on",
            r"working on (a|an)?\s*([a-zA-Z\s]+)\s*(project|system)",
            r"project about ([a-zA-Z\s]+)",
            // èŒä¸š/è§’è‰²ï¿½ï¿½ï¿½å¼
            r"i'm (a|an)\s*([a-zA-Z\s]+)",
            r"i am (a|an)\s*([a-zA-Z\s]+)",
        ];

        for pattern in identity_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if re.is_match(&content_lower) {
                    debug!("Found user identity pattern: {} in content: {}", pattern, &content[..std::cmp::min(100, content.len())]);
                    return true;
                }
            }
        }

        // ğŸ”§ å…³ï¿½ï¿½è¯æ£€æµ‹ä½œä¸ºåå¤‡
        let identity_keywords = [
            "my name", "i'm", "i am", "working on", "project about",
            "alice", "bob", "charlie", "david", "emma", "frank",
            "python", "machine learning", "data science", "ai"
        ];

        for keyword in identity_keywords {
            if content_lower.contains(keyword) {
                debug!("Found identity keyword: {} in content", keyword);
                return true;
            }
        }

        false
    }

    // ğŸ”§ æ–°å¢ï¼šæ£€æµ‹å®¢æˆ·èº«ï¿½ï¿½ä¿¡æ¯ï¼ˆç”¨äºè·¨Agentåœºæ™¯ï¼‰
    fn contains_client_identity(&self, content: &str) -> bool {
        let content_lower = content.to_lowercase();

        let client_patterns = [
            r"client is ([a-zA-Z]+)",
            r"customer ([a-zA-Z]+)",
            r"([a-zA-Z]+) from ([a-zA-Z\s]+)",
            r"working with ([a-zA-Z]+)",
        ];

        for pattern in client_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if re.is_match(&content_lower) {
                    return true;
                }
            }
        }

        content_lower.contains("client") || content_lower.contains("customer")
    }

    // ğŸ”§ æ–°å¢ï¼šæ£€æµ‹ä¸šåŠ¡å…³é”®ä¿¡æ¯
    fn contains_business_info(&self, content: &str) -> bool {
        let content_lower = content.to_lowercase();

        let business_keywords = [
            "project", "system", "application", "solution", "requirements",
            "budget", "timeline", "deadline", "scope", "deliverable"
        ];

        business_keywords.iter().any(|keyword| content_lower.contains(keyword))
    }

    // ğŸ”§ æ–°å¢ï¼šæå–ç»¼åˆå…³é”®ä¿¡æ¯
    fn extract_comprehensive_key_information(&self, contexts: &[SimilarContext]) -> Vec<String> {
        let mut key_info = Vec::new();

        for context in contexts.iter().take(4) { // å¤„ç†æ›´å¤šä¸Šä¸‹æ–‡æ¥è·å¾—å®Œæ•´ä¿¡æ¯
            let content = &context.content;

            // å®¢æˆ·èº«ä»½ä¿¡ï¿½ï¿½
            if let Some(client_info) = self.extract_client_information(content) {
                if !key_info.contains(&client_info) {
                    key_info.push(client_info);
                }
            }

            // ï¿½ï¿½åŠ¡è§„æ¨¡å’Œéœ€ï¿½ï¿½
            if let Some(business_detail) = self.extract_business_details(content) {
                if !key_info.contains(&business_detail) {
                    key_info.push(business_detail);
                }
            }

            // æŠ€æœ¯è®¨è®ºå†…å®¹
            if let Some(tech_info) = self.extract_technical_discussion(content) {
                if !key_info.contains(&tech_info) {
                    key_info.push(tech_info);
                }
            }

            // è§£å†³æ–¹æ¡ˆå’Œå»ºè®®
            if let Some(solution_info) = self.extract_solution_information(content) {
                if !key_info.contains(&solution_info) {
                    key_info.push(solution_info);
                }
            }
        }

        key_info
    }

    // ğŸ”§ ï¿½ï¿½å¢ï¼šæå–ï¿½ï¿½ï¿½æœ¯è®¨è®ºå†…å®¹
    fn extract_technical_discussion(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // æ£€æµ‹æŠ€æœ¯ç›¸å…³çš„è®¨è®º
        if content_lower.contains("architecture") || content_lower.contains("infrastructure") {
            return Some("Discussion: Technical architecture".to_string());
        }

        if content_lower.contains("scalability") || content_lower.contains("scaling") {
            return Some("Discussion: Scalability requirements".to_string());
        }

        if content_lower.contains("security") || content_lower.contains("compliance") {
            return Some("Discussion: Security and compliance".to_string());
        }

        if content_lower.contains("integration") || content_lower.contains("crm") {
            return Some("Discussion: System integration".to_string());
        }

        if content_lower.contains("pricing") || content_lower.contains("budget") {
            return Some("Discussion: Pricing and budget".to_string());
        }

        if content_lower.contains("timeline") || content_lower.contains("implementation") {
            return Some("Discussion: Implementation timeline".to_string());
        }

        None
    }

    // ğŸ”§ æ–°å¢ï¼šæ¸©å’Œå‹ç¼©æ–¹æ³•ï¼ˆä¸“ä¸ºè·¨Agentåœºæ™¯è®¾è®¡ï¼‰
    async fn apply_gentle_compression(&self, messages: &[ChatMessage]) -> Result<Vec<ChatMessage>> {
        debug!("Applying gentle compression to {} messages for cross-agent context", messages.len());

        if messages.len() <= 3 {
            debug!("Too few messages for gentle compression, returning original");
            return Ok(messages.to_vec());
        }

        // ğŸ”§ æ¸©å’Œçš„å‹ç¼©ç­–ç•¥ï¼šä¿ç•™æ›´å¤šæœ€è¿‘çš„æ¶ˆæ¯
        let keep_recent = if messages.len() > 10 {
            4  // å¾ˆé•¿å¯¹è¯ä¿ç•™æœ€è¿‘4æ¡
        } else if messages.len() > 7 {
            3  // ä¸­é•¿å¯¹è¯ä¿ç•™æœ€è¿‘3æ¡
        } else {
            messages.len().saturating_sub(2)  // çŸ­å¯¹è¯ä¿ç•™å‡ ä¹æ‰€æœ‰
        };

        let recent_messages = messages.iter().rev().take(keep_recent).rev().cloned().collect::<Vec<_>>();
        let historical_messages = &messages[..messages.len().saturating_sub(keep_recent)];

        // ğŸ”§ ç”Ÿæˆæ›´è¯¦ç»†çš„å†å²æ‘˜è¦ï¼ˆä¸“ä¸ºè·¨Agentåœºæ™¯ï¼‰
        let compressed_summary = if historical_messages.is_empty() {
            String::new()
        } else {
            self.compress_conversation_history_detailed(historical_messages)
        };

        let mut result = Vec::new();

        // ğŸ”§ ç¡®ä¿æ‘˜è¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯
        if !compressed_summary.is_empty() && compressed_summary.len() > 15 {
            result.push(ChatMessage {
                role: "system".to_string(),
                content: format!("Previous team discussion: {}", compressed_summary),
            });
        }

        result.extend(recent_messages);

        debug!("âœ… Gently compressed {} messages to {} (detailed summary: {} chars)",
               messages.len(), result.len(),
               compressed_summary.len());

        Ok(result)
    }

    // ğŸ”§ æ–°å¢ï¼šè¯¦ç»†çš„å¯¹è¯å†å²å‹ç¼©ï¼ˆä¸“ä¸ºè·¨Agentåœºæ™¯ï¼‰
    fn compress_conversation_history_detailed(&self, messages: &[ChatMessage]) -> String {
        if messages.is_empty() {
            return String::new();
        }

        let mut summary_parts = Vec::new();
        let mut client_info = Vec::new();
        let mut business_info = Vec::new();
        let mut technical_info = Vec::new();

        for message in messages {
            let content = &message.content;

            // æå–å®¢æˆ·ç›¸å…³ä¿¡æ¯
            if let Some(client) = self.extract_client_information(content) {
                if !client_info.contains(&client) {
                    client_info.push(client);
                }
            }

            // æå–ï¿½ï¿½åŠ¡ä¿¡æ¯
            if let Some(business) = self.extract_business_details(content) {
                if !business_info.contains(&business) {
                    business_info.push(business);
                }
            }

            // æå–æŠ€æœ¯è®¨è®º
            if let Some(tech) = self.extract_technical_discussion(content) {
                if !technical_info.contains(&tech) {
                    technical_info.push(tech);
                }
            }
        }

        // æ„å»ºè¯¦ç»†æ‘˜è¦
        if !client_info.is_empty() {
            summary_parts.push(client_info.join(", "));
        }

        if !business_info.is_empty() {
            summary_parts.push(business_info.join(", "));
        }

        if !technical_info.is_empty() {
            summary_parts.push(technical_info.join(", "));
        }

        if summary_parts.is_empty() {
            format!("Previous discussion: {} exchanges between team members", messages.len() / 2)
        } else {
            summary_parts.join("; ")
        }
    }

    // ğŸ”§ æ–°å¢ï¼šæå–æŒä¹…åŒ–ç”¨æˆ·èº«ä»½ä¿¡æ¯ï¼ˆå…³é”®ä¿®å¤ï¼‰
    fn extract_persistent_user_identity(&self, contexts: &[SimilarContext]) -> String {
        let mut identity_parts: Vec<String> = Vec::new();

        for context in contexts.iter() {
            let content = &context.content;
            let _content_lower = content.to_lowercase();

            // ğŸ”§ ä¼˜å…ˆæå–ï¿½ï¿½æˆ·å§“å
            if let Some(name) = self.extract_user_name_robust(content) {
                if !identity_parts.iter().any(|part| part.contains(&name)) {
                    identity_parts.push(format!("Name: {}", name));
                }
            }

            // ğŸ”§ æå–å·¥ä½œ/ï¿½ï¿½ç›®ä¿¡æ¯
            if let Some(project) = self.extract_project_info_robust(content) {
                if !identity_parts.iter().any(|part| part.contains(&project)) {
                    identity_parts.push(format!("Project: {}", project));
                }
            }

            // ğŸ”§ æå–èŒä¸šä¿¡æ¯
            if let Some(role) = self.extract_role_info_robust(content) {
                if !identity_parts.iter().any(|part| part.contains(&role)) {
                    identity_parts.push(format!("Role: {}", role));
                }
            }
        }

        if identity_parts.is_empty() {
            String::new()
        } else {
            identity_parts.join(", ")
        }
    }

    // ğŸ”§ æ–°å¢ï¼šå¼ºåŒ–çš„ç”¨æˆ·å§“åæå–
    fn extract_user_name_robust(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // ç›´æ¥å§“åæ¨¡å¼åŒ¹é…
        let name_patterns = [
            r"my name is ([a-zA-Z]+)",
            r"i'?m ([a-zA-Z]+)(?:\s|,|\.)",
            r"i am ([a-zA-Z]+)(?:\s|,|\.)",
            r"this is ([a-zA-Z]+)(?:\s|,|\.)",
            r"hi,?\s+i'?m ([a-zA-Z]+)",
        ];

        for pattern in name_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(captures) = re.captures(&content_lower) {
                    if let Some(name_match) = captures.get(1) {
                        let name = name_match.as_str();
                        // è¿‡æ»¤æ‰å¸¸è§çš„éå§“åè¯æ±‡
                        if !["working", "doing", "building", "creating", "developing"].contains(&name)
                           && name.len() > 1 && name.len() < 20 {
                            return Some(name.to_string());
                        }
                    }
                }
            }
        }

        // ğŸ”§ å¤‡ç”¨ï¼šç‰¹å®šå§“åæ£€æµ‹
        let common_names = ["alice", "bob", "charlie", "sarah", "david", "emma", "michael", "john"];
        for name in common_names {
            if content_lower.contains(name) {
                return Some(name.to_string());
            }
        }

        None
    }

    // ğŸ”§ æ–°å¢ï¼šï¿½ï¿½ï¿½åŒ–çš„é¡¹ç›®ä¿¡æ¯æå–
    fn extract_project_info_robust(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // é¡¹ç›®æè¿°æ¨¡å¼
        let project_patterns = [
            r"working on (a|an)?\s*([a-zA-Z\s]+?)\s*(project|system)",
            r"project about ([a-zA-Z\s]+)",
            r"building (a|an)?\s*([a-zA-Z\s]+?)\s*(system|application)",
            r"developing (a|an)?\s*([a-zA-Z\s]+?)\s*(solution|tool)",
        ];

        for pattern in project_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(captures) = re.captures(&content_lower) {
                    // æ‰¾åˆ°é¡¹ç›®æè¿°çš„æ•è·ç»„
                    for i in 1..captures.len() {
                        if let Some(project_match) = captures.get(i) {
                            let project = project_match.as_str().trim();
                            if project.len() > 3 && project.len() < 50
                               && !["a", "an", "the", "project", "system"].contains(&project) {
                                return Some(project.to_string());
                            }
                        }
                    }
                }
            }
        }

        // ğŸ”§ ç‰¹å®šæŠ€æœ¯æ ˆæ£€æµ‹
        if content_lower.contains("python") && content_lower.contains("machine learning") {
            return Some("Python machine learning".to_string());
        }

        if content_lower.contains("fraud detection") {
            return Some("fraud detection system".to_string());
        }

        None
    }

    // ï¿½ï¿½ï¿½ï¿½ æ–°å¢ï¼šå¼ºåŒ–çš„è§’ï¿½ï¿½ï¿½ä¿¡æ¯æå–
    fn extract_role_info_robust(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // è§’è‰²æè¿°æ¨¡å¼
        let role_patterns = [
            r"i'?m (a|an)\s*([a-zA-Z\s]+?)(?:\s|,|\.|$)",
            r"i am (a|an)\s*([a-zA-Z\s]+?)(?:\s|,|\.|$)",
            r"work as (a|an)?\s*([a-zA-Z\s]+?)(?:\s|,|\.|$)",
            r"job as (a|an)?\s*([a-zA-Z\s]+?)(?:\s|,|\.|$)",
        ];

        for pattern in role_patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(captures) = re.captures(&content_lower) {
                    if let Some(role_match) = captures.get(2) {
                        let role = role_match.as_str().trim();
                        if role.len() > 3 && role.len() < 30 {
                            return Some(role.to_string());
                        }
                    }
                }
            }
        }

        // ï¿½ï¿½ ç‰¹å®šè§’è‰²æ£€æµ‹
        let roles = [
            "data scientist", "software engineer", "developer", "engineer",
            "analyst", "researcher", "consultant", "manager"
        ];

        for role in roles {
            if content_lower.contains(role) {
                return Some(role.to_string());
            }
        }

        None
    }

    // ğŸ”§ æ–°å¢ï¿½ï¿½ï¿½ï¿½ï¿½ä»½ä¿æŠ¤å‹ç¼©æ–¹æ³•ï¼ˆç¡®ä¿ç”¨æˆ·èº«ä»½ä¸ä¸¢å¤±ï¼‰
    async fn apply_identity_preserving_compression(&self, messages: &[ChatMessage], user_identity: &str) -> Result<Vec<ChatMessage>> {
        debug!("Applying identity-preserving compression to {} messages, preserving: {}", messages.len(), user_identity);

        if messages.len() <= 3 {
            return Ok(messages.to_vec());
        }

        // ğŸ”§ æ›´ä¿å®ˆçš„å‹ç¼©ç­–ç•¥ï¼Œç¡®ä¿ç”¨æˆ·èº«ä»½ä¿¡æ¯ä¸ä¸¢å¤±
        let keep_recent = if messages.len() > 10 {
            4  // é•¿å¯¹è¯ä¿ç•™æœ€è¿‘4æ¡
        } else if messages.len() > 7 {
            3  // ä¸­ç­‰å¯¹è¯ä¿ç•™æœ€è¿‘3æ¡
        } else {
            messages.len().saturating_sub(1)  // çŸ­å¯¹è¯ä¿ç•™å¤§éƒ¨åˆ†
        };

        let recent_messages = messages.iter().rev().take(keep_recent).rev().cloned().collect::<Vec<_>>();
        let historical_messages = &messages[..messages.len().saturating_sub(keep_recent)];

        // ğŸ”§ ç”ŸæˆåŒ…å«ç”¨æˆ·èº«ä»½çš„å‹ç¼©æ‘˜è¦
        let compressed_summary = if historical_messages.is_empty() {
            if !user_identity.is_empty() {
                format!("User context: {}", user_identity)
            } else {
                String::new()
            }
        } else {
            let history_summary = self.compress_conversation_history(historical_messages);
            if !user_identity.is_empty() {
                format!("{} | {}", user_identity, history_summary)
            } else {
                history_summary
            }
        };

        let mut result = Vec::new();

        // ğŸ”§ ï¿½ï¿½åˆ¶æ·»åŠ ç”¨æˆ·ï¿½ï¿½ä»½æ‘˜è¦ï¼ˆå³ä½¿å†å²ï¿½ï¿½ç©ºï¼‰
        if !compressed_summary.is_empty() {
            result.push(ChatMessage {
                role: "system".to_string(),
                content: format!("PRESERVED CONTEXT: {}", compressed_summary),
            });
        }

        result.extend(recent_messages);

        debug!("âœ… Identity-preserving compression: {} -> {} messages, identity preserved: {}",
               messages.len(), result.len(), !user_identity.is_empty());

        Ok(result)
    }
}

#[derive(Debug, Clone)]
pub struct SimilarContext {
    pub content: String,
    pub similarity: f32,
    pub timestamp: i64,
}
